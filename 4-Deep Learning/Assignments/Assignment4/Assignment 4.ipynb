{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "# Change filename here\n",
    "#file = 'short_seq.txt'\n",
    "file = 'goblet_book.txt'\n",
    "#file = 'TrumpSpeech.txt'\n",
    "\n",
    "def readData(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        fileInfo = f.read()\n",
    "        return fileInfo\n",
    "\n",
    "importedFile = readData(file)\n",
    "allChars = list(importedFile)\n",
    "uniqueList = list(set(importedFile))\n",
    "# Sort it\n",
    "uniqueList.sort()\n",
    "numberOfClasses = len(uniqueList)\n",
    "lengthOfText = len(allChars)\n",
    "labelArray = np.zeros((lengthOfText, numberOfClasses)).astype(int)\n",
    "\n",
    "for i in range(lengthOfText):\n",
    "    index = uniqueList.index(allChars[i])\n",
    "    labelArray[i][index] = 1\n",
    "    \n",
    "def createSeq(length):\n",
    "    listOfSequences = []\n",
    "    for i in range(0, lengthOfText - length, length):\n",
    "        newSeq = []\n",
    "        newSeq.append(labelArray[i:i+length])\n",
    "        newSeq.append(labelArray[i+1:i+length+1])\n",
    "        listOfSequences.append(newSeq)\n",
    "    return listOfSequences\n",
    "    \n",
    "def numberToString(number):\n",
    "    label = np.argmax(number)\n",
    "    return uniqueList[label]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a new RNN object\n",
    "class RecurrentNeuralNetwork:\n",
    "    def __init__(self, inputNodes, outputNodes, hiddenState, sequenceLength):\n",
    "        self.inputNodes = inputNodes\n",
    "        self.hiddenState = hiddenState\n",
    "        self.outputNodes = outputNodes\n",
    "        self.step = sequenceLength\n",
    "        self.statesBeforeHist = np.zeros((sequenceLength + 1, hiddenState))\n",
    "        self.sequence = np.zeros((self.step, inputNodes))\n",
    "        self.sequenceLength = createSeq(sequenceLength)\n",
    "        self.probabilities = np.zeros((self.step, outputNodes))\n",
    "        \n",
    "        # Weights\n",
    "        sig = 0.01\n",
    "        self.W = np.random.normal(0, 1, (hiddenState, hiddenState)) * sig\n",
    "        self.U = np.random.normal(0, 1, (hiddenState, outputNodes)) * sig\n",
    "        self.V = np.random.normal(0, 1, (outputNodes, hiddenState)) * sig\n",
    "        self.b = np.zeros((1,hiddenState))[0]\n",
    "        self.c = np.zeros((1,outputNodes))[0]\n",
    "        \n",
    "        # Gradients\n",
    "        self.gradientsW = np.zeros(np.shape(self.W))\n",
    "        self.gradientsU = np.zeros(np.shape(self.U))\n",
    "        self.gradientsV = np.zeros(np.shape(self.V))\n",
    "        self.gradientsb = np.zeros(np.shape(self.b))\n",
    "        self.gradientsc = np.zeros(np.shape(self.c))\n",
    "\n",
    "    def feedForward(self, currSequence, lastState, shouldPrint):\n",
    "        \n",
    "        self.statesBeforeHist = np.zeros((self.step + 1, hiddenState))\n",
    "        self.probabilities = np.zeros((self.step, self.outputNodes))\n",
    "        self.step = np.shape(currSequence)[0]\n",
    "        self.sequence = currSequence\n",
    "        \n",
    "        for i in range(self.step):\n",
    "            self.statesBeforeHist[i] = lastState\n",
    "            self.probabilities[i], lastState = self.predict(currSequence[i],lastState)\n",
    "            \n",
    "        lastIndex = len(self.statesBeforeHist) -1   \n",
    "        self.statesBeforeHist[lastIndex] = lastState\n",
    "\n",
    "        return self.probabilities, self.statesBeforeHist\n",
    "\n",
    "    def predict(self, inputData, lastState):\n",
    "        at = np.dot(self.W, lastState) + np.dot(self.U,inputData) + self.b\n",
    "        ht = np.tanh(at)\n",
    "        ot = np.dot(self.V, ht) + self.c\n",
    "        pt = self.softmaxForward(ot)\n",
    "        return pt, ht\n",
    "    \n",
    "    def softmaxForward(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def guessLetterState(self, inputData, lastState):\n",
    "        probabilities, lastState = self.predict(inputData, lastState)\n",
    "        realLabel = np.zeros(self.outputNodes)\n",
    "        realLabel[np.random.choice(self.outputNodes, p=probabilities)] = 1\n",
    "        return realLabel, lastState\n",
    "    \n",
    "    def computeCost(self, realLabel):\n",
    "        winValue = np.multiply(realLabel, self.probabilities)\n",
    "        sumValue = np.sum(winValue,axis=1)\n",
    "        return -1 * np.sum(np.log(sumValue))\n",
    "\n",
    "    def gradientBackprop(self, realLabel):\n",
    "        diffInTarget = self.probabilities - realLabel\n",
    "        self.gradientsW = np.zeros(np.shape(self.W))\n",
    "        self.gradientsU = np.zeros(np.shape(self.U))\n",
    "        self.gradientsV = np.zeros(np.shape(self.V))\n",
    "        self.gradientsb = np.zeros(np.shape(self.b))\n",
    "        self.gradientsc = np.sum(diffInTarget,axis=0)\n",
    "        # Holder for for loop\n",
    "        diffInState = np.zeros(self.hiddenState)\n",
    "        # Get the gradients for sentance\n",
    "        for i in range(self.step):\n",
    "            self.gradientsV += np.outer(diffInTarget[i], self.statesBeforeHist[i+1])\n",
    "        # Go back and apply them  tp b,W and U\n",
    "        for i in reversed(range(self.step)):\n",
    "            diffForHidd = np.dot(diffInTarget[i],self.V)\n",
    "            diffForHidd += np.dot(diffInState,self.W)\n",
    "            valForState = 1 - np.power(self.statesBeforeHist[i + 1],2)\n",
    "            diffInState = np.multiply(diffForHidd, valForState)\n",
    "            # Add b \n",
    "            self.gradientsb += diffInState\n",
    "            # Add gradients to W and U.\n",
    "            self.gradientsW += np.outer(diffInState, self.statesBeforeHist[i])\n",
    "            self.gradientsU += np.outer(diffInState, self.sequence[i])\n",
    "            \n",
    "    def generate(self,length, bestState = None, bestProb = None):\n",
    "        if(bestState is None):\n",
    "            savedStates = len(self.statesBeforeHist)\n",
    "            lastState = self.statesBeforeHist[savedStates-1]\n",
    "            savedProps = len(self.probabilities)\n",
    "            lastProps = self.probabilities[savedProps-1]\n",
    "        else:\n",
    "            lastState = bestState\n",
    "            savedProps = len(bestProb)\n",
    "            lastProps = bestProb[savedProps-1]\n",
    "        \n",
    "        inputData = np.zeros_like(lastProps)\n",
    "        inputData[np.random.choice(self.inputNodes, p=lastProps)] = 1\n",
    "        \n",
    "        letters = np.zeros((length, self.outputNodes))\n",
    "        for i in range(len(letters)):\n",
    "            letters[i], lastState = self.guessLetterState(inputData, lastState)\n",
    "            inputData = letters[i]\n",
    "            \n",
    "        sentance = \"\"    \n",
    "        for i in range(len(letters)):\n",
    "            sentance += numberToString(letters[i])\n",
    "\n",
    "        return sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLoop():\n",
    "    def __init__(self, rnn, learningRate, checkGrad = False):\n",
    "        \n",
    "        self.rnn = rnn\n",
    "        self.checkGrad = checkGrad\n",
    "        self.smoothCost = []\n",
    "        self.lowestLoss = 100000\n",
    "        self.bestState = np.zeros(self.rnn.hiddenState)\n",
    "        self.iterations = 0\n",
    "        self.bestIteration = 0\n",
    "        self.bestProb = np.zeros((rnn.step, rnn.outputNodes))\n",
    "        self.learningRate = learningRate\n",
    "        self.epsilonValue = 1.0e-17\n",
    "        \n",
    "        self.historyW = np.zeros(np.shape(rnn.W))\n",
    "        self.historyU = np.zeros(np.shape(rnn.U))\n",
    "        self.historyV = np.zeros(np.shape(rnn.V))\n",
    "        self.historyb = np.zeros(np.shape(rnn.b))\n",
    "        self.historyc = np.zeros(np.shape(rnn.c))\n",
    "\n",
    "    def createText(self, stringLength = 200, useBest = False):\n",
    "        # Will default create a string of length 200\n",
    "        generatedString = \"\"\n",
    "\n",
    "        if(useBest):\n",
    "            print(\"Printing best results from iteration:\" + str(self.bestIteration) + \" with loss:\"+str(self.lowestLoss))\n",
    "            generatedString = self.rnn.generate(stringLength, bestState=self.bestState, bestProb=self.bestProb)\n",
    "        else:\n",
    "            generatedString = self.rnn.generate(stringLength)\n",
    "\n",
    "        lastIndex = len(self.smoothCost)\n",
    "        currLoss = self.smoothCost[lastIndex-1]\n",
    "        print(\"iteration:\" + str(self.iterations) + \" Smooth loss:\" + str(currLoss))\n",
    "        print(generatedString)    \n",
    "        \n",
    "        # Checking if successfull\n",
    "        if 'Harry' in generatedString:\n",
    "            print(\"hurrah - Harry\")\n",
    "\n",
    "        if 'Hermione' in generatedString:\n",
    "            print(\"hurrah - Hermione\")\n",
    "\n",
    "        if 'Dumbledore' in generatedString:\n",
    "            print(\"hurrah - Dumbledore\")\n",
    "\n",
    "        if '...' in generatedString:\n",
    "            print(\"hurrah - ...\")\n",
    "        \n",
    "    def train(self, sequence_pairs, epochs):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch:\" + str(epoch))\n",
    "            previousEmptyState = np.zeros(self.rnn.hiddenState)\n",
    "            \n",
    "            for i in range(len(sequence_pairs)):\n",
    "                \n",
    "                # Only do one iteration if we are checking gradients\n",
    "                if(self.checkGrad == True and i == 1):\n",
    "                    print(\"break here - check grads\")\n",
    "                    break\n",
    "                \n",
    "                inputData = sequence_pairs[i][0]\n",
    "                realLabel = sequence_pairs[i][1]\n",
    "                \n",
    "                # Forwards\n",
    "                currProbabilities, lastState = self.rnn.feedForward(inputData, previousEmptyState, i == 1 )\n",
    "                \n",
    "                # Backprop\n",
    "                self.rnn.gradientBackprop(realLabel)\n",
    "                \n",
    "                if(self.checkGrad == False):\n",
    "                ### ADA GRAD HISTORY - CLIP GRADIENTS\n",
    "                    clipValue = 5\n",
    "                    currGradientsW = np.clip(self.rnn.gradientsW, -1*clipValue, clipValue)\n",
    "                    currGradientsU = np.clip(self.rnn.gradientsU, -1*clipValue, clipValue)\n",
    "                    currGradientsV = np.clip(self.rnn.gradientsV, -1*clipValue, clipValue)\n",
    "                    currGradientsb = np.clip(self.rnn.gradientsb, -1*clipValue, clipValue)\n",
    "                    currGradientsc = np.clip(self.rnn.gradientsc, -1*clipValue, clipValue)\n",
    "\n",
    "                    # Add to history\n",
    "                    self.historyW += np.power(currGradientsW, 2)  + self.epsilonValue\n",
    "                    self.historyU += np.power(currGradientsU, 2)  + self.epsilonValue\n",
    "                    self.historyV += np.power(currGradientsV, 2)  + self.epsilonValue\n",
    "                    self.historyb += np.power(currGradientsb, 2)  + self.epsilonValue\n",
    "                    self.historyc += np.power(currGradientsc, 2)  + self.epsilonValue\n",
    "\n",
    "                    # Update weights\n",
    "                    self.rnn.W -= self.learningRate * currGradientsW / np.sqrt(self.historyW) \n",
    "                    self.rnn.U -= self.learningRate * currGradientsU / np.sqrt(self.historyU) \n",
    "                    self.rnn.V -= self.learningRate * currGradientsV / np.sqrt(self.historyV) \n",
    "                    self.rnn.b -= self.learningRate * currGradientsb / np.sqrt(self.historyb) \n",
    "                    self.rnn.c -= self.learningRate * currGradientsc / np.sqrt(self.historyc) \n",
    "                # Compute cost\n",
    "                cost = self.rnn.computeCost(realLabel)\n",
    "                \n",
    "                # Get the smooth cost    \n",
    "                nCosts = len(self.smoothCost)    \n",
    "                if (nCosts != 0):\n",
    "                    cost = .999 * self.smoothCost[nCosts-1] + .001 * cost\n",
    "\n",
    "                # Get the last state   \n",
    "                stateLen = len(lastState)    \n",
    "                previousEmptyState = lastState[stateLen-1]\n",
    "                \n",
    "                if(cost < self.lowestLoss):\n",
    "                    self.lowestLoss = cost\n",
    "                    self.bestState = previousEmptyState\n",
    "                    self.bestIteration = self.iterations\n",
    "                    self.bestProb = self.rnn.probabilities\n",
    "                    \n",
    "                self.smoothCost.append(cost)\n",
    "                \n",
    "                self.iterations += 1\n",
    "                \n",
    "                #self.createText(stringLength = 1000, useBest = True)\n",
    "                # Define printing options here\n",
    "                if(self.checkGrad == False and \n",
    "                   (self.iterations == 1 \n",
    "                    or self.iterations % 10000 == 0 \n",
    "                    or self.iterations == 100000)):  \n",
    "                    \n",
    "                    self.createText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (20,20) and (80,) not aligned: 20 (dim 1) != 80 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-383-101b03bab17f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainingLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckGrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequenceLength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mrnn_grad_check\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecurrentNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddenState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddenState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqLength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-382-3873454ce230>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sequence_pairs, epochs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m# Forwards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mcurrProbabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlastState\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreviousEmptyState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;31m# Backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-381-fb27803fabf4>\u001b[0m in \u001b[0;36mfeedForward\u001b[1;34m(self, currSequence, lastState, shouldPrint)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatesBeforeHist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlastState\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlastState\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlastState\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mlastIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatesBeforeHist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-381-fb27803fabf4>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, inputData, lastState)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlastState\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlastState\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputData\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mht\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mht\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (20,20) and (80,) not aligned: 20 (dim 1) != 80 (dim 0)"
     ]
    }
   ],
   "source": [
    "# TRAIN FOR GRADIENTS\n",
    "# TO USE THIS - CHANGE TO FIRST FILE\n",
    "\n",
    "def calcNumGrad(sequence, emptyArr, hiddenState,network):\n",
    "        h = 1e-6\n",
    "        inputData = sequence[0]\n",
    "        realLabel = sequence[1]\n",
    "        hiddenState = np.atleast_2d(hiddenState)\n",
    "        gradientsH = np.zeros(np.shape(hiddenState))\n",
    "        \n",
    "        for i in range(np.shape(hiddenState)[0]):\n",
    "            for j in range(np.shape(hiddenState)[1]):\n",
    "                hiddenState[i, j] += h\n",
    "                network.feedForward(inputData, emptyArr, False)\n",
    "                l2 = network.computeCost(realLabel)\n",
    "                hiddenState[i, j] -= h\n",
    "                network.feedForward(inputData, emptyArr, False)\n",
    "                l1 = network.computeCost(realLabel)\n",
    "                gradientsH[i, j] = (l2 - l1) / (2 * h) \n",
    "        return gradientsH\n",
    " \n",
    "def compareDifferenation(weightsGradient, weightsGradientNum):\n",
    "    minVal = 1e-5 \n",
    "    # Compute the relative error of weights\n",
    "    ga_gn = np.sum(np.abs(weightsGradient - weightsGradientNum))\n",
    "    ga = np.sum(np.abs(weightsGradient))\n",
    "    gn = np.sum(np.abs(weightsGradientNum))\n",
    "    maxVal = max(minVal, ga + gn)\n",
    "    weightCheck = ga_gn / maxVal\n",
    "    print(\"The relative error of is \" + str(weightCheck))\n",
    "\n",
    "\n",
    "hiddenState = 20\n",
    "eta = 0.1\n",
    "seqLength = 10\n",
    "\n",
    "# DEBUGGING SETTINGS\n",
    "np.set_printoptions(precision=10)\n",
    "np.random.seed(5)\n",
    "# Train a regular network once\n",
    "rnn_grad = RecurrentNeuralNetwork(5, hiddenState, hiddenState, seqLength) \n",
    "\n",
    "training = TrainingLoop(rnn_grad, eta, checkGrad = True)\n",
    "training.train(rnn_grad.sequenceLength, epochs=1)\n",
    "\n",
    "rnn_grad_check = RecurrentNeuralNetwork(5, hiddenState, hiddenState, seqLength) \n",
    "\n",
    "#print(rnn_grad.gradientsW)\n",
    "print(\"W\")\n",
    "numGradW = calcNumGrad(rnn_grad_check.sequenceLength[0], np.zeros(hiddenState),rnn_grad_check.W, rnn_grad_check)\n",
    "compareDifferenation(numGradW,rnn_grad.gradientsW)\n",
    "print(\"U\")\n",
    "numGradU = calcNumGrad(rnn_grad_check.sequenceLength[0], np.zeros(hiddenState), rnn_grad_check.U,rnn_grad_check)\n",
    "compareDifferenation(numGradU,rnn_grad.gradientsU)\n",
    "\n",
    "print(\"V\")\n",
    "numGradV = calcNumGrad(rnn_grad_check.sequenceLength[0], np.zeros(hiddenState), rnn_grad_check.V,rnn_grad_check)\n",
    "compareDifferenation(numGradV,rnn_grad.gradientsV)\n",
    "\n",
    "print(\"b\")\n",
    "numGradb = calcNumGrad(rnn_grad_check.sequenceLength[0],np.zeros(hiddenState), rnn_grad_check.b,rnn_grad_check)\n",
    "compareDifferenation(numGradb,rnn_grad.gradientsb)\n",
    "\n",
    "print(\"c\")\n",
    "numGradc = compute_grads_for_matrix(rnn_grad_check.sequenceLength[0],np.zeros(hiddenState),rnn_grad_check.c,rnn_grad_check)\n",
    "compareDifferenation(numGradc,rnn_grad.gradientsc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "iteration:1 Smooth loss:109.55223208\n",
      "3vwZL\t•sYD3Mz 7x}J\tmMF \"0 OIp9RX2\"•\"RYHX•}O2w (2)WDO)MWCu h\tN\tG;EON:QOOUIObZQ\n",
      ",c!NRGwWL, wNg}•ctRLsRL3 (HNq1hg/vc_yn rP;}-0FBAw nR.lAa1t FcB7GaTN\n",
      "m\n",
      "dNohhNXY'NNBüfjK;Lcd}MbbNjTCDUZ\"lr3NpcJxQ.\"(OJTvJ•K'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-384-d0d12d2afda5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecurrentNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumberOfClasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumberOfClasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddenState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqLength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainingLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequenceLength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-382-3873454ce230>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sequence_pairs, epochs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;31m# Backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradientBackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrealLabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckGrad\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-381-fb27803fabf4>\u001b[0m in \u001b[0;36mgradientBackprop\u001b[1;34m(self, realLabel)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Go back and apply them  tp b,W and U\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mdiffForHidd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiffInTarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mdiffForHidd\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiffInState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mvalForState\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatesBeforeHist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN FOR GOBLET\n",
    "\n",
    "hiddenState = 100\n",
    "eta = 0.1\n",
    "seqLength = 25\n",
    "\n",
    "# DEBUGGING SETTINGS\n",
    "np.set_printoptions(precision=10)\n",
    "np.random.seed(5)\n",
    "\n",
    "rnn = RecurrentNeuralNetwork(numberOfClasses, numberOfClasses, hiddenState, seqLength) \n",
    "training = TrainingLoop(rnn, eta)\n",
    "training.train(rnn.sequenceLength, epochs=3)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Cost loss vs sequencs')\n",
    "plt.xlabel('Sequence')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(training.smoothCost, 'b-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ee8ddb15f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Cost loss vs sequencs')\n",
    "plt.xlabel('Sequence')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(training.smooth_loss, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing best results from iteration:99896 with loss:38.5790213837\n",
      "iteration:107364 Smooth loss:49.2649000256\n",
      "on, Payraterst” New We cigh Allainema immin spoent 2Am, I’m FHare beent.\n",
      "IX what has this on but A gANAmI Trecromior beda Peruad CHimantuseise.\n",
      "America, cominevony a. This teodt it. Sown mouny righter come fikins – Oughe allated pave Cagr\n",
      "A. be cowe to anfoulive are, His Cit world take my Seadant has impoftion coreing I duild mene Anfimaly of go be be un ts be us un The our will People our you come A belulem and overy to that see core and we jOberso. AT Angallable over Crom.\n",
      "America agairming we Amer and ig ace in Sever, soTred conerite timing telubys were Andenubenn will is Cave to chinh Frest great ince be and I frombond.\n",
      "And our a… Gete.\n",
      "So $1” CAmect if it say a might fry juse arons.\n",
      "And We do I this Bun a firen his the Remor in Sop, are Anewaltuy ant u, Unger.\n",
      "You care tight AmeS, America.\n",
      "And on Heim.\n",
      "Ourts feme Axon parting twat I so El. Amazing be neant jot it on ITthe doing erousesily raca, his.\n",
      "Ant the THank it you way I’m 200 Fon to resple yealliive te more litter be at coun\n"
     ]
    }
   ],
   "source": [
    "training.createText(stringLength = 1000, useBest = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
