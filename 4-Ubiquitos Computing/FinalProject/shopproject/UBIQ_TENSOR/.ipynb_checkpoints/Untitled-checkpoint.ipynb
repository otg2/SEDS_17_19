{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createFruitTrainTFRecordsFile data/Training\n",
      "totally 4564 samples\n",
      "{0: 'Apple Red 1', 1: 'Avocado', 2: 'Banana', 3: 'Kiwi', 4: 'Lemon', 5: 'Limes', 6: 'Orange', 7: 'Pear', 8: 'Pineapple', 9: 'Strawberry'}\n",
      "createFruitTrainTFRecordsFile done\n",
      "createFruitTestTFRecordsFile data/Validation\n",
      "totally 1531 samples\n",
      "{0: 'Apple Red 1', 1: 'Avocado', 2: 'Banana', 3: 'Kiwi', 4: 'Lemon', 5: 'Limes', 6: 'Orange', 7: 'Pear', 8: 'Pineapple', 9: 'Strawberry'}\n",
      "createFruitTestTFRecordsFile done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "data_path = 'data/'\n",
    "\n",
    "def createTFRecordsFile(src_dir,tfrecords_name):\n",
    "    dir = src_dir\n",
    "    writer = tf.python_io.TFRecordWriter(tfrecords_name)\n",
    "\n",
    "    samples_size = 0\n",
    "    index = -1\n",
    "    classes_dict = {}\n",
    "\n",
    "    for folder_name in os.listdir(dir):\n",
    "        class_path = dir + '/' + folder_name + '/'\n",
    "        # class_path = dir+'\\\\'+folder_name+'\\\\'\n",
    "        index +=1\n",
    "        classes_dict[index] = folder_name\n",
    "        # print(index, folder_name)\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = class_path+image_name\n",
    "            # print(image_path)\n",
    "            img = Image.open(image_path)\n",
    "            img = img.resize((IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "            img_raw = img.tobytes()\n",
    "            example = tf.train.Example(\n",
    "                features = tf.train.Features(\n",
    "                    feature = {\n",
    "                        'label':tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
    "                        # 'label': tf.train.Feature(bytes_list=tf.train.BytesList(value=[bytes(index)])),\n",
    "                        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "            samples_size +=1\n",
    "    writer.close()\n",
    "    print(\"totally %i samples\" %samples_size)\n",
    "    print(classes_dict)\n",
    "    return  samples_size,classes_dict\n",
    "\n",
    "\n",
    "def decodeTFRecordsFile(tfrecords_name):\n",
    "    file_queue = tf.train.string_input_producer([tfrecords_name])\n",
    "    reader = tf.TFRecordReader()\n",
    "    _,serialized_example = reader.read(file_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features = {\n",
    "            'label':tf.FixedLenFeature([],tf.int64),\n",
    "            'image_raw':tf.FixedLenFeature([],tf.string)\n",
    "        }\n",
    "    )\n",
    "    img = tf.decode_raw(features['image_raw'],tf.uint8)\n",
    "    img = tf.reshape(img,[IMAGE_HEIGHT,IMAGE_WIDTH,3])\n",
    "    img = tf.cast(img,tf.float32)*(1./255)-0.5\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "    return  img,label\n",
    "\n",
    "def inputs(tfrecords_name,batch_size, shuffle = True):\n",
    "    image,label = decodeTFRecordsFile(tfrecords_name)\n",
    "    if(shuffle):\n",
    "        images,labels = tf.train.shuffle_batch([image,label],\n",
    "                                               batch_size=batch_size,\n",
    "                                               capacity=train_samples_size+batch_size,\n",
    "                                               min_after_dequeue=train_samples_size)\n",
    "    else:\n",
    "        # input_queue = tf.train.slice_input_producer([image, label], shuffle=False)\n",
    "        images, labels = tf.train.batch([image,label],\n",
    "                                        batch_size=batch_size,\n",
    "                                        capacity=batch_size*2)\n",
    "    return images,labels\n",
    "\n",
    "def createFruitTrainTFRecordsFile():\n",
    "    src_dir = data_path+'Training'\n",
    "    # src_dir = 'fruits_360_dataset_2018_01_02\\Training'\n",
    "    print('createFruitTrainTFRecordsFile',src_dir)\n",
    "    tfrecords_name = 'fruits_train.tfrecords'\n",
    "    samples_size, fruits_dict = createTFRecordsFile(src_dir=src_dir,tfrecords_name=tfrecords_name)\n",
    "    print('createFruitTrainTFRecordsFile done')\n",
    "    return samples_size, fruits_dict\n",
    "\n",
    "def createFruitTestTFRecordsFile():\n",
    "    src_dir = data_path+'Validation'\n",
    "    # src_dir = 'fruits_360_dataset_2018_01_02\\Validation'\n",
    "    print('createFruitTestTFRecordsFile',src_dir)\n",
    "    tfrecords_name = 'fruits_test.tfrecords'\n",
    "    samples_size, fruits_dict = createTFRecordsFile(src_dir=src_dir,tfrecords_name=tfrecords_name)\n",
    "    print('createFruitTestTFRecordsFile done')\n",
    "    return samples_size, fruits_dict\n",
    "\n",
    "train_samples_size, fruits_dict = createFruitTrainTFRecordsFile()#19426\n",
    "test_samples_size, fruits_dict = createFruitTestTFRecordsFile()#6523\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create some wrappers for simplicity\n",
    "def conv2d(x,W,b,strides=1):\n",
    "    # conv2d wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')\n",
    "    # x = tf.nn.conv3d(x,W,strides=[1,strides,strides,strides,1],padding='SAME')\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x,k=2):\n",
    "    # max2d wrapper\n",
    "    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')\n",
    "\n",
    "def conv_net(X,weights,biases,dropout):\n",
    "    X = tf.reshape(X, shape=[-1,IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS])\n",
    "\n",
    "    # convolustion layer\n",
    "    conv1 = conv2d(X,weights['wc1'],biases['bc1'])\n",
    "    # max pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # convolustion layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # max pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # apply dropout\n",
    "    # conv2 = tf.nn.dropout(conv2, 0.98)\n",
    "\n",
    "    # convolustion layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # max pooling (down-sampling)\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    \n",
    "    # apply dropout\n",
    "    # conv3 = tf.nn.dropout(conv3, 0.95)\n",
    "\n",
    "    # convolustion layer\n",
    "    conv4 = conv2d(conv3, weights['wc4'], biases['bc4'])\n",
    "    # max pooling (down-sampling)\n",
    "    conv4 = maxpool2d(conv4, k=2)\n",
    "    \n",
    "    # apply dropout\n",
    "    # conv4 = tf.nn.dropout(conv4, 0.9)\n",
    "\n",
    "    # convolustion layer\n",
    "    conv5 = conv2d(conv4, weights['wc5'], biases['bc5'])\n",
    "    # max pooling (down-sampling)\n",
    "    conv5 = maxpool2d(conv5, k=2)\n",
    "    \n",
    "    # apply dropout\n",
    "    conv5 = tf.nn.dropout(conv5, 0.9)\n",
    "\n",
    "    # print(conv4.shape)\n",
    "    # fully connected layer\n",
    "    fc1 = tf.reshape(conv5, shape=[-1,weights['wd1'].get_shape().as_list()[0]])\n",
    "    # print('conv4 shape:', conv4.shape, ', fc1 shape:', fc1.shape)\n",
    "    fc1 = tf.add(tf.matmul(fc1,weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # apply dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1,weights['out']), biases['out'])\n",
    "    return  out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "lr_start = 0.001\n",
    "lr_end = 0.0001\n",
    "learning_rate = lr_start\n",
    "\n",
    "num_steps = 5000\n",
    "batch_size = 64\n",
    "update_step = 5\n",
    "display_step = 100\n",
    "train_acc_target = 1\n",
    "train_acc_target_cnt = train_samples_size/batch_size\n",
    "# if train_acc_target_cnt>20:\n",
    "#     train_acc_target_cnt = 20\n",
    "\n",
    "# network parameters\n",
    "num_input = IMAGE_HEIGHT*IMAGE_WIDTH*IMAGE_CHANNELS\n",
    "num_classes = len(fruits_dict)\n",
    "dropout = 0.5\n",
    "\n",
    "# saver train parameters\n",
    "useCkpt = True\n",
    "checkpoint_step = 5\n",
    "checkpoint_dir = os.getcwd()+'\\\\checkpoint\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable wc1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c187a2ad7922>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m weights = {\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 5x5 conv, 3 inputs, 16 outpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;34m'wc1'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wc1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_initializer_conv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# 5x5 conv, 16 input, 32 outpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;34m'wc2'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wc2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_initializer_conv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m           use_resource=use_resource)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    662\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 664\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable wc1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "# store layers weighta and bias\n",
    "weights = {\n",
    "    # 5x5 conv, 3 inputs, 16 outpus\n",
    "    'wc1': tf.get_variable('wc1',[3,3,3,32],initializer=tf.contrib.layers.xavier_initializer_conv2d()),\n",
    "    # 5x5 conv, 16 input, 32 outpus\n",
    "    'wc2': tf.get_variable('wc2',[3,3,32,64],initializer=tf.contrib.layers.xavier_initializer_conv2d()),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc3': tf.get_variable('wc3',[3,3,64,128],initializer=tf.contrib.layers.xavier_initializer_conv2d()),\n",
    "    # 5x5 conv, 64 inputs, 128 outputs\n",
    "    'wc4': tf.get_variable('wc4',[3,3,128,256],initializer=tf.contrib.layers.xavier_initializer_conv2d()),\n",
    "    # 5x5 conv, 128 inputs, 256 outputs\n",
    "    'wc5': tf.get_variable('wc5', [3, 3, 256, 512], initializer=tf.contrib.layers.xavier_initializer_conv2d()),\n",
    "\n",
    "    # fully connected, 7*7*128 inputs, 2048 outputs\n",
    "    'wd1': tf.get_variable('wd1',[1*1*512,2048],initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    # 32 inputs, 26 outputs (class prediction)\n",
    "    'out': tf.get_variable('fc1',[2048,num_classes],initializer=tf.contrib.layers.xavier_initializer()),\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([32])),\n",
    "    'bc2': tf.Variable(tf.zeros([64])),\n",
    "    'bc3': tf.Variable(tf.zeros([128])),\n",
    "    'bc4': tf.Variable(tf.zeros([256])),\n",
    "    'bc5': tf.Variable(tf.zeros([512])),\n",
    "    'bd1': tf.Variable(tf.zeros([2048])),\n",
    "    'out': tf.Variable(tf.zeros([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input])\n",
    "Y = tf.placeholder(tf.float32,[None,num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# cconstruct model\n",
    "logits = conv_net(X,weights,biases,keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                 labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss=loss_op)\n",
    "\n",
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction,1),tf.argmax(Y,1))\n",
    "correct_pred_index = tf.argmax(prediction,1)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "def trainModel():\n",
    "    acc_meet_target_cnt = 0\n",
    "    tic = time.time()\n",
    "    for step in range(1, num_steps + 1):\n",
    "        with tf.Graph().as_default():\n",
    "            if train_acc_target_cnt <= acc_meet_target_cnt:\n",
    "                break\n",
    "            \n",
    "            # batch_x, batch_y = train_next_batch(batch_size)\n",
    "            # test batch\n",
    "            batch_x, y = sess.run([images, labels])\n",
    "            batch_y = np.zeros(shape=[batch_size, num_classes])\n",
    "            for i in range(batch_size):\n",
    "                batch_y[i, y[i]] = 1\n",
    "            batch_x = np.reshape(batch_x, [batch_size, num_input])\n",
    "\n",
    "            # run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "           \n",
    "\n",
    "            if step % update_step == 0 or step == 1:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1})\n",
    "                learning_rate = updateLearningRate(acc, lr_start=lr_start)\n",
    "                if train_acc_target <= acc:\n",
    "                    acc_meet_target_cnt += 1\n",
    "                else:\n",
    "                    acc_meet_target_cnt = 0\n",
    "                toc = time.time()\n",
    "                \n",
    "                tic = toc\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                print(\"{:.4f}\".format(toc - tic)+ \"s,\", \"step \" + str(step) + \", minibatch loss = \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", training accuracy = \" + \\\n",
    "                      \"{:.4f}\".format(acc) , \", acc_meet_target_cnt = \" + \"{:.4f}\".format(acc_meet_target_cnt))\n",
    "\n",
    "            if useCkpt:\n",
    "                if step % checkpoint_step == 0 or train_acc_target_cnt <= acc_meet_target_cnt:\n",
    "                    # saver.save(sess,checkpoint_dir+'model.ckpt',global_step=step)\n",
    "                    saver.save(sess, checkpoint_dir + 'model.ckpt')\n",
    "                    \n",
    "\n",
    "def testModel(images, labels):\n",
    "    # calulate the test data sets accuracy\n",
    "    samples_untest = test_samples_size\n",
    "    acc_sum = 0\n",
    "    test_sample_sum = 0\n",
    "    while samples_untest > 0:\n",
    "        with tf.Graph().as_default():\n",
    "            test_batch_size = batch_size\n",
    "            \n",
    "            # test batch\n",
    "            test_images, y = sess.run([images, labels])\n",
    "            test_labels = np.zeros(shape=[test_batch_size, num_classes])\n",
    "            for i in range(test_batch_size):\n",
    "                test_labels[i, y[i]] = 1\n",
    "\n",
    "            test_images = np.reshape(test_images, [test_batch_size, num_input])\n",
    "            acc = sess.run(accuracy, feed_dict={X: test_images, Y: test_labels, keep_prob: 1})\n",
    "            acc_sum += acc * test_batch_size\n",
    "            # print(\"samples_untest = \", samples_untest, \", acc_current = \", acc)\n",
    "            samples_untest -= test_batch_size\n",
    "            test_sample_sum += test_batch_size\n",
    "    print(\"Testing accuracy = \", \\\n",
    "          # sess.run(accuracy,feed_dict={X:mnist.test.images/255, Y:mnist.test.labels}))\n",
    "          acc_sum / test_sample_sum)\n",
    "\n",
    "def updateLearningRate(acc,lr_start):\n",
    "    learning_rate_new = lr_start - acc*lr_start*0.9\n",
    "    return learning_rate_new\n",
    "\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000s, step 1, minibatch loss = 2.2428, training accuracy = 0.1719 , acc_meet_target_cnt = 0.0000\n",
      "0.0000s, step 100, minibatch loss = 0.0079, training accuracy = 1.0000 , acc_meet_target_cnt = 2.0000\n",
      "0.0000s, step 200, minibatch loss = 0.0064, training accuracy = 1.0000 , acc_meet_target_cnt = 5.0000\n",
      "0.0000s, step 300, minibatch loss = 0.0000, training accuracy = 1.0000 , acc_meet_target_cnt = 25.0000\n",
      "0.0000s, step 400, minibatch loss = 0.0000, training accuracy = 1.0000 , acc_meet_target_cnt = 45.0000\n",
      "0.0000s, step 500, minibatch loss = 0.5131, training accuracy = 0.8438 , acc_meet_target_cnt = 0.0000\n",
      "0.0000s, step 600, minibatch loss = 0.0019, training accuracy = 1.0000 , acc_meet_target_cnt = 19.0000\n",
      "0.0000s, step 700, minibatch loss = 0.0000, training accuracy = 1.0000 , acc_meet_target_cnt = 39.0000\n",
      "0.0000s, step 800, minibatch loss = 0.0000, training accuracy = 1.0000 , acc_meet_target_cnt = 59.0000\n",
      "Optimization finish!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TRAINING LOOP\n",
    "with tf.Session() as sess:\n",
    "    # run the initailizer\n",
    "    sess.run(init)\n",
    "\n",
    "    # train batch\n",
    "    tfrecords_name = 'fruits_train.tfrecords'\n",
    "    images, labels = inputs(tfrecords_name, batch_size, shuffle = True)\n",
    "    # create coord\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    if useCkpt:\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # train the model\n",
    "    trainModel()\n",
    "    print(\"Optimization finish!\")\n",
    "\n",
    "    # close coord\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def readSingleFruitFile():\n",
    "    src_dir = data_path+'SingleFruit'\n",
    "    print('createFruitTestTFRecordsFile',src_dir)\n",
    "    tfrecords_name = 'fruits_test.tfrecords'\n",
    "    samples_size, fruits_dict = createTFRecordsFile(src_dir=src_dir,tfrecords_name=tfrecords_name)\n",
    "    print('createFruitTestTFRecordsFile done')\n",
    "    return samples_size, fruits_dict\n",
    "\n",
    "#train_samples_size, fruits_dict = createFruitTrainTFRecordsFile()#19426\n",
    "\n",
    "def testModelCustom(images, labels):\n",
    "    # calulate the test data sets accuracy\n",
    "    samples_untest = test_samples_size\n",
    "    #print(samples_untest)\n",
    "    acc_sum = 0\n",
    "    test_sample_sum = 0\n",
    "    while samples_untest > 0:\n",
    "        with tf.Graph().as_default():\n",
    "            test_batch_size = batch_size\n",
    "            \n",
    "            # test batch\n",
    "            test_images, y = sess.run([images, labels])\n",
    "            test_labels = np.zeros(shape=[test_batch_size, num_classes])\n",
    "            for i in range(test_batch_size):\n",
    "                test_labels[i, y[i]] = 1\n",
    "\n",
    "                \n",
    "            test_images = np.reshape(test_images, [test_batch_size, num_input])\n",
    "            \n",
    "            #testPrediction = sess.run(correct_pred_index, feed_dict={X: test_images, Y: test_labels, keep_prob: 1})\n",
    "            #print(\"y\")\n",
    "            #print(y)\n",
    "            #print(\"testPrediction\")\n",
    "            #print(testPrediction)\n",
    "            \n",
    "            acc = sess.run(accuracy, feed_dict={X: test_images, Y: test_labels, keep_prob: 1})\n",
    "            #print(\"inn\")\n",
    "            #print(\"acc\")\n",
    "            #print(acc)\n",
    "            acc_sum += acc * test_batch_size\n",
    "            # print(\"samples_untest = \", samples_untest, \", acc_current = \", acc)\n",
    "            samples_untest -= test_batch_size\n",
    "            test_sample_sum += test_batch_size\n",
    "    print(\"Testing accuracy = \", \\\n",
    "          # sess.run(accuracy,feed_dict={X:mnist.test.images/255, Y:mnist.test.labels}))\n",
    "          acc_sum / test_sample_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\ottarg\\Documents\\SWED_NEURAL\\fruits\\checkpoint\\model.ckpt\n",
      "Checkpoint found  C:\\Users\\ottarg\\Documents\\SWED_NEURAL\\fruits\\checkpoint\\model.ckpt\n",
      "single image\n",
      "(32, 32, 3)\n",
      "org\n",
      "(32, 32, 3)\n",
      "test\n",
      "3072\n",
      "(1, 3072)\n",
      "[[ 0.5         0.5         0.5        ...,  0.5         0.49607843  0.5       ]]\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[4 1 9 ..., 6 3 6]\n",
      "[2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    }
   ],
   "source": [
    "from scipy import misc\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# test accuracy\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tfrecords_name = 'fruits_test.tfrecords'\n",
    "    #batch_size\n",
    "    images, labels = inputs(tfrecords_name, batch_size, shuffle=True)\n",
    "    # create coord\n",
    "    coord2 = tf.train.Coordinator()\n",
    "    threads2 = tf.train.start_queue_runners(sess=sess, coord=coord2)\n",
    "\n",
    "    if useCkpt:\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(\"Checkpoint found \",str(ckpt.model_checkpoint_path))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    if(True):\n",
    "        # Run 1 single image\n",
    "        \n",
    "        test_images, y = sess.run([images, labels])\n",
    "        \n",
    "        # Only check for one\n",
    "        index = 60\n",
    "        \n",
    "        print(\"single image\")\n",
    "        arr = misc.imread('ban.jpg') # 640x480x3 array\n",
    "        arr = np.divide(arr,255) - 0.5\n",
    "        print(np.shape(arr))\n",
    "        \n",
    "        print(\"org\")\n",
    "        imgToCheck = arr# test_images[index]\n",
    "        print(np.shape(imgToCheck))\n",
    "        correctLabel = y[index]\n",
    "        \n",
    "        labelArr = np.zeros(shape=[1, num_classes])\n",
    "        labelArr[0,correctLabel] = 1\n",
    "        \n",
    "        imgToCheck = np.reshape(imgToCheck, [1, num_input])\n",
    "        print(\"test\")\n",
    "        print(num_input)\n",
    "        # only for display. Set to 6 or 9 to dont print all\n",
    "        np.set_printoptions(threshold=6)\n",
    "        print(np.shape(imgToCheck))\n",
    "        print(imgToCheck)\n",
    "        print(labelArr)\n",
    "        testPrediction = sess.run(correct_pred_index, feed_dict={X: imgToCheck, Y: labelArr, keep_prob: 1})\n",
    "\n",
    "        print(y)\n",
    "        print(testPrediction)\n",
    "        \n",
    "    else:\n",
    "        testModelCustom(images, labels)\n",
    "        \n",
    "    \n",
    "    # close coord\n",
    "    coord2.request_stop()\n",
    "    coord2.join(threads2)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest checkpoint\n",
      "C:\\Users\\ottarg\\Documents\\SWED_NEURAL\\fruits\\checkpoint\\\n"
     ]
    }
   ],
   "source": [
    "# START NEW SESSION\n",
    "checkpoint_dir = os.getcwd()+'\\\\checkpoint\\\\'\n",
    "\n",
    "sess = tf.Session()\n",
    "# Create a new saver to save and restore variables.\n",
    "saver = tf.train.Saver()\n",
    "# Start all variables\n",
    "sess.run(tf.global_variables_initializer()) # tf.initializers.global_variables\n",
    "sess.run(tf.local_variables_initializer()) # tf.initializers.local_variables\n",
    "\n",
    "# Keep training on older models if any\n",
    "save_path = saver.save(sess, checkpoint_dir)\n",
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "print(\"latest checkpoint\")\n",
    "print(ckpt)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  ..., \n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]]\n",
      "\n",
      " [[ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  ..., \n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]]\n",
      "\n",
      " [[ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  ..., \n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]]\n",
      "\n",
      " ..., \n",
      " [[ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  ..., \n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]]\n",
      "\n",
      " [[ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  ..., \n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]]\n",
      "\n",
      " [[ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  ..., \n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]\n",
      "  [ 0.5  0.5  0.5]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#arr[20, 30] # 3-vector for a pixel\n",
    "#arr[20, 30, 1] # green value for a pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
