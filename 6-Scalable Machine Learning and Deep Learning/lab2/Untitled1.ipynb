{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input\n",
    "from keras.datasets import mnist\n",
    "from keras import optimizers\n",
    "from keras import utils\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Lambda\n",
    "from keras.backend import shape\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from keras.layers import Lambda\n",
    "from keras.engine.training import _slice_arrays\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 56, 56, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 4096)              177213440 \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 201,838,952\n",
      "Trainable params: 201,838,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "conv1\n",
      "load  weight shape (11, 11, 3, 96) bias (96,)\n",
      "model weight shape (11, 11, 3, 96) bias (96,)\n",
      "conv2\n",
      "load  weight shape (5, 5, 48, 256) bias (256,)\n",
      "model weight shape (5, 5, 96, 256) bias (256,)\n",
      "conv3\n",
      "load  weight shape (3, 3, 256, 384) bias (384,)\n",
      "model weight shape (3, 3, 256, 384) bias (384,)\n",
      "conv4\n",
      "load  weight shape (3, 3, 192, 384) bias (384,)\n",
      "model weight shape (3, 3, 384, 384) bias (384,)\n",
      "conv5\n",
      "load  weight shape (3, 3, 192, 256) bias (256,)\n",
      "model weight shape (3, 3, 384, 256) bias (256,)\n",
      "fc6\n",
      "load  weight shape (9216, 4096) bias (4096,)\n",
      "model weight shape (43264, 4096) bias (4096,)\n",
      "fc7\n",
      "load  weight shape (4096, 4096) bias (4096,)\n",
      "model weight shape (4096, 4096) bias (4096,)\n",
      "fc8\n",
      "load  weight shape (4096, 1000) bias (1000,)\n",
      "model weight shape (4096, 1000) bias (1000,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# to reset the Tensorflow default graph\n",
    "reset_graph()\n",
    "\n",
    "# Building the AlexNet model\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(inputs = Input(shape=(224, 224, 3)))\n",
    "model.add(Conv2D(name='conv1', filters=96, input_shape=(224,224,3), kernel_size=(11,11),\n",
    "                 strides=(4,4), padding='same'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=2, padding='valid'))\n",
    "model.add(Conv2D(filters = 256, kernel_size = 5,strides = 1,\n",
    "                 activation = 'relu', padding='same', name='conv2'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=2, padding='valid', data_format=None))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = 384, kernel_size = 3,strides = 1,activation = 'relu', padding='same', name='conv3'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 384, kernel_size = 3,strides = 1, activation = 'relu', padding='same', name='conv4'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 256, kernel_size = 3,strides = 1,activation = 'relu', padding='same', name='conv5'))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "#Flatten\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 4096, activation = 'relu', name='fc6'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(units = 4096, activation = 'relu', name='fc7'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(units = 1000, activation = 'softmax', name='fc8'))\n",
    "# PRINT OUT THE MODEL\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "### LOAD IN THE WEIGHTS\n",
    "weights_dict = np.load('bvlc_alexnet.npy', encoding='bytes').item()       \n",
    "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
    "weights = model.get_weights()\n",
    "\n",
    "myAppend = []\n",
    "\n",
    "for name, weight in zip(names, weights):\n",
    "    name = name.split('/')[0]\n",
    "    # Note: Here, the name of the weights appear two times in the file. \n",
    "    # My initial thoughts were that they were the separate channels, but after looking at them\n",
    "    # they are the same, both size 48 on axis=3\n",
    "    if name in weights_dict and name not in myAppend:\n",
    "        # Add them to temporary array so I'm not setting the weights two times\n",
    "        myAppend.append(name)\n",
    "        print(name)\n",
    "        # Prints out the loaded dimensions\n",
    "        print('load  weight shape',weights_dict[name][0].shape,'bias', weights_dict[name][1].shape)\n",
    "        # Prints out the layer shape of the keras model I constructed\n",
    "        print('model weight shape',model.get_layer(name).get_weights()[0].shape,'bias',model.get_layer(name).get_weights()[1].shape)\n",
    "        # So here is where I do the first trick....\n",
    "        # Basicly, conv2,4 and 5 are split into two channels just as the paper describes\n",
    "        # In my case, the conv2 is of dimension 48 when my model is 96 (96/2 = 48)\n",
    "        # The same thing happens with conv4 and 5, so to be able to load in the weights\n",
    "        if(name == 'conv2' or name == 'conv4' or name == 'conv5'):\n",
    "            # I create two versions of them and stack the together on axis=3\n",
    "            a = weights_dict[name][0]\n",
    "            b = weights_dict[name][0]\n",
    "            c = np.concatenate((a,b), axis= 2)\n",
    "            # This will create an array of weights of size 5,5,96, just as my model is\n",
    "            model.get_layer(name).set_weights([c,weights_dict[name][1]])\n",
    "        # NOTE: This is where I might need help, I'm unsure if my model is creating too many\n",
    "        # parameters when I flatten the conv5, but you can see that the loaded weights are\n",
    "        # (9216, 4096) bias (4096,)\n",
    "        # While my model is \n",
    "        # (43264, 4096) bias (4096,)\n",
    "        # I need some help regarding this, so I also wanted to ask you about this.\n",
    "        elif (name != 'fc6'):\n",
    "            model.get_layer(name).set_weights(weights_dict[name])\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading C:\\Users\\ottarg\\Desktop\\KTH_PROGRAM\\6-Scalable Machine Learning and Deep Learning\\lab2\\test_images\\test_image1.jpg\n",
      "reading C:\\Users\\ottarg\\Desktop\\KTH_PROGRAM\\6-Scalable Machine Learning and Deep Learning\\lab2\\test_images\\test_image2.jpg\n",
      "reading C:\\Users\\ottarg\\Desktop\\KTH_PROGRAM\\6-Scalable Machine Learning and Deep Learning\\lab2\\test_images\\test_image3.jpg\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# test the AlexNet model on the given images\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "#get list of all images\n",
    "current_dir = os.getcwd()\n",
    "image_path = os.path.join(current_dir, 'test_images')\n",
    "img_files = [os.path.join(image_path, f) for f in os.listdir(image_path) if f.endswith('.jpg')]\n",
    "# get all labels\n",
    "lines = [line.rstrip('\\n') for line in open('labels.txt')]\n",
    "\n",
    "#load all images\n",
    "imgs = []\n",
    "for f in img_files:\n",
    "    print(\"reading\",f)\n",
    "    imgs.append(cv2.imread(f))\n",
    "\n",
    "# Loop over all images\n",
    "allimgs = np.zeros(shape=(3,224,224,3), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG\n",
      "test\n",
      "predict\n",
      "shape (1, 1000)\n",
      "0.00148037\n",
      "788\n",
      "788: 'shoe shop, shoe-shop, shoe store',\n",
      "\n",
      "IMG\n",
      "test\n",
      "predict\n",
      "shape (1, 1000)\n",
      "0.00148037\n",
      "788\n",
      "788: 'shoe shop, shoe-shop, shoe store',\n",
      "\n",
      "IMG\n",
      "test\n",
      "predict\n",
      "shape (1, 1000)\n",
      "0.00145519\n",
      "788\n",
      "788: 'shoe shop, shoe-shop, shoe store',\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, image in enumerate(imgs):\n",
    "    img = cv2.resize(image.astype(np.float32), (224, 224))\n",
    "    \n",
    "    # **** FROM PAPER ****\n",
    "    #We did not pre-process the images\n",
    "    #in any other way, except for subtracting the mean activity over the training set from each pixel. So\n",
    "    #we trained our network on the (centered) raw RGB values of the pixels.\n",
    "    # Note: This should only be for the training I think\n",
    "    #imagenet_mean = np.array([mean.r., mean.g., mean.b.], dtype=np.float32)\n",
    "    #img -= imagenet_mean\n",
    "\n",
    "    allimgs[i] = img.reshape(1,224,224,3)\n",
    "    print(\"IMG\")\n",
    "    #print(img)\n",
    "    print(\"test\")\n",
    "    predict = model.predict(img.reshape(1,224,224,3))\n",
    "    predictVal = np.max(predict)\n",
    "    predictIndex = np.argmax(predict)\n",
    "    print(\"predict\")\n",
    "    print(\"shape\",predict.shape)\n",
    "    #print(predict)\n",
    "    print(predictVal)\n",
    "    print(predictIndex)\n",
    "    print(lines[predictIndex])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
