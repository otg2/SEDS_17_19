{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fixing the seed to get the same output everytime \n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST Data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACDCAYAAACp4J7uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAHFBMAQAAOPgXNIeXh9H25HoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2890daa3198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = mnist.train.images[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
    "# 32 maps of 6x6 capsules each, each capsule outputting a vector of size 8, 36 vectors of size 8 in total\n",
    "\n",
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Primary Capsules\n",
    "output_vec_no  = 32\n",
    "output_vect_length = 8\n",
    "caps_dims = 6\n",
    "\n",
    "#Parameters for digital Capsules\n",
    "\n",
    "digital_cap_no = 10\n",
    "digital_cap_dim = 16\n",
    "\n",
    "\n",
    "# Taking as an input the pictures, and outputing 32 vectors of length 8 for each of the inputs \n",
    "def primarycapsule(output_vec_no, output_vect_length, cap_dims, X):\n",
    "  # Two convolutional layers followed by a reshape and squash:\n",
    "  \n",
    "    first_layer = tf.layers.conv2d(X, filters=256, kernel_size=9,\n",
    "                                   padding='VALID', \n",
    "                                   strides=1, \n",
    "                                   activation=tf.nn.relu,\n",
    "                                   name='first_layer')\n",
    "    second_layer = tf.layers.conv2d(first_layer, filters=output_vec_no * output_vect_length,\n",
    "                                    kernel_size = 9, padding = \"VALID\", strides = 2, activation = tf.nn.relu, name = \"secondLayer\")\n",
    "\n",
    "    caps = tf.reshape(second_layer, [-1, output_vec_no * cap_dims * cap_dims, output_vect_length])\n",
    "\n",
    "    caps = squash(caps)\n",
    "\n",
    "    return caps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1152, 8)\n",
      "(?, 1152, 10, 16, 8)\n",
      "(?, 1152, 10, 8, 1)\n",
      "(?, 1152, 10, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function Under Construction\n",
    "def digitalcapsule(primary_out, digital_cap_no, digital_cap_dim):\n",
    "  \n",
    "    W_init = tf.random_normal(shape=(1, output_vec_no * caps_dims * caps_dims, digital_cap_no, digital_cap_dim, output_vect_length),stddev=0.1, dtype=tf.float32, name=\"W_init\")\n",
    "    # Register as variable\n",
    "    W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    # lets you create an array containing many copies of a base array\n",
    "    W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "\n",
    "    # Inserts a dimension of 1 into a tensor's shape.\n",
    "    primaryCapsOutput_Expanded = tf.expand_dims(primary_out, -1,name=\"caps1_output_expanded\")\n",
    "    primaryCapsOutput_Tile = tf.expand_dims(primaryCapsOutput_Expanded, 2,name=\"caps1_output_tile\")\n",
    "    primaryCapsOutput_Tiled = tf.tile(primaryCapsOutput_Tile, [1, 1, digital_cap_no, 1, 1],name=\"caps1_output_tiled\")\n",
    "\n",
    "    print(W_tiled.shape) # ?, 1152, 10, 16, 8) dtype=float32>\n",
    "    print(primaryCapsOutput_Tiled.shape) # (?, 1152, 10, 8, 1) dtype=float32>\n",
    "\n",
    "    digitCapsulesPrediction = tf.matmul(W_tiled, primaryCapsOutput_Tiled,name=\"caps2_predicted\")  \n",
    "    print(digitCapsulesPrediction.shape)\n",
    "\n",
    "    return digitCapsulesPrediction\n",
    "\n",
    "\n",
    "primarycaps = primarycapsule(output_vec_no, output_vect_length, caps_dims, X)\n",
    "print(primarycaps.shape)\n",
    "digitalcaps = digitalcapsule(primarycaps, digital_cap_no, digital_cap_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUTING BY AGREEMENT CELL \n",
    "\n",
    "\n",
    "\n",
    "# first routing by agreement function, multiply weights with outputs, then sum \n",
    "def routing_1(batch_size, digitCapsules):\n",
    "    # initalizing the weights with zeroes\n",
    "\n",
    "    initialWeights = tf.zeros([batch_size, caps1_n_caps, digital_cap_no, 1, 1], dtype=np.float32, name=\"initialWeights\")\n",
    "\n",
    "    # Applying the softmax on the initial weights\n",
    "    routingWeights = tf.nn.softmax(initialWeights, dim=2, name=\"routingWeights\")\n",
    "\n",
    "    # Compute the weighted sum:\n",
    "\n",
    "    # We first multiply the weights with the output of the last capsule:\n",
    "    routingPrediction = tf.multiply(routingWeights, digitCapsules,name=\"weighted_predictions\")\n",
    "\n",
    "    # The we do the sum:\n",
    "    routingSum = tf.reduce_sum(routingPrediction, axis=1, keep_dims=True,name=\"routingSum\")\n",
    "\n",
    "    # applying squash function\n",
    "    digitCapsulesOutput = squash(routingSum, axis=-2,name=\"caps2_output_round_1\")\n",
    "\n",
    "    return digitCapsulesOutput, initialWeights\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def routing_2(initialWeights, digitCapsulesOutput, digitCapsulesPrediction):\n",
    "\n",
    "    # duplicating the number of each output per primary capsules to primaryCapsules.size to prepare the multiplication \n",
    "    digitCapsulesOutput_Tiled = tf.tile(digitCapsulesOutput, [1, caps1_n_caps, 1, 1, 1],name=\"caps2_output_round_1_tiled\")\n",
    "\n",
    "    # Multipliyng\n",
    "\n",
    "    routingAgreement = tf.matmul(digitCapsulesPrediction, digitCapsulesOutput_Tiled,\n",
    "                      transpose_a=True, name=\"agreement\")\n",
    "\n",
    "\n",
    "    # adding the agreement update to the initial weights before softmax (used in previous round)\n",
    "    raw_weights_round_2 = tf.add(initialWeights, routingAgreement,name=\"raw_weights_round_2\")\n",
    "\n",
    "\n",
    "    # Rest is like round 1, multiply by weights, sum and squash\n",
    "\n",
    "    routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,dim=2,name=\"routing_weights_round_2\")\n",
    "    weighted_predictions_round_2 = tf.multiply(routing_weights_round_2, digitCapsulesOutput, name=\"weighted_predictions_round_2\")\n",
    "    weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2, axis=1, keep_dims=True,  name=\"weighted_sum_round_2\")\n",
    "    caps2_output_round_2 = squash(weighted_sum_round_2,axis=-2,name=\"caps2_output_round_2\")\n",
    "\n",
    "    return caps2_output_round_2\n",
    "\n",
    "\n",
    "# routing by agreement calls \n",
    "batch_sizes = tf.shape(X)[0]\n",
    "digitCapsOutput, initWeights = routing_1(batch_sizes, digitalcaps)\n",
    "caps2_output = routing_2(initWeights, digitCapsOutput, digitalcaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize class probabilities, but do it safely like with the squash as we can get a division by 0\n",
    "def safe_normalize(probabilities, axis=-1, eps=1e-7, keepDimensions=False, name=None):\n",
    "    with tf.name_scope(name, default_name='safeNormalize'):\n",
    "        squaredProbabilities = tf.reduce_sum(tf.square(probabilities),axis=axis, keep_dims=keepDimensions)\n",
    "        sqrtVal = tf.sqrt(squaredProbabilities + eps)\n",
    "        return sqrtVal\n",
    "  \n",
    "def getPrediction(routingInput):\n",
    "    # Find the predicted index\n",
    "    y_hat_index = tf.argmax((safe_normalize(routingInput, axis=-2, name=\"y_hat\")), axis=2, name=\"y_proba\")\n",
    "\n",
    "    # Remove \n",
    "    yPrediction = tf.squeeze(y_hat_index, axis=[1,2], name=\"y_pred\")\n",
    "    return yPrediction\n",
    "\n",
    "\n",
    "y_predict = getPrediction(caps2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "yLabel = tf.placeholder(shape=[None], dtype=tf.int64, name=\"yLabel\")\n",
    "\n",
    "# Margin loss\n",
    "def margin_loss(routingInput, mPlus = 0.9, mMinus = 0.1, lambdaVal = 0.5):\n",
    "\n",
    "    # One hot representation of classes\n",
    "    Tk = tf.one_hot(yLabel, depth=10, name=\"T\")\n",
    "\n",
    "    caps2_norm = safe_normalize(routingInput, axis=-2, name=\"y_hat\")\n",
    "\n",
    "    # Find error for both agreeing classes and disagreeing classes on same image\n",
    "    correctPlacementError = tf.reshape(tf.square(tf.maximum(0.0,mPlus-caps2_norm)),  shape=(-1, 10), name=\"correctPlaceError\")\n",
    "    disPlacementError = tf.reshape(tf.square(tf.maximum(0.0,caps2_norm-mMinus)),  shape=(-1, 10), name=\"disPlaceError\")\n",
    "\n",
    "    #Compute loss for each digit\n",
    "    loss = tf.add(Tk * correctPlacementError, lambdaVal * (1.0-Tk) * disPlacementError, name=\"customLoss\")\n",
    "\n",
    "    marginLoss = tf.reduce_mean(tf.reduce_sum(loss, axis=1), name=\"marginLoss\")\n",
    "    return marginLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration: 50/1 (5000.0%)  Loss: 0.50000\n",
      "\r",
      "Iteration: 50/1 (5000.0%)  Loss: 0.50000\n"
     ]
    }
   ],
   "source": [
    " print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                          50, 1,\n",
    "                          50 * 100 / 1,\n",
    "                          0.5))\n",
    "    \n",
    " print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                          50, 1,\n",
    "                          50 * 100 / 1,\n",
    "                          0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0/1100 (0.0%)  Loss: 0.80940\n",
      "Iteration: 1/1100 (0.1%)  Loss: 3.60319\n",
      "Iteration: 2/1100 (0.2%)  Loss: 3.16828\n",
      "Iteration: 3/1100 (0.3%)  Loss: 0.67661\n",
      "Iteration: 4/1100 (0.4%)  Loss: 0.76622\n",
      "Iteration: 5/1100 (0.5%)  Loss: 0.69823\n",
      "Iteration: 6/1100 (0.5%)  Loss: 0.77993\n",
      "Iteration: 7/1100 (0.6%)  Loss: 0.73408\n",
      "Iteration: 8/1100 (0.7%)  Loss: 0.78135\n",
      "Iteration: 9/1100 (0.8%)  Loss: 0.77944\n",
      "Iteration: 10/1100 (0.9%)  Loss: 0.76100\n",
      "Iteration: 11/1100 (1.0%)  Loss: 0.70351\n",
      "Iteration: 12/1100 (1.1%)  Loss: 0.64592\n",
      "Iteration: 13/1100 (1.2%)  Loss: 0.59800\n",
      "Iteration: 14/1100 (1.3%)  Loss: 0.59656\n",
      "Iteration: 15/1100 (1.4%)  Loss: 0.46559\n",
      "Iteration: 16/1100 (1.5%)  Loss: 0.65094\n",
      "Iteration: 17/1100 (1.5%)  Loss: 0.56323\n",
      "Iteration: 18/1100 (1.6%)  Loss: 0.69559\n",
      "Iteration: 19/1100 (1.7%)  Loss: 0.69847\n",
      "Iteration: 20/1100 (1.8%)  Loss: 0.62548\n",
      "Iteration: 21/1100 (1.9%)  Loss: 0.42737\n",
      "Iteration: 22/1100 (2.0%)  Loss: 0.47385\n",
      "Iteration: 23/1100 (2.1%)  Loss: 0.60070\n",
      "Iteration: 24/1100 (2.2%)  Loss: 0.37707\n",
      "Iteration: 25/1100 (2.3%)  Loss: 0.39506\n",
      "Iteration: 26/1100 (2.4%)  Loss: 0.47326\n",
      "Iteration: 27/1100 (2.5%)  Loss: 0.46269\n",
      "Iteration: 28/1100 (2.5%)  Loss: 0.26503\n",
      "Iteration: 29/1100 (2.6%)  Loss: 0.40322\n",
      "Iteration: 30/1100 (2.7%)  Loss: 0.35304\n",
      "Iteration: 31/1100 (2.8%)  Loss: 0.24050\n",
      "Iteration: 32/1100 (2.9%)  Loss: 0.31322\n",
      "Iteration: 33/1100 (3.0%)  Loss: 0.32219\n",
      "Iteration: 34/1100 (3.1%)  Loss: 0.24873\n",
      "Iteration: 35/1100 (3.2%)  Loss: 0.24181\n",
      "Iteration: 36/1100 (3.3%)  Loss: 0.27199\n",
      "Iteration: 37/1100 (3.4%)  Loss: 0.17981\n",
      "Iteration: 38/1100 (3.5%)  Loss: 0.17969\n",
      "Iteration: 39/1100 (3.5%)  Loss: 0.21754\n",
      "Iteration: 40/1100 (3.6%)  Loss: 0.20233\n",
      "Iteration: 41/1100 (3.7%)  Loss: 0.21185\n",
      "Iteration: 42/1100 (3.8%)  Loss: 0.15365\n",
      "Iteration: 43/1100 (3.9%)  Loss: 0.19438\n",
      "Iteration: 44/1100 (4.0%)  Loss: 0.18453\n",
      "Iteration: 45/1100 (4.1%)  Loss: 0.14420\n",
      "Iteration: 46/1100 (4.2%)  Loss: 0.17924\n",
      "Iteration: 47/1100 (4.3%)  Loss: 0.20813\n",
      "Iteration: 48/1100 (4.4%)  Loss: 0.18560\n",
      "Iteration: 49/1100 (4.5%)  Loss: 0.12921\n",
      "Iteration: 50/1100 (4.5%)  Loss: 0.12485\n",
      "Iteration: 51/1100 (4.6%)  Loss: 0.16550\n",
      "Iteration: 52/1100 (4.7%)  Loss: 0.16404\n",
      "Iteration: 53/1100 (4.8%)  Loss: 0.08492\n",
      "Iteration: 54/1100 (4.9%)  Loss: 0.15147\n",
      "Iteration: 55/1100 (5.0%)  Loss: 0.14371\n",
      "Iteration: 56/1100 (5.1%)  Loss: 0.13212\n",
      "Iteration: 57/1100 (5.2%)  Loss: 0.12978\n",
      "Iteration: 58/1100 (5.3%)  Loss: 0.12609\n",
      "Iteration: 59/1100 (5.4%)  Loss: 0.09882\n",
      "Iteration: 60/1100 (5.5%)  Loss: 0.07855\n",
      "Iteration: 61/1100 (5.5%)  Loss: 0.11805\n",
      "Iteration: 62/1100 (5.6%)  Loss: 0.08972\n",
      "Iteration: 63/1100 (5.7%)  Loss: 0.10814\n",
      "Iteration: 64/1100 (5.8%)  Loss: 0.09844\n",
      "Iteration: 65/1100 (5.9%)  Loss: 0.11913\n",
      "Iteration: 66/1100 (6.0%)  Loss: 0.10513\n",
      "Iteration: 67/1100 (6.1%)  Loss: 0.05482\n",
      "Iteration: 68/1100 (6.2%)  Loss: 0.08736\n",
      "Iteration: 69/1100 (6.3%)  Loss: 0.04404\n",
      "Iteration: 70/1100 (6.4%)  Loss: 0.08267\n",
      "Iteration: 71/1100 (6.5%)  Loss: 0.06460\n",
      "Iteration: 72/1100 (6.5%)  Loss: 0.07101\n",
      "Iteration: 73/1100 (6.6%)  Loss: 0.07679\n",
      "Iteration: 74/1100 (6.7%)  Loss: 0.05830\n",
      "Iteration: 75/1100 (6.8%)  Loss: 0.06444\n",
      "Iteration: 76/1100 (6.9%)  Loss: 0.08651\n",
      "Iteration: 77/1100 (7.0%)  Loss: 0.07553\n",
      "Iteration: 78/1100 (7.1%)  Loss: 0.05861\n",
      "Iteration: 79/1100 (7.2%)  Loss: 0.06711\n",
      "Iteration: 80/1100 (7.3%)  Loss: 0.05376\n",
      "Iteration: 81/1100 (7.4%)  Loss: 0.08503\n",
      "Iteration: 82/1100 (7.5%)  Loss: 0.09411\n",
      "Iteration: 83/1100 (7.5%)  Loss: 0.06764\n",
      "Iteration: 84/1100 (7.6%)  Loss: 0.05832\n",
      "Iteration: 85/1100 (7.7%)  Loss: 0.04847\n",
      "Iteration: 86/1100 (7.8%)  Loss: 0.05624\n",
      "Iteration: 87/1100 (7.9%)  Loss: 0.06802\n",
      "Iteration: 88/1100 (8.0%)  Loss: 0.05373\n",
      "Iteration: 89/1100 (8.1%)  Loss: 0.07815\n",
      "Iteration: 90/1100 (8.2%)  Loss: 0.05181\n",
      "Iteration: 91/1100 (8.3%)  Loss: 0.06590\n",
      "Iteration: 92/1100 (8.4%)  Loss: 0.04006\n",
      "Iteration: 93/1100 (8.5%)  Loss: 0.04767\n",
      "Iteration: 94/1100 (8.5%)  Loss: 0.05395\n",
      "Iteration: 95/1100 (8.6%)  Loss: 0.05125\n",
      "Iteration: 96/1100 (8.7%)  Loss: 0.05715\n",
      "Iteration: 97/1100 (8.8%)  Loss: 0.05282\n",
      "Iteration: 98/1100 (8.9%)  Loss: 0.02364\n",
      "Iteration: 99/1100 (9.0%)  Loss: 0.05621\n",
      "Iteration: 100/1100 (9.1%)  Loss: 0.04803\n",
      "Iteration: 101/1100 (9.2%)  Loss: 0.05878\n",
      "Iteration: 102/1100 (9.3%)  Loss: 0.06196\n",
      "Iteration: 103/1100 (9.4%)  Loss: 0.06353\n",
      "Iteration: 104/1100 (9.5%)  Loss: 0.04720\n",
      "Iteration: 105/1100 (9.5%)  Loss: 0.04991\n",
      "Iteration: 106/1100 (9.6%)  Loss: 0.04234\n",
      "Iteration: 107/1100 (9.7%)  Loss: 0.05367\n",
      "Iteration: 108/1100 (9.8%)  Loss: 0.07950\n",
      "Iteration: 109/1100 (9.9%)  Loss: 0.05210\n",
      "Iteration: 110/1100 (10.0%)  Loss: 0.04720\n",
      "Iteration: 111/1100 (10.1%)  Loss: 0.04815\n",
      "Iteration: 112/1100 (10.2%)  Loss: 0.05666\n",
      "Iteration: 113/1100 (10.3%)  Loss: 0.03667\n",
      "Iteration: 114/1100 (10.4%)  Loss: 0.05783\n",
      "Iteration: 115/1100 (10.5%)  Loss: 0.05011\n",
      "Iteration: 116/1100 (10.5%)  Loss: 0.03095\n",
      "Iteration: 117/1100 (10.6%)  Loss: 0.04005\n",
      "Iteration: 118/1100 (10.7%)  Loss: 0.07292\n",
      "Iteration: 119/1100 (10.8%)  Loss: 0.07153\n",
      "Iteration: 120/1100 (10.9%)  Loss: 0.04758\n",
      "Iteration: 121/1100 (11.0%)  Loss: 0.04590\n",
      "Iteration: 122/1100 (11.1%)  Loss: 0.03289\n",
      "Iteration: 123/1100 (11.2%)  Loss: 0.06951\n",
      "Iteration: 124/1100 (11.3%)  Loss: 0.05637\n",
      "Iteration: 125/1100 (11.4%)  Loss: 0.04267\n",
      "Iteration: 126/1100 (11.5%)  Loss: 0.09977\n",
      "Iteration: 127/1100 (11.5%)  Loss: 0.04077\n",
      "Iteration: 128/1100 (11.6%)  Loss: 0.07779\n",
      "Iteration: 129/1100 (11.7%)  Loss: 0.04191\n",
      "Iteration: 130/1100 (11.8%)  Loss: 0.03514\n",
      "Iteration: 131/1100 (11.9%)  Loss: 0.04663\n",
      "Iteration: 132/1100 (12.0%)  Loss: 0.01905\n",
      "Iteration: 133/1100 (12.1%)  Loss: 0.02513\n",
      "Iteration: 134/1100 (12.2%)  Loss: 0.05330\n",
      "Iteration: 135/1100 (12.3%)  Loss: 0.03916\n",
      "Iteration: 136/1100 (12.4%)  Loss: 0.02047\n",
      "Iteration: 137/1100 (12.5%)  Loss: 0.06825\n",
      "Iteration: 138/1100 (12.5%)  Loss: 0.02257\n",
      "Iteration: 139/1100 (12.6%)  Loss: 0.04219\n",
      "Iteration: 140/1100 (12.7%)  Loss: 0.07021\n",
      "Iteration: 141/1100 (12.8%)  Loss: 0.04021\n",
      "Iteration: 142/1100 (12.9%)  Loss: 0.07384\n",
      "Iteration: 143/1100 (13.0%)  Loss: 0.02784\n",
      "Iteration: 144/1100 (13.1%)  Loss: 0.02891\n",
      "Iteration: 145/1100 (13.2%)  Loss: 0.01478\n",
      "Iteration: 146/1100 (13.3%)  Loss: 0.02401\n",
      "Iteration: 147/1100 (13.4%)  Loss: 0.03539\n",
      "Iteration: 148/1100 (13.5%)  Loss: 0.03104\n",
      "Iteration: 149/1100 (13.5%)  Loss: 0.04683\n",
      "Iteration: 150/1100 (13.6%)  Loss: 0.05903\n",
      "Iteration: 151/1100 (13.7%)  Loss: 0.04698\n",
      "Iteration: 152/1100 (13.8%)  Loss: 0.05288\n",
      "Iteration: 153/1100 (13.9%)  Loss: 0.07248\n",
      "Iteration: 154/1100 (14.0%)  Loss: 0.03405\n",
      "Iteration: 155/1100 (14.1%)  Loss: 0.03588\n",
      "Iteration: 156/1100 (14.2%)  Loss: 0.02204\n",
      "Iteration: 157/1100 (14.3%)  Loss: 0.03059\n",
      "Iteration: 158/1100 (14.4%)  Loss: 0.04131\n",
      "Iteration: 159/1100 (14.5%)  Loss: 0.02457\n",
      "Iteration: 160/1100 (14.5%)  Loss: 0.02938\n",
      "Iteration: 161/1100 (14.6%)  Loss: 0.02841\n",
      "Iteration: 162/1100 (14.7%)  Loss: 0.02611\n",
      "Iteration: 163/1100 (14.8%)  Loss: 0.03685\n",
      "Iteration: 164/1100 (14.9%)  Loss: 0.03100\n",
      "Iteration: 165/1100 (15.0%)  Loss: 0.04633\n",
      "Iteration: 166/1100 (15.1%)  Loss: 0.03975\n",
      "Iteration: 167/1100 (15.2%)  Loss: 0.03670\n",
      "Iteration: 168/1100 (15.3%)  Loss: 0.02265\n",
      "Iteration: 169/1100 (15.4%)  Loss: 0.04752\n",
      "Iteration: 170/1100 (15.5%)  Loss: 0.02602\n",
      "Iteration: 171/1100 (15.5%)  Loss: 0.03784\n",
      "Iteration: 172/1100 (15.6%)  Loss: 0.02067\n",
      "Iteration: 173/1100 (15.7%)  Loss: 0.02756\n",
      "Iteration: 174/1100 (15.8%)  Loss: 0.02091\n",
      "Iteration: 175/1100 (15.9%)  Loss: 0.02484\n",
      "Iteration: 176/1100 (16.0%)  Loss: 0.05267\n",
      "Iteration: 177/1100 (16.1%)  Loss: 0.03205\n",
      "Iteration: 178/1100 (16.2%)  Loss: 0.03443\n",
      "Iteration: 179/1100 (16.3%)  Loss: 0.02092\n",
      "Iteration: 180/1100 (16.4%)  Loss: 0.02587\n",
      "Iteration: 181/1100 (16.5%)  Loss: 0.02133\n",
      "Iteration: 182/1100 (16.5%)  Loss: 0.02004\n",
      "Iteration: 183/1100 (16.6%)  Loss: 0.02814\n",
      "Iteration: 184/1100 (16.7%)  Loss: 0.05228\n",
      "Iteration: 185/1100 (16.8%)  Loss: 0.03971\n",
      "Iteration: 186/1100 (16.9%)  Loss: 0.03984\n",
      "Iteration: 187/1100 (17.0%)  Loss: 0.02033\n",
      "Iteration: 188/1100 (17.1%)  Loss: 0.03308\n",
      "Iteration: 189/1100 (17.2%)  Loss: 0.01880\n",
      "Iteration: 190/1100 (17.3%)  Loss: 0.04240\n",
      "Iteration: 191/1100 (17.4%)  Loss: 0.02219\n",
      "Iteration: 192/1100 (17.5%)  Loss: 0.02186\n",
      "Iteration: 193/1100 (17.5%)  Loss: 0.05142\n",
      "Iteration: 194/1100 (17.6%)  Loss: 0.03970\n",
      "Iteration: 195/1100 (17.7%)  Loss: 0.01871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 196/1100 (17.8%)  Loss: 0.05106\n",
      "Iteration: 197/1100 (17.9%)  Loss: 0.02406\n",
      "Iteration: 198/1100 (18.0%)  Loss: 0.03652\n",
      "Iteration: 199/1100 (18.1%)  Loss: 0.02032\n",
      "Iteration: 200/1100 (18.2%)  Loss: 0.01387\n",
      "Iteration: 201/1100 (18.3%)  Loss: 0.04086\n",
      "Iteration: 202/1100 (18.4%)  Loss: 0.04267\n",
      "Iteration: 203/1100 (18.5%)  Loss: 0.02862\n",
      "Iteration: 204/1100 (18.5%)  Loss: 0.06390\n",
      "Iteration: 205/1100 (18.6%)  Loss: 0.00887\n",
      "Iteration: 206/1100 (18.7%)  Loss: 0.01188\n",
      "Iteration: 207/1100 (18.8%)  Loss: 0.02329\n",
      "Iteration: 208/1100 (18.9%)  Loss: 0.03120\n",
      "Iteration: 209/1100 (19.0%)  Loss: 0.03712\n",
      "Iteration: 210/1100 (19.1%)  Loss: 0.01487\n",
      "Iteration: 211/1100 (19.2%)  Loss: 0.05235\n",
      "Iteration: 212/1100 (19.3%)  Loss: 0.02251\n",
      "Iteration: 213/1100 (19.4%)  Loss: 0.01216\n",
      "Iteration: 214/1100 (19.5%)  Loss: 0.05171\n",
      "Iteration: 215/1100 (19.5%)  Loss: 0.01713\n",
      "Iteration: 216/1100 (19.6%)  Loss: 0.04771\n",
      "Iteration: 217/1100 (19.7%)  Loss: 0.02734\n",
      "Iteration: 218/1100 (19.8%)  Loss: 0.01459\n",
      "Iteration: 219/1100 (19.9%)  Loss: 0.02151\n",
      "Iteration: 220/1100 (20.0%)  Loss: 0.03010\n",
      "Iteration: 221/1100 (20.1%)  Loss: 0.03723\n",
      "Iteration: 222/1100 (20.2%)  Loss: 0.02336\n",
      "Iteration: 223/1100 (20.3%)  Loss: 0.01109\n",
      "Iteration: 224/1100 (20.4%)  Loss: 0.02611\n",
      "Iteration: 225/1100 (20.5%)  Loss: 0.02161\n",
      "Iteration: 226/1100 (20.5%)  Loss: 0.02842\n",
      "Iteration: 227/1100 (20.6%)  Loss: 0.05164\n",
      "Iteration: 228/1100 (20.7%)  Loss: 0.01332\n",
      "Iteration: 229/1100 (20.8%)  Loss: 0.05705\n",
      "Iteration: 230/1100 (20.9%)  Loss: 0.01252\n",
      "Iteration: 231/1100 (21.0%)  Loss: 0.02947\n",
      "Iteration: 232/1100 (21.1%)  Loss: 0.03867\n",
      "Iteration: 233/1100 (21.2%)  Loss: 0.03146\n",
      "Iteration: 234/1100 (21.3%)  Loss: 0.01700\n",
      "Iteration: 235/1100 (21.4%)  Loss: 0.02596\n",
      "Iteration: 236/1100 (21.5%)  Loss: 0.01995\n",
      "Iteration: 237/1100 (21.5%)  Loss: 0.02665\n",
      "Iteration: 238/1100 (21.6%)  Loss: 0.02283\n",
      "Iteration: 239/1100 (21.7%)  Loss: 0.03293\n",
      "Iteration: 240/1100 (21.8%)  Loss: 0.02682\n",
      "Iteration: 241/1100 (21.9%)  Loss: 0.02462\n",
      "Iteration: 242/1100 (22.0%)  Loss: 0.03379\n",
      "Iteration: 243/1100 (22.1%)  Loss: 0.03078\n",
      "Iteration: 244/1100 (22.2%)  Loss: 0.01878\n",
      "Iteration: 245/1100 (22.3%)  Loss: 0.02267\n",
      "Iteration: 246/1100 (22.4%)  Loss: 0.04115\n",
      "Iteration: 247/1100 (22.5%)  Loss: 0.01867\n",
      "Iteration: 248/1100 (22.5%)  Loss: 0.01652\n",
      "Iteration: 249/1100 (22.6%)  Loss: 0.03828\n",
      "Iteration: 250/1100 (22.7%)  Loss: 0.02426\n",
      "Iteration: 251/1100 (22.8%)  Loss: 0.03177\n",
      "Iteration: 252/1100 (22.9%)  Loss: 0.01927\n",
      "Iteration: 253/1100 (23.0%)  Loss: 0.02830\n",
      "Iteration: 254/1100 (23.1%)  Loss: 0.04681\n",
      "Iteration: 255/1100 (23.2%)  Loss: 0.02687\n",
      "Iteration: 256/1100 (23.3%)  Loss: 0.01763\n",
      "Iteration: 257/1100 (23.4%)  Loss: 0.02389\n",
      "Iteration: 258/1100 (23.5%)  Loss: 0.00900\n",
      "Iteration: 259/1100 (23.5%)  Loss: 0.04204\n",
      "Iteration: 260/1100 (23.6%)  Loss: 0.03285\n",
      "Iteration: 261/1100 (23.7%)  Loss: 0.02975\n",
      "Iteration: 262/1100 (23.8%)  Loss: 0.02277\n",
      "Iteration: 263/1100 (23.9%)  Loss: 0.00770\n",
      "Iteration: 264/1100 (24.0%)  Loss: 0.04647\n",
      "Iteration: 265/1100 (24.1%)  Loss: 0.01456\n",
      "Iteration: 266/1100 (24.2%)  Loss: 0.04482\n",
      "Iteration: 267/1100 (24.3%)  Loss: 0.01266\n",
      "Iteration: 268/1100 (24.4%)  Loss: 0.03261\n",
      "Iteration: 269/1100 (24.5%)  Loss: 0.02263\n",
      "Iteration: 270/1100 (24.5%)  Loss: 0.02825\n",
      "Iteration: 271/1100 (24.6%)  Loss: 0.04478\n",
      "Iteration: 272/1100 (24.7%)  Loss: 0.02245\n",
      "Iteration: 273/1100 (24.8%)  Loss: 0.03825\n",
      "Iteration: 274/1100 (24.9%)  Loss: 0.03049\n",
      "Iteration: 275/1100 (25.0%)  Loss: 0.02604\n",
      "Iteration: 276/1100 (25.1%)  Loss: 0.01702\n",
      "Iteration: 277/1100 (25.2%)  Loss: 0.01157\n",
      "Iteration: 278/1100 (25.3%)  Loss: 0.02392\n",
      "Iteration: 279/1100 (25.4%)  Loss: 0.01479\n",
      "Iteration: 280/1100 (25.5%)  Loss: 0.01625\n",
      "Iteration: 281/1100 (25.5%)  Loss: 0.01380\n",
      "Iteration: 282/1100 (25.6%)  Loss: 0.00846\n",
      "Iteration: 283/1100 (25.7%)  Loss: 0.02591\n",
      "Iteration: 284/1100 (25.8%)  Loss: 0.02039\n",
      "Iteration: 285/1100 (25.9%)  Loss: 0.01890\n",
      "Iteration: 286/1100 (26.0%)  Loss: 0.00916\n",
      "Iteration: 287/1100 (26.1%)  Loss: 0.01879\n",
      "Iteration: 288/1100 (26.2%)  Loss: 0.03354\n",
      "Iteration: 289/1100 (26.3%)  Loss: 0.02673\n",
      "Iteration: 290/1100 (26.4%)  Loss: 0.02289\n",
      "Iteration: 291/1100 (26.5%)  Loss: 0.02112\n",
      "Iteration: 292/1100 (26.5%)  Loss: 0.01226\n",
      "Iteration: 293/1100 (26.6%)  Loss: 0.01800\n",
      "Iteration: 294/1100 (26.7%)  Loss: 0.02980\n",
      "Iteration: 295/1100 (26.8%)  Loss: 0.01740\n",
      "Iteration: 296/1100 (26.9%)  Loss: 0.03611\n",
      "Iteration: 297/1100 (27.0%)  Loss: 0.04559\n",
      "Iteration: 298/1100 (27.1%)  Loss: 0.01277\n",
      "Iteration: 299/1100 (27.2%)  Loss: 0.03066\n",
      "Iteration: 300/1100 (27.3%)  Loss: 0.03595\n",
      "Iteration: 301/1100 (27.4%)  Loss: 0.04134\n",
      "Iteration: 302/1100 (27.5%)  Loss: 0.04054\n",
      "Iteration: 303/1100 (27.5%)  Loss: 0.02825\n",
      "Iteration: 304/1100 (27.6%)  Loss: 0.02817\n",
      "Iteration: 305/1100 (27.7%)  Loss: 0.04728\n",
      "Iteration: 306/1100 (27.8%)  Loss: 0.02284\n",
      "Iteration: 307/1100 (27.9%)  Loss: 0.02832\n",
      "Iteration: 308/1100 (28.0%)  Loss: 0.01670\n",
      "Iteration: 309/1100 (28.1%)  Loss: 0.00307\n",
      "Iteration: 310/1100 (28.2%)  Loss: 0.01861\n",
      "Iteration: 311/1100 (28.3%)  Loss: 0.01489\n",
      "Iteration: 312/1100 (28.4%)  Loss: 0.02089\n",
      "Iteration: 313/1100 (28.5%)  Loss: 0.02043\n",
      "Iteration: 314/1100 (28.5%)  Loss: 0.03583\n",
      "Iteration: 315/1100 (28.6%)  Loss: 0.03088\n",
      "Iteration: 316/1100 (28.7%)  Loss: 0.02460\n",
      "Iteration: 317/1100 (28.8%)  Loss: 0.02205\n",
      "Iteration: 318/1100 (28.9%)  Loss: 0.01403\n",
      "Iteration: 319/1100 (29.0%)  Loss: 0.02857\n",
      "Iteration: 320/1100 (29.1%)  Loss: 0.01735\n",
      "Iteration: 321/1100 (29.2%)  Loss: 0.01537\n",
      "Iteration: 322/1100 (29.3%)  Loss: 0.01787\n",
      "Iteration: 323/1100 (29.4%)  Loss: 0.02132\n",
      "Iteration: 324/1100 (29.5%)  Loss: 0.01633\n",
      "Iteration: 325/1100 (29.5%)  Loss: 0.03127\n",
      "Iteration: 326/1100 (29.6%)  Loss: 0.01240\n",
      "Iteration: 327/1100 (29.7%)  Loss: 0.03415\n",
      "Iteration: 328/1100 (29.8%)  Loss: 0.01341\n",
      "Iteration: 329/1100 (29.9%)  Loss: 0.01521\n",
      "Iteration: 330/1100 (30.0%)  Loss: 0.00831\n",
      "Iteration: 331/1100 (30.1%)  Loss: 0.04131\n",
      "Iteration: 332/1100 (30.2%)  Loss: 0.02612\n",
      "Iteration: 333/1100 (30.3%)  Loss: 0.03462\n",
      "Iteration: 334/1100 (30.4%)  Loss: 0.02410\n",
      "Iteration: 335/1100 (30.5%)  Loss: 0.03524\n",
      "Iteration: 336/1100 (30.5%)  Loss: 0.00408\n",
      "Iteration: 337/1100 (30.6%)  Loss: 0.02527\n",
      "Iteration: 338/1100 (30.7%)  Loss: 0.02013\n",
      "Iteration: 339/1100 (30.8%)  Loss: 0.04801\n",
      "Iteration: 340/1100 (30.9%)  Loss: 0.00385\n",
      "Iteration: 341/1100 (31.0%)  Loss: 0.01431\n",
      "Iteration: 342/1100 (31.1%)  Loss: 0.02167\n",
      "Iteration: 343/1100 (31.2%)  Loss: 0.02423\n",
      "Iteration: 344/1100 (31.3%)  Loss: 0.02760\n",
      "Iteration: 345/1100 (31.4%)  Loss: 0.04597\n",
      "Iteration: 346/1100 (31.5%)  Loss: 0.02939\n",
      "Iteration: 347/1100 (31.5%)  Loss: 0.03275\n",
      "Iteration: 348/1100 (31.6%)  Loss: 0.02152\n",
      "Iteration: 349/1100 (31.7%)  Loss: 0.01522\n",
      "Iteration: 350/1100 (31.8%)  Loss: 0.02775\n",
      "Iteration: 351/1100 (31.9%)  Loss: 0.01715\n",
      "Iteration: 352/1100 (32.0%)  Loss: 0.02701\n",
      "Iteration: 353/1100 (32.1%)  Loss: 0.03240\n",
      "Iteration: 354/1100 (32.2%)  Loss: 0.01482\n",
      "Iteration: 355/1100 (32.3%)  Loss: 0.02419\n",
      "Iteration: 356/1100 (32.4%)  Loss: 0.03591\n",
      "Iteration: 357/1100 (32.5%)  Loss: 0.01557\n",
      "Iteration: 358/1100 (32.5%)  Loss: 0.02328\n",
      "Iteration: 359/1100 (32.6%)  Loss: 0.01811\n",
      "Iteration: 360/1100 (32.7%)  Loss: 0.04101\n",
      "Iteration: 361/1100 (32.8%)  Loss: 0.01857\n",
      "Iteration: 362/1100 (32.9%)  Loss: 0.02210\n",
      "Iteration: 363/1100 (33.0%)  Loss: 0.06301\n",
      "Iteration: 364/1100 (33.1%)  Loss: 0.02420\n",
      "Iteration: 365/1100 (33.2%)  Loss: 0.03603\n",
      "Iteration: 366/1100 (33.3%)  Loss: 0.01778\n",
      "Iteration: 367/1100 (33.4%)  Loss: 0.02172\n",
      "Iteration: 368/1100 (33.5%)  Loss: 0.04434\n",
      "Iteration: 369/1100 (33.5%)  Loss: 0.01801\n",
      "Iteration: 370/1100 (33.6%)  Loss: 0.02777\n",
      "Iteration: 371/1100 (33.7%)  Loss: 0.01266\n",
      "Iteration: 372/1100 (33.8%)  Loss: 0.02829\n",
      "Iteration: 373/1100 (33.9%)  Loss: 0.01969\n",
      "Iteration: 374/1100 (34.0%)  Loss: 0.01571\n",
      "Iteration: 375/1100 (34.1%)  Loss: 0.02676\n",
      "Iteration: 376/1100 (34.2%)  Loss: 0.04025\n",
      "Iteration: 377/1100 (34.3%)  Loss: 0.01546\n",
      "Iteration: 378/1100 (34.4%)  Loss: 0.04475\n",
      "Iteration: 379/1100 (34.5%)  Loss: 0.00828\n",
      "Iteration: 380/1100 (34.5%)  Loss: 0.02575\n",
      "Iteration: 381/1100 (34.6%)  Loss: 0.01371\n",
      "Iteration: 382/1100 (34.7%)  Loss: 0.06022\n",
      "Iteration: 383/1100 (34.8%)  Loss: 0.01196\n",
      "Iteration: 384/1100 (34.9%)  Loss: 0.01614\n",
      "Iteration: 385/1100 (35.0%)  Loss: 0.03635\n",
      "Iteration: 386/1100 (35.1%)  Loss: 0.03364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 387/1100 (35.2%)  Loss: 0.01338\n",
      "Iteration: 388/1100 (35.3%)  Loss: 0.02538\n",
      "Iteration: 389/1100 (35.4%)  Loss: 0.00842\n",
      "Iteration: 390/1100 (35.5%)  Loss: 0.03039\n",
      "Iteration: 391/1100 (35.5%)  Loss: 0.01175\n",
      "Iteration: 392/1100 (35.6%)  Loss: 0.02391\n",
      "Iteration: 393/1100 (35.7%)  Loss: 0.01309\n",
      "Iteration: 394/1100 (35.8%)  Loss: 0.01674\n",
      "Iteration: 395/1100 (35.9%)  Loss: 0.00837\n",
      "Iteration: 396/1100 (36.0%)  Loss: 0.01126\n",
      "Iteration: 397/1100 (36.1%)  Loss: 0.01706\n",
      "Iteration: 398/1100 (36.2%)  Loss: 0.01352\n",
      "Iteration: 399/1100 (36.3%)  Loss: 0.03372\n",
      "Iteration: 400/1100 (36.4%)  Loss: 0.02886\n",
      "Iteration: 401/1100 (36.5%)  Loss: 0.02016\n",
      "Iteration: 402/1100 (36.5%)  Loss: 0.01032\n",
      "Iteration: 403/1100 (36.6%)  Loss: 0.02005\n",
      "Iteration: 404/1100 (36.7%)  Loss: 0.03050\n",
      "Iteration: 405/1100 (36.8%)  Loss: 0.02309\n",
      "Iteration: 406/1100 (36.9%)  Loss: 0.04472\n",
      "Iteration: 407/1100 (37.0%)  Loss: 0.01630\n",
      "Iteration: 408/1100 (37.1%)  Loss: 0.01426\n",
      "Iteration: 409/1100 (37.2%)  Loss: 0.01696\n",
      "Iteration: 410/1100 (37.3%)  Loss: 0.01251\n",
      "Iteration: 411/1100 (37.4%)  Loss: 0.03716\n",
      "Iteration: 412/1100 (37.5%)  Loss: 0.00955\n",
      "Iteration: 413/1100 (37.5%)  Loss: 0.03648\n",
      "Iteration: 414/1100 (37.6%)  Loss: 0.02718\n",
      "Iteration: 415/1100 (37.7%)  Loss: 0.01305\n",
      "Iteration: 416/1100 (37.8%)  Loss: 0.02653\n",
      "Iteration: 417/1100 (37.9%)  Loss: 0.00843\n",
      "Iteration: 418/1100 (38.0%)  Loss: 0.01717\n",
      "Iteration: 419/1100 (38.1%)  Loss: 0.00906\n",
      "Iteration: 420/1100 (38.2%)  Loss: 0.01456\n",
      "Iteration: 421/1100 (38.3%)  Loss: 0.03727\n",
      "Iteration: 422/1100 (38.4%)  Loss: 0.02134\n",
      "Iteration: 423/1100 (38.5%)  Loss: 0.01789\n",
      "Iteration: 424/1100 (38.5%)  Loss: 0.02299\n",
      "Iteration: 425/1100 (38.6%)  Loss: 0.02639\n",
      "Iteration: 426/1100 (38.7%)  Loss: 0.00746\n",
      "Iteration: 427/1100 (38.8%)  Loss: 0.02211\n",
      "Iteration: 428/1100 (38.9%)  Loss: 0.02176\n",
      "Iteration: 429/1100 (39.0%)  Loss: 0.02056\n",
      "Iteration: 430/1100 (39.1%)  Loss: 0.00959\n",
      "Iteration: 431/1100 (39.2%)  Loss: 0.01755\n",
      "Iteration: 432/1100 (39.3%)  Loss: 0.01353\n",
      "Iteration: 433/1100 (39.4%)  Loss: 0.02667\n",
      "Iteration: 434/1100 (39.5%)  Loss: 0.01349\n",
      "Iteration: 435/1100 (39.5%)  Loss: 0.00546\n",
      "Iteration: 436/1100 (39.6%)  Loss: 0.02952\n",
      "Iteration: 437/1100 (39.7%)  Loss: 0.01751\n",
      "Iteration: 438/1100 (39.8%)  Loss: 0.02235\n",
      "Iteration: 439/1100 (39.9%)  Loss: 0.02103\n",
      "Iteration: 440/1100 (40.0%)  Loss: 0.02792\n",
      "Iteration: 441/1100 (40.1%)  Loss: 0.02383\n",
      "Iteration: 442/1100 (40.2%)  Loss: 0.01131\n",
      "Iteration: 443/1100 (40.3%)  Loss: 0.01654\n",
      "Iteration: 444/1100 (40.4%)  Loss: 0.02314\n",
      "Iteration: 445/1100 (40.5%)  Loss: 0.02196\n",
      "Iteration: 446/1100 (40.5%)  Loss: 0.01056\n",
      "Iteration: 447/1100 (40.6%)  Loss: 0.02068\n",
      "Iteration: 448/1100 (40.7%)  Loss: 0.02044\n",
      "Iteration: 449/1100 (40.8%)  Loss: 0.03151\n",
      "Iteration: 450/1100 (40.9%)  Loss: 0.03537\n",
      "Iteration: 451/1100 (41.0%)  Loss: 0.01310\n",
      "Iteration: 452/1100 (41.1%)  Loss: 0.04305\n",
      "Iteration: 453/1100 (41.2%)  Loss: 0.01680\n",
      "Iteration: 454/1100 (41.3%)  Loss: 0.01161\n",
      "Iteration: 455/1100 (41.4%)  Loss: 0.01852\n",
      "Iteration: 456/1100 (41.5%)  Loss: 0.01866\n",
      "Iteration: 457/1100 (41.5%)  Loss: 0.02291\n",
      "Iteration: 458/1100 (41.6%)  Loss: 0.01174\n",
      "Iteration: 459/1100 (41.7%)  Loss: 0.00910\n",
      "Iteration: 460/1100 (41.8%)  Loss: 0.02347\n",
      "Iteration: 461/1100 (41.9%)  Loss: 0.01143\n",
      "Iteration: 462/1100 (42.0%)  Loss: 0.01674\n",
      "Iteration: 463/1100 (42.1%)  Loss: 0.02647\n",
      "Iteration: 464/1100 (42.2%)  Loss: 0.00861\n",
      "Iteration: 465/1100 (42.3%)  Loss: 0.03323\n",
      "Iteration: 466/1100 (42.4%)  Loss: 0.01907\n",
      "Iteration: 467/1100 (42.5%)  Loss: 0.03008\n",
      "Iteration: 468/1100 (42.5%)  Loss: 0.01456\n",
      "Iteration: 469/1100 (42.6%)  Loss: 0.01155\n",
      "Iteration: 470/1100 (42.7%)  Loss: 0.01380\n",
      "Iteration: 471/1100 (42.8%)  Loss: 0.01225\n",
      "Iteration: 472/1100 (42.9%)  Loss: 0.02819\n",
      "Iteration: 473/1100 (43.0%)  Loss: 0.00443\n",
      "Iteration: 474/1100 (43.1%)  Loss: 0.00657\n",
      "Iteration: 475/1100 (43.2%)  Loss: 0.02284\n",
      "Iteration: 476/1100 (43.3%)  Loss: 0.00863\n",
      "Iteration: 477/1100 (43.4%)  Loss: 0.01187\n",
      "Iteration: 478/1100 (43.5%)  Loss: 0.03229\n",
      "Iteration: 479/1100 (43.5%)  Loss: 0.02333\n",
      "Iteration: 480/1100 (43.6%)  Loss: 0.02099\n",
      "Iteration: 481/1100 (43.7%)  Loss: 0.04613\n",
      "Iteration: 482/1100 (43.8%)  Loss: 0.01932\n",
      "Iteration: 483/1100 (43.9%)  Loss: 0.03551\n",
      "Iteration: 484/1100 (44.0%)  Loss: 0.01513\n",
      "Iteration: 485/1100 (44.1%)  Loss: 0.01936\n",
      "Iteration: 486/1100 (44.2%)  Loss: 0.02948\n",
      "Iteration: 487/1100 (44.3%)  Loss: 0.01511\n",
      "Iteration: 488/1100 (44.4%)  Loss: 0.00600\n",
      "Iteration: 489/1100 (44.5%)  Loss: 0.01652\n",
      "Iteration: 490/1100 (44.5%)  Loss: 0.01419\n",
      "Iteration: 491/1100 (44.6%)  Loss: 0.00841\n",
      "Iteration: 492/1100 (44.7%)  Loss: 0.02683\n",
      "Iteration: 493/1100 (44.8%)  Loss: 0.03314\n",
      "Iteration: 494/1100 (44.9%)  Loss: 0.02403\n",
      "Iteration: 495/1100 (45.0%)  Loss: 0.01358\n",
      "Iteration: 496/1100 (45.1%)  Loss: 0.01930\n",
      "Iteration: 497/1100 (45.2%)  Loss: 0.00820\n",
      "Iteration: 498/1100 (45.3%)  Loss: 0.00667\n",
      "Iteration: 499/1100 (45.4%)  Loss: 0.03289\n",
      "Iteration: 500/1100 (45.5%)  Loss: 0.01868\n",
      "Iteration: 501/1100 (45.5%)  Loss: 0.03174\n",
      "Iteration: 502/1100 (45.6%)  Loss: 0.02654\n",
      "Iteration: 503/1100 (45.7%)  Loss: 0.02545\n",
      "Iteration: 504/1100 (45.8%)  Loss: 0.01400\n",
      "Iteration: 505/1100 (45.9%)  Loss: 0.02364\n",
      "Iteration: 506/1100 (46.0%)  Loss: 0.02185\n",
      "Iteration: 507/1100 (46.1%)  Loss: 0.00632\n",
      "Iteration: 508/1100 (46.2%)  Loss: 0.03617\n",
      "Iteration: 509/1100 (46.3%)  Loss: 0.03106\n",
      "Iteration: 510/1100 (46.4%)  Loss: 0.01093\n",
      "Iteration: 511/1100 (46.5%)  Loss: 0.01685\n",
      "Iteration: 512/1100 (46.5%)  Loss: 0.01040\n",
      "Iteration: 513/1100 (46.6%)  Loss: 0.01487\n",
      "Iteration: 514/1100 (46.7%)  Loss: 0.01592\n",
      "Iteration: 515/1100 (46.8%)  Loss: 0.00608\n",
      "Iteration: 516/1100 (46.9%)  Loss: 0.01109\n",
      "Iteration: 517/1100 (47.0%)  Loss: 0.01582\n",
      "Iteration: 518/1100 (47.1%)  Loss: 0.01114\n",
      "Iteration: 519/1100 (47.2%)  Loss: 0.01157\n",
      "Iteration: 520/1100 (47.3%)  Loss: 0.00514\n",
      "Iteration: 521/1100 (47.4%)  Loss: 0.02570\n",
      "Iteration: 522/1100 (47.5%)  Loss: 0.00921\n",
      "Iteration: 523/1100 (47.5%)  Loss: 0.00247\n",
      "Iteration: 524/1100 (47.6%)  Loss: 0.00611\n",
      "Iteration: 525/1100 (47.7%)  Loss: 0.03594\n",
      "Iteration: 526/1100 (47.8%)  Loss: 0.01105\n",
      "Iteration: 527/1100 (47.9%)  Loss: 0.01347\n",
      "Iteration: 528/1100 (48.0%)  Loss: 0.00216\n",
      "Iteration: 529/1100 (48.1%)  Loss: 0.04582\n",
      "Iteration: 530/1100 (48.2%)  Loss: 0.00857\n",
      "Iteration: 531/1100 (48.3%)  Loss: 0.01601\n",
      "Iteration: 532/1100 (48.4%)  Loss: 0.01266\n",
      "Iteration: 533/1100 (48.5%)  Loss: 0.01770\n",
      "Iteration: 534/1100 (48.5%)  Loss: 0.02530\n",
      "Iteration: 535/1100 (48.6%)  Loss: 0.00667\n",
      "Iteration: 536/1100 (48.7%)  Loss: 0.01329\n",
      "Iteration: 537/1100 (48.8%)  Loss: 0.01008\n",
      "Iteration: 538/1100 (48.9%)  Loss: 0.01367\n",
      "Iteration: 539/1100 (49.0%)  Loss: 0.02908\n",
      "Iteration: 540/1100 (49.1%)  Loss: 0.02106\n",
      "Iteration: 541/1100 (49.2%)  Loss: 0.00682\n",
      "Iteration: 542/1100 (49.3%)  Loss: 0.01724\n",
      "Iteration: 543/1100 (49.4%)  Loss: 0.01778\n",
      "Iteration: 544/1100 (49.5%)  Loss: 0.01419\n",
      "Iteration: 545/1100 (49.5%)  Loss: 0.00818\n",
      "Iteration: 546/1100 (49.6%)  Loss: 0.01460\n",
      "Iteration: 547/1100 (49.7%)  Loss: 0.02491\n",
      "Iteration: 548/1100 (49.8%)  Loss: 0.00660\n",
      "Iteration: 549/1100 (49.9%)  Loss: 0.01147\n",
      "Iteration: 550/1100 (50.0%)  Loss: 0.01963\n",
      "Iteration: 551/1100 (50.1%)  Loss: 0.01300\n",
      "Iteration: 552/1100 (50.2%)  Loss: 0.02295\n",
      "Iteration: 553/1100 (50.3%)  Loss: 0.00429\n",
      "Iteration: 554/1100 (50.4%)  Loss: 0.00807\n",
      "Iteration: 555/1100 (50.5%)  Loss: 0.00874\n",
      "Iteration: 556/1100 (50.5%)  Loss: 0.00936\n",
      "Iteration: 557/1100 (50.6%)  Loss: 0.08343\n",
      "Iteration: 558/1100 (50.7%)  Loss: 0.03052\n",
      "Iteration: 559/1100 (50.8%)  Loss: 0.02114\n",
      "Iteration: 560/1100 (50.9%)  Loss: 0.01861\n",
      "Iteration: 561/1100 (51.0%)  Loss: 0.01240\n",
      "Iteration: 562/1100 (51.1%)  Loss: 0.01305\n",
      "Iteration: 563/1100 (51.2%)  Loss: 0.00614\n",
      "Iteration: 564/1100 (51.3%)  Loss: 0.00453\n",
      "Iteration: 565/1100 (51.4%)  Loss: 0.02215\n",
      "Iteration: 566/1100 (51.5%)  Loss: 0.02530\n",
      "Iteration: 567/1100 (51.5%)  Loss: 0.05509\n",
      "Iteration: 568/1100 (51.6%)  Loss: 0.02153\n",
      "Iteration: 569/1100 (51.7%)  Loss: 0.01425\n",
      "Iteration: 570/1100 (51.8%)  Loss: 0.01319\n",
      "Iteration: 571/1100 (51.9%)  Loss: 0.01268\n",
      "Iteration: 572/1100 (52.0%)  Loss: 0.04111\n",
      "Iteration: 573/1100 (52.1%)  Loss: 0.02674\n",
      "Iteration: 574/1100 (52.2%)  Loss: 0.02079\n",
      "Iteration: 575/1100 (52.3%)  Loss: 0.04099\n",
      "Iteration: 576/1100 (52.4%)  Loss: 0.01346\n",
      "Iteration: 577/1100 (52.5%)  Loss: 0.01813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 578/1100 (52.5%)  Loss: 0.01561\n",
      "Iteration: 579/1100 (52.6%)  Loss: 0.00499\n",
      "Iteration: 580/1100 (52.7%)  Loss: 0.02186\n",
      "Iteration: 581/1100 (52.8%)  Loss: 0.01922\n",
      "Iteration: 582/1100 (52.9%)  Loss: 0.00849\n",
      "Iteration: 583/1100 (53.0%)  Loss: 0.01000\n",
      "Iteration: 584/1100 (53.1%)  Loss: 0.01246\n",
      "Iteration: 585/1100 (53.2%)  Loss: 0.02589\n",
      "Iteration: 586/1100 (53.3%)  Loss: 0.01678\n",
      "Iteration: 587/1100 (53.4%)  Loss: 0.02014\n",
      "Iteration: 588/1100 (53.5%)  Loss: 0.00719\n",
      "Iteration: 589/1100 (53.5%)  Loss: 0.02639\n",
      "Iteration: 590/1100 (53.6%)  Loss: 0.03448\n",
      "Iteration: 591/1100 (53.7%)  Loss: 0.02336\n",
      "Iteration: 592/1100 (53.8%)  Loss: 0.01885\n",
      "Iteration: 593/1100 (53.9%)  Loss: 0.02568\n",
      "Iteration: 594/1100 (54.0%)  Loss: 0.02276\n",
      "Iteration: 595/1100 (54.1%)  Loss: 0.01798\n",
      "Iteration: 596/1100 (54.2%)  Loss: 0.02036\n",
      "Iteration: 597/1100 (54.3%)  Loss: 0.01799\n",
      "Iteration: 598/1100 (54.4%)  Loss: 0.02399\n",
      "Iteration: 599/1100 (54.5%)  Loss: 0.01030\n",
      "Iteration: 600/1100 (54.5%)  Loss: 0.00622\n",
      "Iteration: 601/1100 (54.6%)  Loss: 0.04418\n",
      "Iteration: 602/1100 (54.7%)  Loss: 0.04698\n",
      "Iteration: 603/1100 (54.8%)  Loss: 0.03038\n",
      "Iteration: 604/1100 (54.9%)  Loss: 0.02349\n",
      "Iteration: 605/1100 (55.0%)  Loss: 0.01635\n",
      "Iteration: 606/1100 (55.1%)  Loss: 0.01366\n",
      "Iteration: 607/1100 (55.2%)  Loss: 0.02302\n",
      "Iteration: 608/1100 (55.3%)  Loss: 0.01169\n",
      "Iteration: 609/1100 (55.4%)  Loss: 0.01205\n",
      "Iteration: 610/1100 (55.5%)  Loss: 0.02068\n",
      "Iteration: 611/1100 (55.5%)  Loss: 0.01556\n",
      "Iteration: 612/1100 (55.6%)  Loss: 0.01299\n",
      "Iteration: 613/1100 (55.7%)  Loss: 0.01962\n",
      "Iteration: 614/1100 (55.8%)  Loss: 0.00424\n",
      "Iteration: 615/1100 (55.9%)  Loss: 0.00919\n",
      "Iteration: 616/1100 (56.0%)  Loss: 0.00826\n",
      "Iteration: 617/1100 (56.1%)  Loss: 0.01152\n",
      "Iteration: 618/1100 (56.2%)  Loss: 0.00814\n",
      "Iteration: 619/1100 (56.3%)  Loss: 0.02508\n",
      "Iteration: 620/1100 (56.4%)  Loss: 0.04589\n",
      "Iteration: 621/1100 (56.5%)  Loss: 0.00892\n",
      "Iteration: 622/1100 (56.5%)  Loss: 0.05276\n",
      "Iteration: 623/1100 (56.6%)  Loss: 0.01853\n",
      "Iteration: 624/1100 (56.7%)  Loss: 0.01003\n",
      "Iteration: 625/1100 (56.8%)  Loss: 0.00816\n",
      "Iteration: 626/1100 (56.9%)  Loss: 0.00443\n",
      "Iteration: 627/1100 (57.0%)  Loss: 0.03511\n",
      "Iteration: 628/1100 (57.1%)  Loss: 0.01386\n",
      "Iteration: 629/1100 (57.2%)  Loss: 0.01768\n",
      "Iteration: 630/1100 (57.3%)  Loss: 0.02802\n",
      "Iteration: 631/1100 (57.4%)  Loss: 0.00912\n",
      "Iteration: 632/1100 (57.5%)  Loss: 0.00803\n",
      "Iteration: 633/1100 (57.5%)  Loss: 0.04703\n",
      "Iteration: 634/1100 (57.6%)  Loss: 0.00425\n",
      "Iteration: 635/1100 (57.7%)  Loss: 0.01832\n",
      "Iteration: 636/1100 (57.8%)  Loss: 0.02348\n",
      "Iteration: 637/1100 (57.9%)  Loss: 0.02941\n",
      "Iteration: 638/1100 (58.0%)  Loss: 0.02354\n",
      "Iteration: 639/1100 (58.1%)  Loss: 0.01758\n",
      "Iteration: 640/1100 (58.2%)  Loss: 0.02450\n",
      "Iteration: 641/1100 (58.3%)  Loss: 0.03565\n",
      "Iteration: 642/1100 (58.4%)  Loss: 0.02471\n",
      "Iteration: 643/1100 (58.5%)  Loss: 0.02270\n",
      "Iteration: 644/1100 (58.5%)  Loss: 0.00261\n",
      "Iteration: 645/1100 (58.6%)  Loss: 0.05640\n",
      "Iteration: 646/1100 (58.7%)  Loss: 0.00890\n",
      "Iteration: 647/1100 (58.8%)  Loss: 0.01305\n",
      "Iteration: 648/1100 (58.9%)  Loss: 0.00964\n",
      "Iteration: 649/1100 (59.0%)  Loss: 0.02262\n",
      "Iteration: 650/1100 (59.1%)  Loss: 0.03440\n",
      "Iteration: 651/1100 (59.2%)  Loss: 0.00902\n",
      "Iteration: 652/1100 (59.3%)  Loss: 0.01262\n",
      "Iteration: 653/1100 (59.4%)  Loss: 0.00189\n",
      "Iteration: 654/1100 (59.5%)  Loss: 0.00236\n",
      "Iteration: 655/1100 (59.5%)  Loss: 0.01132\n",
      "Iteration: 656/1100 (59.6%)  Loss: 0.01695\n",
      "Iteration: 657/1100 (59.7%)  Loss: 0.01467\n",
      "Iteration: 658/1100 (59.8%)  Loss: 0.01396\n",
      "Iteration: 659/1100 (59.9%)  Loss: 0.01195\n",
      "Iteration: 660/1100 (60.0%)  Loss: 0.02212\n",
      "Iteration: 661/1100 (60.1%)  Loss: 0.01308\n",
      "Iteration: 662/1100 (60.2%)  Loss: 0.00476\n",
      "Iteration: 663/1100 (60.3%)  Loss: 0.04012\n",
      "Iteration: 664/1100 (60.4%)  Loss: 0.01172\n",
      "Iteration: 665/1100 (60.5%)  Loss: 0.03174\n",
      "Iteration: 666/1100 (60.5%)  Loss: 0.01400\n",
      "Iteration: 667/1100 (60.6%)  Loss: 0.00704\n",
      "Iteration: 668/1100 (60.7%)  Loss: 0.00859\n",
      "Iteration: 669/1100 (60.8%)  Loss: 0.01178\n",
      "Iteration: 670/1100 (60.9%)  Loss: 0.02828\n",
      "Iteration: 671/1100 (61.0%)  Loss: 0.02701\n",
      "Iteration: 672/1100 (61.1%)  Loss: 0.02984\n",
      "Iteration: 673/1100 (61.2%)  Loss: 0.05278\n",
      "Iteration: 674/1100 (61.3%)  Loss: 0.00737\n",
      "Iteration: 675/1100 (61.4%)  Loss: 0.00528\n",
      "Iteration: 676/1100 (61.5%)  Loss: 0.01783\n",
      "Iteration: 677/1100 (61.5%)  Loss: 0.01292\n",
      "Iteration: 678/1100 (61.6%)  Loss: 0.00487\n",
      "Iteration: 679/1100 (61.7%)  Loss: 0.02301\n",
      "Iteration: 680/1100 (61.8%)  Loss: 0.00490\n",
      "Iteration: 681/1100 (61.9%)  Loss: 0.03135\n",
      "Iteration: 682/1100 (62.0%)  Loss: 0.04263\n",
      "Iteration: 683/1100 (62.1%)  Loss: 0.02457\n",
      "Iteration: 684/1100 (62.2%)  Loss: 0.00927\n",
      "Iteration: 685/1100 (62.3%)  Loss: 0.01373\n",
      "Iteration: 686/1100 (62.4%)  Loss: 0.03004\n",
      "Iteration: 687/1100 (62.5%)  Loss: 0.01529\n",
      "Iteration: 688/1100 (62.5%)  Loss: 0.01003\n",
      "Iteration: 689/1100 (62.6%)  Loss: 0.02684\n",
      "Iteration: 690/1100 (62.7%)  Loss: 0.01797\n",
      "Iteration: 691/1100 (62.8%)  Loss: 0.00809\n",
      "Iteration: 692/1100 (62.9%)  Loss: 0.01464\n",
      "Iteration: 693/1100 (63.0%)  Loss: 0.01270\n",
      "Iteration: 694/1100 (63.1%)  Loss: 0.00435\n",
      "Iteration: 695/1100 (63.2%)  Loss: 0.02436\n",
      "Iteration: 696/1100 (63.3%)  Loss: 0.02474\n",
      "Iteration: 697/1100 (63.4%)  Loss: 0.01361\n",
      "Iteration: 698/1100 (63.5%)  Loss: 0.00836\n",
      "Iteration: 699/1100 (63.5%)  Loss: 0.01422\n",
      "Iteration: 700/1100 (63.6%)  Loss: 0.01430\n",
      "Iteration: 701/1100 (63.7%)  Loss: 0.00527\n",
      "Iteration: 702/1100 (63.8%)  Loss: 0.00683\n",
      "Iteration: 703/1100 (63.9%)  Loss: 0.02580\n",
      "Iteration: 704/1100 (64.0%)  Loss: 0.02570\n",
      "Iteration: 705/1100 (64.1%)  Loss: 0.02613\n",
      "Iteration: 706/1100 (64.2%)  Loss: 0.02859\n",
      "Iteration: 707/1100 (64.3%)  Loss: 0.01257\n",
      "Iteration: 708/1100 (64.4%)  Loss: 0.00793\n",
      "Iteration: 709/1100 (64.5%)  Loss: 0.01983\n",
      "Iteration: 710/1100 (64.5%)  Loss: 0.01571\n",
      "Iteration: 711/1100 (64.6%)  Loss: 0.01427\n",
      "Iteration: 712/1100 (64.7%)  Loss: 0.01898\n",
      "Iteration: 713/1100 (64.8%)  Loss: 0.03024\n",
      "Iteration: 714/1100 (64.9%)  Loss: 0.03651\n",
      "Iteration: 715/1100 (65.0%)  Loss: 0.03120\n",
      "Iteration: 716/1100 (65.1%)  Loss: 0.01322\n",
      "Iteration: 717/1100 (65.2%)  Loss: 0.00428\n",
      "Iteration: 718/1100 (65.3%)  Loss: 0.00712\n",
      "Iteration: 719/1100 (65.4%)  Loss: 0.01525\n",
      "Iteration: 720/1100 (65.5%)  Loss: 0.01825\n",
      "Iteration: 721/1100 (65.5%)  Loss: 0.00476\n",
      "Iteration: 722/1100 (65.6%)  Loss: 0.00090\n",
      "Iteration: 723/1100 (65.7%)  Loss: 0.01670\n",
      "Iteration: 724/1100 (65.8%)  Loss: 0.01013\n",
      "Iteration: 725/1100 (65.9%)  Loss: 0.03189\n",
      "Iteration: 726/1100 (66.0%)  Loss: 0.02722\n",
      "Iteration: 727/1100 (66.1%)  Loss: 0.02860\n",
      "Iteration: 728/1100 (66.2%)  Loss: 0.00449\n",
      "Iteration: 729/1100 (66.3%)  Loss: 0.03332\n",
      "Iteration: 730/1100 (66.4%)  Loss: 0.01082\n",
      "Iteration: 731/1100 (66.5%)  Loss: 0.00879\n",
      "Iteration: 732/1100 (66.5%)  Loss: 0.02421\n",
      "Iteration: 733/1100 (66.6%)  Loss: 0.02070\n",
      "Iteration: 734/1100 (66.7%)  Loss: 0.01544\n",
      "Iteration: 735/1100 (66.8%)  Loss: 0.00754\n",
      "Iteration: 736/1100 (66.9%)  Loss: 0.02567\n",
      "Iteration: 737/1100 (67.0%)  Loss: 0.01842\n",
      "Iteration: 738/1100 (67.1%)  Loss: 0.00694\n",
      "Iteration: 739/1100 (67.2%)  Loss: 0.02241\n",
      "Iteration: 740/1100 (67.3%)  Loss: 0.00840\n",
      "Iteration: 741/1100 (67.4%)  Loss: 0.00751\n",
      "Iteration: 742/1100 (67.5%)  Loss: 0.00428\n",
      "Iteration: 743/1100 (67.5%)  Loss: 0.01777\n",
      "Iteration: 744/1100 (67.6%)  Loss: 0.00911\n",
      "Iteration: 745/1100 (67.7%)  Loss: 0.00620\n",
      "Iteration: 746/1100 (67.8%)  Loss: 0.01431\n",
      "Iteration: 747/1100 (67.9%)  Loss: 0.00842\n",
      "Iteration: 748/1100 (68.0%)  Loss: 0.02263\n",
      "Iteration: 749/1100 (68.1%)  Loss: 0.02739\n",
      "Iteration: 750/1100 (68.2%)  Loss: 0.01550\n",
      "Iteration: 751/1100 (68.3%)  Loss: 0.03919\n",
      "Iteration: 752/1100 (68.4%)  Loss: 0.02772\n",
      "Iteration: 753/1100 (68.5%)  Loss: 0.01929\n",
      "Iteration: 754/1100 (68.5%)  Loss: 0.02356\n",
      "Iteration: 755/1100 (68.6%)  Loss: 0.02754\n",
      "Iteration: 756/1100 (68.7%)  Loss: 0.01342\n",
      "Iteration: 757/1100 (68.8%)  Loss: 0.01038\n",
      "Iteration: 758/1100 (68.9%)  Loss: 0.01287\n",
      "Iteration: 759/1100 (69.0%)  Loss: 0.01273\n",
      "Iteration: 760/1100 (69.1%)  Loss: 0.00271\n",
      "Iteration: 761/1100 (69.2%)  Loss: 0.01385\n",
      "Iteration: 762/1100 (69.3%)  Loss: 0.02946\n",
      "Iteration: 763/1100 (69.4%)  Loss: 0.01483\n",
      "Iteration: 764/1100 (69.5%)  Loss: 0.02088\n",
      "Iteration: 765/1100 (69.5%)  Loss: 0.01053\n",
      "Iteration: 766/1100 (69.6%)  Loss: 0.00995\n",
      "Iteration: 767/1100 (69.7%)  Loss: 0.00468\n",
      "Iteration: 768/1100 (69.8%)  Loss: 0.00587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 769/1100 (69.9%)  Loss: 0.00705\n",
      "Iteration: 770/1100 (70.0%)  Loss: 0.01975\n",
      "Iteration: 771/1100 (70.1%)  Loss: 0.00368\n",
      "Iteration: 772/1100 (70.2%)  Loss: 0.00421\n",
      "Iteration: 773/1100 (70.3%)  Loss: 0.00125\n",
      "Iteration: 774/1100 (70.4%)  Loss: 0.00359\n",
      "Iteration: 775/1100 (70.5%)  Loss: 0.01064\n",
      "Iteration: 776/1100 (70.5%)  Loss: 0.03070\n",
      "Iteration: 777/1100 (70.6%)  Loss: 0.02659\n",
      "Iteration: 778/1100 (70.7%)  Loss: 0.01481\n",
      "Iteration: 779/1100 (70.8%)  Loss: 0.01568\n",
      "Iteration: 780/1100 (70.9%)  Loss: 0.02142\n",
      "Iteration: 781/1100 (71.0%)  Loss: 0.02589\n",
      "Iteration: 782/1100 (71.1%)  Loss: 0.00601\n",
      "Iteration: 783/1100 (71.2%)  Loss: 0.01252\n",
      "Iteration: 784/1100 (71.3%)  Loss: 0.01500\n",
      "Iteration: 785/1100 (71.4%)  Loss: 0.00718\n",
      "Iteration: 786/1100 (71.5%)  Loss: 0.01455\n",
      "Iteration: 787/1100 (71.5%)  Loss: 0.00540\n",
      "Iteration: 788/1100 (71.6%)  Loss: 0.02602\n",
      "Iteration: 789/1100 (71.7%)  Loss: 0.00333\n",
      "Iteration: 790/1100 (71.8%)  Loss: 0.00815\n",
      "Iteration: 791/1100 (71.9%)  Loss: 0.01265\n",
      "Iteration: 792/1100 (72.0%)  Loss: 0.00823\n",
      "Iteration: 793/1100 (72.1%)  Loss: 0.00791\n",
      "Iteration: 794/1100 (72.2%)  Loss: 0.00893\n",
      "Iteration: 795/1100 (72.3%)  Loss: 0.01081\n",
      "Iteration: 796/1100 (72.4%)  Loss: 0.00694\n",
      "Iteration: 797/1100 (72.5%)  Loss: 0.02749\n",
      "Iteration: 798/1100 (72.5%)  Loss: 0.00935\n",
      "Iteration: 799/1100 (72.6%)  Loss: 0.00816\n",
      "Iteration: 800/1100 (72.7%)  Loss: 0.01071\n",
      "Iteration: 801/1100 (72.8%)  Loss: 0.00978\n",
      "Iteration: 802/1100 (72.9%)  Loss: 0.00978\n",
      "Iteration: 803/1100 (73.0%)  Loss: 0.02926\n",
      "Iteration: 804/1100 (73.1%)  Loss: 0.00634\n",
      "Iteration: 805/1100 (73.2%)  Loss: 0.00676\n",
      "Iteration: 806/1100 (73.3%)  Loss: 0.00987\n",
      "Iteration: 807/1100 (73.4%)  Loss: 0.01826\n",
      "Iteration: 808/1100 (73.5%)  Loss: 0.00822\n",
      "Iteration: 809/1100 (73.5%)  Loss: 0.02540\n",
      "Iteration: 810/1100 (73.6%)  Loss: 0.00510\n",
      "Iteration: 811/1100 (73.7%)  Loss: 0.03924\n",
      "Iteration: 812/1100 (73.8%)  Loss: 0.01100\n",
      "Iteration: 813/1100 (73.9%)  Loss: 0.01030\n",
      "Iteration: 814/1100 (74.0%)  Loss: 0.00969\n",
      "Iteration: 815/1100 (74.1%)  Loss: 0.00878\n",
      "Iteration: 816/1100 (74.2%)  Loss: 0.01992\n",
      "Iteration: 817/1100 (74.3%)  Loss: 0.00925\n",
      "Iteration: 818/1100 (74.4%)  Loss: 0.01736\n",
      "Iteration: 819/1100 (74.5%)  Loss: 0.04009\n",
      "Iteration: 820/1100 (74.5%)  Loss: 0.00429\n",
      "Iteration: 821/1100 (74.6%)  Loss: 0.01443\n",
      "Iteration: 822/1100 (74.7%)  Loss: 0.03001\n",
      "Iteration: 823/1100 (74.8%)  Loss: 0.00459\n",
      "Iteration: 824/1100 (74.9%)  Loss: 0.02765\n",
      "Iteration: 825/1100 (75.0%)  Loss: 0.00760\n",
      "Iteration: 826/1100 (75.1%)  Loss: 0.00527\n",
      "Iteration: 827/1100 (75.2%)  Loss: 0.01039\n",
      "Iteration: 828/1100 (75.3%)  Loss: 0.02190\n",
      "Iteration: 829/1100 (75.4%)  Loss: 0.01136\n",
      "Iteration: 830/1100 (75.5%)  Loss: 0.00606\n",
      "Iteration: 831/1100 (75.5%)  Loss: 0.03092\n",
      "Iteration: 832/1100 (75.6%)  Loss: 0.00180\n",
      "Iteration: 833/1100 (75.7%)  Loss: 0.01982\n",
      "Iteration: 834/1100 (75.8%)  Loss: 0.01920\n",
      "Iteration: 835/1100 (75.9%)  Loss: 0.01634\n",
      "Iteration: 836/1100 (76.0%)  Loss: 0.02230\n",
      "Iteration: 837/1100 (76.1%)  Loss: 0.00232\n",
      "Iteration: 838/1100 (76.2%)  Loss: 0.00342\n",
      "Iteration: 839/1100 (76.3%)  Loss: 0.00620\n",
      "Iteration: 840/1100 (76.4%)  Loss: 0.01915\n",
      "Iteration: 841/1100 (76.5%)  Loss: 0.01183\n",
      "Iteration: 842/1100 (76.5%)  Loss: 0.02650\n",
      "Iteration: 843/1100 (76.6%)  Loss: 0.04223\n",
      "Iteration: 844/1100 (76.7%)  Loss: 0.02231\n",
      "Iteration: 845/1100 (76.8%)  Loss: 0.01008\n",
      "Iteration: 846/1100 (76.9%)  Loss: 0.01325\n",
      "Iteration: 847/1100 (77.0%)  Loss: 0.00975\n",
      "Iteration: 848/1100 (77.1%)  Loss: 0.00859\n",
      "Iteration: 849/1100 (77.2%)  Loss: 0.00962\n",
      "Iteration: 850/1100 (77.3%)  Loss: 0.00293\n",
      "Iteration: 851/1100 (77.4%)  Loss: 0.01144\n",
      "Iteration: 852/1100 (77.5%)  Loss: 0.04135\n",
      "Iteration: 853/1100 (77.5%)  Loss: 0.00611\n",
      "Iteration: 854/1100 (77.6%)  Loss: 0.01260\n",
      "Iteration: 855/1100 (77.7%)  Loss: 0.00789\n",
      "Iteration: 856/1100 (77.8%)  Loss: 0.01243\n",
      "Iteration: 857/1100 (77.9%)  Loss: 0.00749\n",
      "Iteration: 858/1100 (78.0%)  Loss: 0.01881\n",
      "Iteration: 859/1100 (78.1%)  Loss: 0.00618\n",
      "Iteration: 860/1100 (78.2%)  Loss: 0.00762\n",
      "Iteration: 861/1100 (78.3%)  Loss: 0.01733\n",
      "Iteration: 862/1100 (78.4%)  Loss: 0.02792\n",
      "Iteration: 863/1100 (78.5%)  Loss: 0.01270\n",
      "Iteration: 864/1100 (78.5%)  Loss: 0.01927\n",
      "Iteration: 865/1100 (78.6%)  Loss: 0.03778\n",
      "Iteration: 866/1100 (78.7%)  Loss: 0.01402\n",
      "Iteration: 867/1100 (78.8%)  Loss: 0.01983\n",
      "Iteration: 868/1100 (78.9%)  Loss: 0.00266\n",
      "Iteration: 869/1100 (79.0%)  Loss: 0.02072\n",
      "Iteration: 870/1100 (79.1%)  Loss: 0.00355\n",
      "Iteration: 871/1100 (79.2%)  Loss: 0.00490\n",
      "Iteration: 872/1100 (79.3%)  Loss: 0.00715\n",
      "Iteration: 873/1100 (79.4%)  Loss: 0.01534\n",
      "Iteration: 874/1100 (79.5%)  Loss: 0.02589\n",
      "Iteration: 875/1100 (79.5%)  Loss: 0.01420\n",
      "Iteration: 876/1100 (79.6%)  Loss: 0.01808\n",
      "Iteration: 877/1100 (79.7%)  Loss: 0.01216\n",
      "Iteration: 878/1100 (79.8%)  Loss: 0.00952\n",
      "Iteration: 879/1100 (79.9%)  Loss: 0.01283\n",
      "Iteration: 880/1100 (80.0%)  Loss: 0.00100\n",
      "Iteration: 881/1100 (80.1%)  Loss: 0.00777\n",
      "Iteration: 882/1100 (80.2%)  Loss: 0.00868\n",
      "Iteration: 883/1100 (80.3%)  Loss: 0.01043\n",
      "Iteration: 884/1100 (80.4%)  Loss: 0.01217\n",
      "Iteration: 885/1100 (80.5%)  Loss: 0.00689\n",
      "Iteration: 886/1100 (80.5%)  Loss: 0.01827\n",
      "Iteration: 887/1100 (80.6%)  Loss: 0.00680\n",
      "Iteration: 888/1100 (80.7%)  Loss: 0.01438\n",
      "Iteration: 889/1100 (80.8%)  Loss: 0.00481\n",
      "Iteration: 890/1100 (80.9%)  Loss: 0.00375\n",
      "Iteration: 891/1100 (81.0%)  Loss: 0.02275\n",
      "Iteration: 892/1100 (81.1%)  Loss: 0.01065\n",
      "Iteration: 893/1100 (81.2%)  Loss: 0.03326\n",
      "Iteration: 894/1100 (81.3%)  Loss: 0.00199\n",
      "Iteration: 895/1100 (81.4%)  Loss: 0.01462\n",
      "Iteration: 896/1100 (81.5%)  Loss: 0.01801\n",
      "Iteration: 897/1100 (81.5%)  Loss: 0.00687\n",
      "Iteration: 898/1100 (81.6%)  Loss: 0.01765\n",
      "Iteration: 899/1100 (81.7%)  Loss: 0.02296\n",
      "Iteration: 900/1100 (81.8%)  Loss: 0.00676\n",
      "Iteration: 901/1100 (81.9%)  Loss: 0.00417\n",
      "Iteration: 902/1100 (82.0%)  Loss: 0.01249\n",
      "Iteration: 903/1100 (82.1%)  Loss: 0.00790\n",
      "Iteration: 904/1100 (82.2%)  Loss: 0.01026\n",
      "Iteration: 905/1100 (82.3%)  Loss: 0.00485\n",
      "Iteration: 906/1100 (82.4%)  Loss: 0.00969\n",
      "Iteration: 907/1100 (82.5%)  Loss: 0.03363\n",
      "Iteration: 908/1100 (82.5%)  Loss: 0.00843\n",
      "Iteration: 909/1100 (82.6%)  Loss: 0.01950\n",
      "Iteration: 910/1100 (82.7%)  Loss: 0.02163\n",
      "Iteration: 911/1100 (82.8%)  Loss: 0.00983\n",
      "Iteration: 912/1100 (82.9%)  Loss: 0.01265\n",
      "Iteration: 913/1100 (83.0%)  Loss: 0.00515\n",
      "Iteration: 914/1100 (83.1%)  Loss: 0.01755\n",
      "Iteration: 915/1100 (83.2%)  Loss: 0.00890\n",
      "Iteration: 916/1100 (83.3%)  Loss: 0.01346\n",
      "Iteration: 917/1100 (83.4%)  Loss: 0.00899\n",
      "Iteration: 918/1100 (83.5%)  Loss: 0.02148\n",
      "Iteration: 919/1100 (83.5%)  Loss: 0.00757\n",
      "Iteration: 920/1100 (83.6%)  Loss: 0.00720\n",
      "Iteration: 921/1100 (83.7%)  Loss: 0.01056\n",
      "Iteration: 922/1100 (83.8%)  Loss: 0.00387\n",
      "Iteration: 923/1100 (83.9%)  Loss: 0.01720\n",
      "Iteration: 924/1100 (84.0%)  Loss: 0.00559\n",
      "Iteration: 925/1100 (84.1%)  Loss: 0.02406\n",
      "Iteration: 926/1100 (84.2%)  Loss: 0.02509\n",
      "Iteration: 927/1100 (84.3%)  Loss: 0.02112\n",
      "Iteration: 928/1100 (84.4%)  Loss: 0.03502\n",
      "Iteration: 929/1100 (84.5%)  Loss: 0.02050\n",
      "Iteration: 930/1100 (84.5%)  Loss: 0.00745\n",
      "Iteration: 931/1100 (84.6%)  Loss: 0.01437\n",
      "Iteration: 932/1100 (84.7%)  Loss: 0.02530\n",
      "Iteration: 933/1100 (84.8%)  Loss: 0.02132\n",
      "Iteration: 934/1100 (84.9%)  Loss: 0.00890\n",
      "Iteration: 935/1100 (85.0%)  Loss: 0.01755\n",
      "Iteration: 936/1100 (85.1%)  Loss: 0.00208\n",
      "Iteration: 937/1100 (85.2%)  Loss: 0.01864\n",
      "Iteration: 938/1100 (85.3%)  Loss: 0.00391\n",
      "Iteration: 939/1100 (85.4%)  Loss: 0.03557\n",
      "Iteration: 940/1100 (85.5%)  Loss: 0.01853\n",
      "Iteration: 941/1100 (85.5%)  Loss: 0.01182\n",
      "Iteration: 942/1100 (85.6%)  Loss: 0.02063\n",
      "Iteration: 943/1100 (85.7%)  Loss: 0.02659\n",
      "Iteration: 944/1100 (85.8%)  Loss: 0.01409\n",
      "Iteration: 945/1100 (85.9%)  Loss: 0.02889\n",
      "Iteration: 946/1100 (86.0%)  Loss: 0.01460\n",
      "Iteration: 947/1100 (86.1%)  Loss: 0.00949\n",
      "Iteration: 948/1100 (86.2%)  Loss: 0.00982\n",
      "Iteration: 949/1100 (86.3%)  Loss: 0.02889\n",
      "Iteration: 950/1100 (86.4%)  Loss: 0.01490\n",
      "Iteration: 951/1100 (86.5%)  Loss: 0.01165\n",
      "Iteration: 952/1100 (86.5%)  Loss: 0.02235\n",
      "Iteration: 953/1100 (86.6%)  Loss: 0.00927\n",
      "Iteration: 954/1100 (86.7%)  Loss: 0.00942\n",
      "Iteration: 955/1100 (86.8%)  Loss: 0.01030\n",
      "Iteration: 956/1100 (86.9%)  Loss: 0.02735\n",
      "Iteration: 957/1100 (87.0%)  Loss: 0.00642\n",
      "Iteration: 958/1100 (87.1%)  Loss: 0.05013\n",
      "Iteration: 959/1100 (87.2%)  Loss: 0.01451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 960/1100 (87.3%)  Loss: 0.00640\n",
      "Iteration: 961/1100 (87.4%)  Loss: 0.01570\n",
      "Iteration: 962/1100 (87.5%)  Loss: 0.00922\n",
      "Iteration: 963/1100 (87.5%)  Loss: 0.03335\n",
      "Iteration: 964/1100 (87.6%)  Loss: 0.02452\n",
      "Iteration: 965/1100 (87.7%)  Loss: 0.01947\n",
      "Iteration: 966/1100 (87.8%)  Loss: 0.01624\n",
      "Iteration: 967/1100 (87.9%)  Loss: 0.01597\n",
      "Iteration: 968/1100 (88.0%)  Loss: 0.00553\n",
      "Iteration: 969/1100 (88.1%)  Loss: 0.00326\n",
      "Iteration: 970/1100 (88.2%)  Loss: 0.00651\n",
      "Iteration: 971/1100 (88.3%)  Loss: 0.02443\n",
      "Iteration: 972/1100 (88.4%)  Loss: 0.03348\n",
      "Iteration: 973/1100 (88.5%)  Loss: 0.00142\n",
      "Iteration: 974/1100 (88.5%)  Loss: 0.01133\n",
      "Iteration: 975/1100 (88.6%)  Loss: 0.02609\n",
      "Iteration: 976/1100 (88.7%)  Loss: 0.01600\n",
      "Iteration: 977/1100 (88.8%)  Loss: 0.00792\n",
      "Iteration: 978/1100 (88.9%)  Loss: 0.01519\n",
      "Iteration: 979/1100 (89.0%)  Loss: 0.00567\n",
      "Iteration: 980/1100 (89.1%)  Loss: 0.01624\n",
      "Iteration: 981/1100 (89.2%)  Loss: 0.02037\n",
      "Iteration: 982/1100 (89.3%)  Loss: 0.00447\n",
      "Iteration: 983/1100 (89.4%)  Loss: 0.00861\n",
      "Iteration: 984/1100 (89.5%)  Loss: 0.02191\n",
      "Iteration: 985/1100 (89.5%)  Loss: 0.02253\n",
      "Iteration: 986/1100 (89.6%)  Loss: 0.02878\n",
      "Iteration: 987/1100 (89.7%)  Loss: 0.01416\n",
      "Iteration: 988/1100 (89.8%)  Loss: 0.00472\n",
      "Iteration: 989/1100 (89.9%)  Loss: 0.00926\n",
      "Iteration: 990/1100 (90.0%)  Loss: 0.02133\n",
      "Iteration: 991/1100 (90.1%)  Loss: 0.00242\n",
      "Iteration: 992/1100 (90.2%)  Loss: 0.02202\n",
      "Iteration: 993/1100 (90.3%)  Loss: 0.02959\n",
      "Iteration: 994/1100 (90.4%)  Loss: 0.02451\n",
      "Iteration: 995/1100 (90.5%)  Loss: 0.02892\n",
      "Iteration: 996/1100 (90.5%)  Loss: 0.01740\n",
      "Iteration: 997/1100 (90.6%)  Loss: 0.00579\n",
      "Iteration: 998/1100 (90.7%)  Loss: 0.01561\n",
      "Iteration: 999/1100 (90.8%)  Loss: 0.02440\n",
      "Iteration: 1000/1100 (90.9%)  Loss: 0.01934\n",
      "Iteration: 1001/1100 (91.0%)  Loss: 0.00615\n",
      "Iteration: 1002/1100 (91.1%)  Loss: 0.00357\n",
      "Iteration: 1003/1100 (91.2%)  Loss: 0.03117\n",
      "Iteration: 1004/1100 (91.3%)  Loss: 0.03711\n",
      "Iteration: 1005/1100 (91.4%)  Loss: 0.00948\n",
      "Iteration: 1006/1100 (91.5%)  Loss: 0.02961\n",
      "Iteration: 1007/1100 (91.5%)  Loss: 0.01216\n",
      "Iteration: 1008/1100 (91.6%)  Loss: 0.01911\n",
      "Iteration: 1009/1100 (91.7%)  Loss: 0.01218\n",
      "Iteration: 1010/1100 (91.8%)  Loss: 0.01361\n",
      "Iteration: 1011/1100 (91.9%)  Loss: 0.00975\n",
      "Iteration: 1012/1100 (92.0%)  Loss: 0.00609\n",
      "Iteration: 1013/1100 (92.1%)  Loss: 0.00955\n",
      "Iteration: 1014/1100 (92.2%)  Loss: 0.03094\n",
      "Iteration: 1015/1100 (92.3%)  Loss: 0.01481\n",
      "Iteration: 1016/1100 (92.4%)  Loss: 0.02548\n",
      "Iteration: 1017/1100 (92.5%)  Loss: 0.00958\n",
      "Iteration: 1018/1100 (92.5%)  Loss: 0.01637\n",
      "Iteration: 1019/1100 (92.6%)  Loss: 0.00515\n",
      "Iteration: 1020/1100 (92.7%)  Loss: 0.00789\n",
      "Iteration: 1021/1100 (92.8%)  Loss: 0.01140\n",
      "Iteration: 1022/1100 (92.9%)  Loss: 0.00382\n",
      "Iteration: 1023/1100 (93.0%)  Loss: 0.00932\n",
      "Iteration: 1024/1100 (93.1%)  Loss: 0.00190\n",
      "Iteration: 1025/1100 (93.2%)  Loss: 0.02496\n",
      "Iteration: 1026/1100 (93.3%)  Loss: 0.02492\n",
      "Iteration: 1027/1100 (93.4%)  Loss: 0.00442\n",
      "Iteration: 1028/1100 (93.5%)  Loss: 0.01900\n",
      "Iteration: 1029/1100 (93.5%)  Loss: 0.01365\n",
      "Iteration: 1030/1100 (93.6%)  Loss: 0.00854\n",
      "Iteration: 1031/1100 (93.7%)  Loss: 0.00493\n",
      "Iteration: 1032/1100 (93.8%)  Loss: 0.02385\n",
      "Iteration: 1033/1100 (93.9%)  Loss: 0.00426\n",
      "Iteration: 1034/1100 (94.0%)  Loss: 0.00142\n",
      "Iteration: 1035/1100 (94.1%)  Loss: 0.01740\n",
      "Iteration: 1036/1100 (94.2%)  Loss: 0.02563\n",
      "Iteration: 1037/1100 (94.3%)  Loss: 0.01857\n",
      "Iteration: 1038/1100 (94.4%)  Loss: 0.01249\n",
      "Iteration: 1039/1100 (94.5%)  Loss: 0.01640\n",
      "Iteration: 1040/1100 (94.5%)  Loss: 0.03039\n",
      "Iteration: 1041/1100 (94.6%)  Loss: 0.03060\n",
      "Iteration: 1042/1100 (94.7%)  Loss: 0.00915\n",
      "Iteration: 1043/1100 (94.8%)  Loss: 0.02180\n",
      "Iteration: 1044/1100 (94.9%)  Loss: 0.01320\n",
      "Iteration: 1045/1100 (95.0%)  Loss: 0.00852\n",
      "Iteration: 1046/1100 (95.1%)  Loss: 0.03173\n",
      "Iteration: 1047/1100 (95.2%)  Loss: 0.01833\n",
      "Iteration: 1048/1100 (95.3%)  Loss: 0.00773\n",
      "Iteration: 1049/1100 (95.4%)  Loss: 0.02860\n",
      "Iteration: 1050/1100 (95.5%)  Loss: 0.01227\n",
      "Iteration: 1051/1100 (95.5%)  Loss: 0.01813\n",
      "Iteration: 1052/1100 (95.6%)  Loss: 0.03560\n",
      "Iteration: 1053/1100 (95.7%)  Loss: 0.01240\n",
      "Iteration: 1054/1100 (95.8%)  Loss: 0.01152\n",
      "Iteration: 1055/1100 (95.9%)  Loss: 0.01072\n",
      "Iteration: 1056/1100 (96.0%)  Loss: 0.02480\n",
      "Iteration: 1057/1100 (96.1%)  Loss: 0.02708\n",
      "Iteration: 1058/1100 (96.2%)  Loss: 0.00177\n",
      "Iteration: 1059/1100 (96.3%)  Loss: 0.01613\n",
      "Iteration: 1060/1100 (96.4%)  Loss: 0.01927\n",
      "Iteration: 1061/1100 (96.5%)  Loss: 0.01074\n",
      "Iteration: 1062/1100 (96.5%)  Loss: 0.01274\n",
      "Iteration: 1063/1100 (96.6%)  Loss: 0.00397\n",
      "Iteration: 1064/1100 (96.7%)  Loss: 0.00496\n",
      "Iteration: 1065/1100 (96.8%)  Loss: 0.02007\n",
      "Iteration: 1066/1100 (96.9%)  Loss: 0.02486\n",
      "Iteration: 1067/1100 (97.0%)  Loss: 0.03218\n",
      "Iteration: 1068/1100 (97.1%)  Loss: 0.00992\n",
      "Iteration: 1069/1100 (97.2%)  Loss: 0.01937\n",
      "Iteration: 1070/1100 (97.3%)  Loss: 0.00614\n",
      "Iteration: 1071/1100 (97.4%)  Loss: 0.00283\n",
      "Iteration: 1072/1100 (97.5%)  Loss: 0.00438\n",
      "Iteration: 1073/1100 (97.5%)  Loss: 0.00555\n",
      "Iteration: 1074/1100 (97.6%)  Loss: 0.01242\n",
      "Iteration: 1075/1100 (97.7%)  Loss: 0.03544\n",
      "Iteration: 1076/1100 (97.8%)  Loss: 0.00609\n",
      "Iteration: 1077/1100 (97.9%)  Loss: 0.01876\n",
      "Iteration: 1078/1100 (98.0%)  Loss: 0.01071\n",
      "Iteration: 1079/1100 (98.1%)  Loss: 0.00710\n",
      "Iteration: 1080/1100 (98.2%)  Loss: 0.00624\n",
      "Iteration: 1081/1100 (98.3%)  Loss: 0.00606\n",
      "Iteration: 1082/1100 (98.4%)  Loss: 0.02091\n",
      "Iteration: 1083/1100 (98.5%)  Loss: 0.02664\n",
      "Iteration: 1084/1100 (98.5%)  Loss: 0.02311\n",
      "Iteration: 1085/1100 (98.6%)  Loss: 0.01894\n",
      "Iteration: 1086/1100 (98.7%)  Loss: 0.01608\n",
      "Iteration: 1087/1100 (98.8%)  Loss: 0.00481\n",
      "Iteration: 1088/1100 (98.9%)  Loss: 0.00531\n",
      "Iteration: 1089/1100 (99.0%)  Loss: 0.01665\n",
      "Iteration: 1090/1100 (99.1%)  Loss: 0.00589\n",
      "Iteration: 1091/1100 (99.2%)  Loss: 0.01046\n",
      "Iteration: 1092/1100 (99.3%)  Loss: 0.01339\n",
      "Iteration: 1093/1100 (99.4%)  Loss: 0.00994\n",
      "Iteration: 1094/1100 (99.5%)  Loss: 0.02620\n",
      "Iteration: 1095/1100 (99.5%)  Loss: 0.02865\n",
      "Iteration: 1096/1100 (99.6%)  Loss: 0.01331\n",
      "Iteration: 1097/1100 (99.7%)  Loss: 0.01861\n",
      "Iteration: 1098/1100 (99.8%)  Loss: 0.00412\n",
      "Iteration: 1099/1100 (99.9%)  Loss: 0.00966\n",
      "Iteration: 0/1100 (0.0%)  Loss: 0.01346\n",
      "Iteration: 1/1100 (0.1%)  Loss: 0.00668\n",
      "Iteration: 2/1100 (0.2%)  Loss: 0.03025\n",
      "Iteration: 3/1100 (0.3%)  Loss: 0.01694\n",
      "Iteration: 4/1100 (0.4%)  Loss: 0.01056\n",
      "Iteration: 5/1100 (0.5%)  Loss: 0.01864\n",
      "Iteration: 6/1100 (0.5%)  Loss: 0.00649\n",
      "Iteration: 7/1100 (0.6%)  Loss: 0.00288\n",
      "Iteration: 8/1100 (0.7%)  Loss: 0.01341\n",
      "Iteration: 9/1100 (0.8%)  Loss: 0.01405\n",
      "Iteration: 10/1100 (0.9%)  Loss: 0.00858\n",
      "Iteration: 11/1100 (1.0%)  Loss: 0.00332\n",
      "Iteration: 12/1100 (1.1%)  Loss: 0.00609\n",
      "Iteration: 13/1100 (1.2%)  Loss: 0.01147\n",
      "Iteration: 14/1100 (1.3%)  Loss: 0.00850\n",
      "Iteration: 15/1100 (1.4%)  Loss: 0.01214\n",
      "Iteration: 16/1100 (1.5%)  Loss: 0.01948\n",
      "Iteration: 17/1100 (1.5%)  Loss: 0.00935\n",
      "Iteration: 18/1100 (1.6%)  Loss: 0.00733\n",
      "Iteration: 19/1100 (1.7%)  Loss: 0.01701\n",
      "Iteration: 20/1100 (1.8%)  Loss: 0.01756\n",
      "Iteration: 21/1100 (1.9%)  Loss: 0.00036\n",
      "Iteration: 22/1100 (2.0%)  Loss: 0.00765\n",
      "Iteration: 23/1100 (2.1%)  Loss: 0.01404\n",
      "Iteration: 24/1100 (2.2%)  Loss: 0.02443\n",
      "Iteration: 25/1100 (2.3%)  Loss: 0.00467\n",
      "Iteration: 26/1100 (2.4%)  Loss: 0.00442\n",
      "Iteration: 27/1100 (2.5%)  Loss: 0.00462\n",
      "Iteration: 28/1100 (2.5%)  Loss: 0.00387\n",
      "Iteration: 29/1100 (2.6%)  Loss: 0.00340\n",
      "Iteration: 30/1100 (2.7%)  Loss: 0.00586\n",
      "Iteration: 31/1100 (2.8%)  Loss: 0.00794\n",
      "Iteration: 32/1100 (2.9%)  Loss: 0.01096\n",
      "Iteration: 33/1100 (3.0%)  Loss: 0.00737\n",
      "Iteration: 34/1100 (3.1%)  Loss: 0.01719\n",
      "Iteration: 35/1100 (3.2%)  Loss: 0.00798\n",
      "Iteration: 36/1100 (3.3%)  Loss: 0.01135\n",
      "Iteration: 37/1100 (3.4%)  Loss: 0.00490\n",
      "Iteration: 38/1100 (3.5%)  Loss: 0.00775\n",
      "Iteration: 39/1100 (3.5%)  Loss: 0.00847\n",
      "Iteration: 40/1100 (3.6%)  Loss: 0.01756\n",
      "Iteration: 41/1100 (3.7%)  Loss: 0.00371\n",
      "Iteration: 42/1100 (3.8%)  Loss: 0.01315\n",
      "Iteration: 43/1100 (3.9%)  Loss: 0.00241\n",
      "Iteration: 44/1100 (4.0%)  Loss: 0.00225\n",
      "Iteration: 45/1100 (4.1%)  Loss: 0.01184\n",
      "Iteration: 46/1100 (4.2%)  Loss: 0.02025\n",
      "Iteration: 47/1100 (4.3%)  Loss: 0.00387\n",
      "Iteration: 48/1100 (4.4%)  Loss: 0.00485\n",
      "Iteration: 49/1100 (4.5%)  Loss: 0.00890\n",
      "Iteration: 50/1100 (4.5%)  Loss: 0.02244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 51/1100 (4.6%)  Loss: 0.00634\n",
      "Iteration: 52/1100 (4.7%)  Loss: 0.01930\n",
      "Iteration: 53/1100 (4.8%)  Loss: 0.00842\n",
      "Iteration: 54/1100 (4.9%)  Loss: 0.01092\n",
      "Iteration: 55/1100 (5.0%)  Loss: 0.00439\n",
      "Iteration: 56/1100 (5.1%)  Loss: 0.00817\n",
      "Iteration: 57/1100 (5.2%)  Loss: 0.00118\n",
      "Iteration: 58/1100 (5.3%)  Loss: 0.00590\n",
      "Iteration: 59/1100 (5.4%)  Loss: 0.01694\n",
      "Iteration: 60/1100 (5.5%)  Loss: 0.00843\n",
      "Iteration: 61/1100 (5.5%)  Loss: 0.00114\n",
      "Iteration: 62/1100 (5.6%)  Loss: 0.01311\n",
      "Iteration: 63/1100 (5.7%)  Loss: 0.00224\n",
      "Iteration: 64/1100 (5.8%)  Loss: 0.00368\n",
      "Iteration: 65/1100 (5.9%)  Loss: 0.01542\n",
      "Iteration: 66/1100 (6.0%)  Loss: 0.01174\n",
      "Iteration: 67/1100 (6.1%)  Loss: 0.00206\n",
      "Iteration: 68/1100 (6.2%)  Loss: 0.00216\n",
      "Iteration: 69/1100 (6.3%)  Loss: 0.00160\n",
      "Iteration: 70/1100 (6.4%)  Loss: 0.00201\n",
      "Iteration: 71/1100 (6.5%)  Loss: 0.02531\n",
      "Iteration: 72/1100 (6.5%)  Loss: 0.01390\n",
      "Iteration: 73/1100 (6.6%)  Loss: 0.00442\n",
      "Iteration: 74/1100 (6.7%)  Loss: 0.00780\n",
      "Iteration: 75/1100 (6.8%)  Loss: 0.00574\n",
      "Iteration: 76/1100 (6.9%)  Loss: 0.00783\n",
      "Iteration: 77/1100 (7.0%)  Loss: 0.01506\n",
      "Iteration: 78/1100 (7.1%)  Loss: 0.01602\n",
      "Iteration: 79/1100 (7.2%)  Loss: 0.02507\n",
      "Iteration: 80/1100 (7.3%)  Loss: 0.00216\n",
      "Iteration: 81/1100 (7.4%)  Loss: 0.00816\n",
      "Iteration: 82/1100 (7.5%)  Loss: 0.00969\n",
      "Iteration: 83/1100 (7.5%)  Loss: 0.01937\n",
      "Iteration: 84/1100 (7.6%)  Loss: 0.01517\n",
      "Iteration: 85/1100 (7.7%)  Loss: 0.00543\n",
      "Iteration: 86/1100 (7.8%)  Loss: 0.01028\n",
      "Iteration: 87/1100 (7.9%)  Loss: 0.01383\n",
      "Iteration: 88/1100 (8.0%)  Loss: 0.01190\n",
      "Iteration: 89/1100 (8.1%)  Loss: 0.00293\n",
      "Iteration: 90/1100 (8.2%)  Loss: 0.01117\n",
      "Iteration: 91/1100 (8.3%)  Loss: 0.01036\n",
      "Iteration: 92/1100 (8.4%)  Loss: 0.01407\n",
      "Iteration: 93/1100 (8.5%)  Loss: 0.00760\n",
      "Iteration: 94/1100 (8.5%)  Loss: 0.00200\n",
      "Iteration: 95/1100 (8.6%)  Loss: 0.01697\n",
      "Iteration: 96/1100 (8.7%)  Loss: 0.02210\n",
      "Iteration: 97/1100 (8.8%)  Loss: 0.02308\n",
      "Iteration: 98/1100 (8.9%)  Loss: 0.00365\n",
      "Iteration: 99/1100 (9.0%)  Loss: 0.01283\n",
      "Iteration: 100/1100 (9.1%)  Loss: 0.01169\n",
      "Iteration: 101/1100 (9.2%)  Loss: 0.00685\n",
      "Iteration: 102/1100 (9.3%)  Loss: 0.00560\n",
      "Iteration: 103/1100 (9.4%)  Loss: 0.01970\n",
      "Iteration: 104/1100 (9.5%)  Loss: 0.00327\n",
      "Iteration: 105/1100 (9.5%)  Loss: 0.00204\n",
      "Iteration: 106/1100 (9.6%)  Loss: 0.01170\n",
      "Iteration: 107/1100 (9.7%)  Loss: 0.01551\n",
      "Iteration: 108/1100 (9.8%)  Loss: 0.00637\n",
      "Iteration: 109/1100 (9.9%)  Loss: 0.00777\n",
      "Iteration: 110/1100 (10.0%)  Loss: 0.01087\n",
      "Iteration: 111/1100 (10.1%)  Loss: 0.00564\n",
      "Iteration: 112/1100 (10.2%)  Loss: 0.00988\n",
      "Iteration: 113/1100 (10.3%)  Loss: 0.01048\n",
      "Iteration: 114/1100 (10.4%)  Loss: 0.01159\n",
      "Iteration: 115/1100 (10.5%)  Loss: 0.01146\n",
      "Iteration: 116/1100 (10.5%)  Loss: 0.01247\n",
      "Iteration: 117/1100 (10.6%)  Loss: 0.01727\n",
      "Iteration: 118/1100 (10.7%)  Loss: 0.03004\n",
      "Iteration: 119/1100 (10.8%)  Loss: 0.00454\n",
      "Iteration: 120/1100 (10.9%)  Loss: 0.02091\n",
      "Iteration: 121/1100 (11.0%)  Loss: 0.01474\n",
      "Iteration: 122/1100 (11.1%)  Loss: 0.00302\n",
      "Iteration: 123/1100 (11.2%)  Loss: 0.02145\n",
      "Iteration: 124/1100 (11.3%)  Loss: 0.00737\n",
      "Iteration: 125/1100 (11.4%)  Loss: 0.01443\n",
      "Iteration: 126/1100 (11.5%)  Loss: 0.00070\n",
      "Iteration: 127/1100 (11.5%)  Loss: 0.01456\n",
      "Iteration: 128/1100 (11.6%)  Loss: 0.00718\n",
      "Iteration: 129/1100 (11.7%)  Loss: 0.00179\n",
      "Iteration: 130/1100 (11.8%)  Loss: 0.00834\n",
      "Iteration: 131/1100 (11.9%)  Loss: 0.00774\n",
      "Iteration: 132/1100 (12.0%)  Loss: 0.01593\n",
      "Iteration: 133/1100 (12.1%)  Loss: 0.00293\n",
      "Iteration: 134/1100 (12.2%)  Loss: 0.00776\n",
      "Iteration: 135/1100 (12.3%)  Loss: 0.01953\n",
      "Iteration: 136/1100 (12.4%)  Loss: 0.01855\n",
      "Iteration: 137/1100 (12.5%)  Loss: 0.00423\n",
      "Iteration: 138/1100 (12.5%)  Loss: 0.00591\n",
      "Iteration: 139/1100 (12.6%)  Loss: 0.00629\n",
      "Iteration: 140/1100 (12.7%)  Loss: 0.00768\n",
      "Iteration: 141/1100 (12.8%)  Loss: 0.00553\n",
      "Iteration: 142/1100 (12.9%)  Loss: 0.00948\n",
      "Iteration: 143/1100 (13.0%)  Loss: 0.01008\n",
      "Iteration: 144/1100 (13.1%)  Loss: 0.01031\n",
      "Iteration: 145/1100 (13.2%)  Loss: 0.00623\n",
      "Iteration: 146/1100 (13.3%)  Loss: 0.00433\n",
      "Iteration: 147/1100 (13.4%)  Loss: 0.00228\n",
      "Iteration: 148/1100 (13.5%)  Loss: 0.00323\n",
      "Iteration: 149/1100 (13.5%)  Loss: 0.01136\n",
      "Iteration: 150/1100 (13.6%)  Loss: 0.01049\n",
      "Iteration: 151/1100 (13.7%)  Loss: 0.01118\n",
      "Iteration: 152/1100 (13.8%)  Loss: 0.00844\n",
      "Iteration: 153/1100 (13.9%)  Loss: 0.00518\n",
      "Iteration: 154/1100 (14.0%)  Loss: 0.02265\n",
      "Iteration: 155/1100 (14.1%)  Loss: 0.01342\n",
      "Iteration: 156/1100 (14.2%)  Loss: 0.01238\n",
      "Iteration: 157/1100 (14.3%)  Loss: 0.01221\n",
      "Iteration: 158/1100 (14.4%)  Loss: 0.00388\n",
      "Iteration: 159/1100 (14.5%)  Loss: 0.00220\n",
      "Iteration: 160/1100 (14.5%)  Loss: 0.00260\n",
      "Iteration: 161/1100 (14.6%)  Loss: 0.01389\n",
      "Iteration: 162/1100 (14.7%)  Loss: 0.00832\n",
      "Iteration: 163/1100 (14.8%)  Loss: 0.00183\n",
      "Iteration: 164/1100 (14.9%)  Loss: 0.00374\n",
      "Iteration: 165/1100 (15.0%)  Loss: 0.00174\n",
      "Iteration: 166/1100 (15.1%)  Loss: 0.00881\n",
      "Iteration: 167/1100 (15.2%)  Loss: 0.00618\n",
      "Iteration: 168/1100 (15.3%)  Loss: 0.00328\n",
      "Iteration: 169/1100 (15.4%)  Loss: 0.00331\n",
      "Iteration: 170/1100 (15.5%)  Loss: 0.00847\n",
      "Iteration: 171/1100 (15.5%)  Loss: 0.00704\n",
      "Iteration: 172/1100 (15.6%)  Loss: 0.01291\n",
      "Iteration: 173/1100 (15.7%)  Loss: 0.00485\n",
      "Iteration: 174/1100 (15.8%)  Loss: 0.00882\n",
      "Iteration: 175/1100 (15.9%)  Loss: 0.01423\n",
      "Iteration: 176/1100 (16.0%)  Loss: 0.02391\n",
      "Iteration: 177/1100 (16.1%)  Loss: 0.00358\n",
      "Iteration: 178/1100 (16.2%)  Loss: 0.00474\n",
      "Iteration: 179/1100 (16.3%)  Loss: 0.00385\n",
      "Iteration: 180/1100 (16.4%)  Loss: 0.01613\n",
      "Iteration: 181/1100 (16.5%)  Loss: 0.01022\n",
      "Iteration: 182/1100 (16.5%)  Loss: 0.00984\n",
      "Iteration: 183/1100 (16.6%)  Loss: 0.01666\n",
      "Iteration: 184/1100 (16.7%)  Loss: 0.00464\n",
      "Iteration: 185/1100 (16.8%)  Loss: 0.00456\n",
      "Iteration: 186/1100 (16.9%)  Loss: 0.00528\n",
      "Iteration: 187/1100 (17.0%)  Loss: 0.01236\n",
      "Iteration: 188/1100 (17.1%)  Loss: 0.00363\n",
      "Iteration: 189/1100 (17.2%)  Loss: 0.00923\n",
      "Iteration: 190/1100 (17.3%)  Loss: 0.02856\n",
      "Iteration: 191/1100 (17.4%)  Loss: 0.00648\n",
      "Iteration: 192/1100 (17.5%)  Loss: 0.00367\n",
      "Iteration: 193/1100 (17.5%)  Loss: 0.00714\n",
      "Iteration: 194/1100 (17.6%)  Loss: 0.00356\n",
      "Iteration: 195/1100 (17.7%)  Loss: 0.01247\n",
      "Iteration: 196/1100 (17.8%)  Loss: 0.00496\n",
      "Iteration: 197/1100 (17.9%)  Loss: 0.00214\n",
      "Iteration: 198/1100 (18.0%)  Loss: 0.01042\n",
      "Iteration: 199/1100 (18.1%)  Loss: 0.00558\n",
      "Iteration: 200/1100 (18.2%)  Loss: 0.00675\n",
      "Iteration: 201/1100 (18.3%)  Loss: 0.00138\n",
      "Iteration: 202/1100 (18.4%)  Loss: 0.01816\n",
      "Iteration: 203/1100 (18.5%)  Loss: 0.00689\n",
      "Iteration: 204/1100 (18.5%)  Loss: 0.00226\n",
      "Iteration: 205/1100 (18.6%)  Loss: 0.00848\n",
      "Iteration: 206/1100 (18.7%)  Loss: 0.00923\n",
      "Iteration: 207/1100 (18.8%)  Loss: 0.00860\n",
      "Iteration: 208/1100 (18.9%)  Loss: 0.00606\n",
      "Iteration: 209/1100 (19.0%)  Loss: 0.01324\n",
      "Iteration: 210/1100 (19.1%)  Loss: 0.00066\n",
      "Iteration: 211/1100 (19.2%)  Loss: 0.00455\n",
      "Iteration: 212/1100 (19.3%)  Loss: 0.02116\n",
      "Iteration: 213/1100 (19.4%)  Loss: 0.00650\n",
      "Iteration: 214/1100 (19.5%)  Loss: 0.01399\n",
      "Iteration: 215/1100 (19.5%)  Loss: 0.01301\n",
      "Iteration: 216/1100 (19.6%)  Loss: 0.00478\n",
      "Iteration: 217/1100 (19.7%)  Loss: 0.02242\n",
      "Iteration: 218/1100 (19.8%)  Loss: 0.00345\n",
      "Iteration: 219/1100 (19.9%)  Loss: 0.01623\n",
      "Iteration: 220/1100 (20.0%)  Loss: 0.02349\n",
      "Iteration: 221/1100 (20.1%)  Loss: 0.00514\n",
      "Iteration: 222/1100 (20.2%)  Loss: 0.00929\n",
      "Iteration: 223/1100 (20.3%)  Loss: 0.00656\n",
      "Iteration: 224/1100 (20.4%)  Loss: 0.01845\n",
      "Iteration: 225/1100 (20.5%)  Loss: 0.00234\n",
      "Iteration: 226/1100 (20.5%)  Loss: 0.03101\n",
      "Iteration: 227/1100 (20.6%)  Loss: 0.00420\n",
      "Iteration: 228/1100 (20.7%)  Loss: 0.00790\n",
      "Iteration: 229/1100 (20.8%)  Loss: 0.05008\n",
      "Iteration: 230/1100 (20.9%)  Loss: 0.00463\n",
      "Iteration: 231/1100 (21.0%)  Loss: 0.02910\n",
      "Iteration: 232/1100 (21.1%)  Loss: 0.01673\n",
      "Iteration: 233/1100 (21.2%)  Loss: 0.01905\n",
      "Iteration: 234/1100 (21.3%)  Loss: 0.03129\n",
      "Iteration: 235/1100 (21.4%)  Loss: 0.02241\n",
      "Iteration: 236/1100 (21.5%)  Loss: 0.00446\n",
      "Iteration: 237/1100 (21.5%)  Loss: 0.00935\n",
      "Iteration: 238/1100 (21.6%)  Loss: 0.00547\n",
      "Iteration: 239/1100 (21.7%)  Loss: 0.01028\n",
      "Iteration: 240/1100 (21.8%)  Loss: 0.00463\n",
      "Iteration: 241/1100 (21.9%)  Loss: 0.00737\n",
      "Iteration: 242/1100 (22.0%)  Loss: 0.00338\n",
      "Iteration: 243/1100 (22.1%)  Loss: 0.01923\n",
      "Iteration: 244/1100 (22.2%)  Loss: 0.01166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 245/1100 (22.3%)  Loss: 0.00700\n",
      "Iteration: 246/1100 (22.4%)  Loss: 0.00617\n",
      "Iteration: 247/1100 (22.5%)  Loss: 0.00100\n",
      "Iteration: 248/1100 (22.5%)  Loss: 0.00523\n",
      "Iteration: 249/1100 (22.6%)  Loss: 0.00475\n",
      "Iteration: 250/1100 (22.7%)  Loss: 0.03283\n",
      "Iteration: 251/1100 (22.8%)  Loss: 0.00586\n",
      "Iteration: 252/1100 (22.9%)  Loss: 0.01686\n",
      "Iteration: 253/1100 (23.0%)  Loss: 0.02283\n",
      "Iteration: 254/1100 (23.1%)  Loss: 0.00957\n",
      "Iteration: 255/1100 (23.2%)  Loss: 0.01100\n",
      "Iteration: 256/1100 (23.3%)  Loss: 0.00433\n",
      "Iteration: 257/1100 (23.4%)  Loss: 0.02085\n",
      "Iteration: 258/1100 (23.5%)  Loss: 0.00480\n",
      "Iteration: 259/1100 (23.5%)  Loss: 0.00560\n",
      "Iteration: 260/1100 (23.6%)  Loss: 0.00645\n",
      "Iteration: 261/1100 (23.7%)  Loss: 0.01192\n",
      "Iteration: 262/1100 (23.8%)  Loss: 0.00255\n",
      "Iteration: 263/1100 (23.9%)  Loss: 0.02377\n",
      "Iteration: 264/1100 (24.0%)  Loss: 0.00685\n",
      "Iteration: 265/1100 (24.1%)  Loss: 0.00105\n",
      "Iteration: 266/1100 (24.2%)  Loss: 0.00199\n",
      "Iteration: 267/1100 (24.3%)  Loss: 0.01262\n",
      "Iteration: 268/1100 (24.4%)  Loss: 0.01048\n",
      "Iteration: 269/1100 (24.5%)  Loss: 0.01451\n",
      "Iteration: 270/1100 (24.5%)  Loss: 0.01667\n",
      "Iteration: 271/1100 (24.6%)  Loss: 0.00807\n",
      "Iteration: 272/1100 (24.7%)  Loss: 0.00718\n",
      "Iteration: 273/1100 (24.8%)  Loss: 0.01121\n",
      "Iteration: 274/1100 (24.9%)  Loss: 0.00876\n",
      "Iteration: 275/1100 (25.0%)  Loss: 0.00574\n",
      "Iteration: 276/1100 (25.1%)  Loss: 0.00500\n",
      "Iteration: 277/1100 (25.2%)  Loss: 0.00418\n",
      "Iteration: 278/1100 (25.3%)  Loss: 0.00663\n",
      "Iteration: 279/1100 (25.4%)  Loss: 0.00327\n",
      "Iteration: 280/1100 (25.5%)  Loss: 0.00884\n",
      "Iteration: 281/1100 (25.5%)  Loss: 0.00473\n",
      "Iteration: 282/1100 (25.6%)  Loss: 0.02863\n",
      "Iteration: 283/1100 (25.7%)  Loss: 0.00547\n",
      "Iteration: 284/1100 (25.8%)  Loss: 0.00905\n",
      "Iteration: 285/1100 (25.9%)  Loss: 0.00303\n",
      "Iteration: 286/1100 (26.0%)  Loss: 0.00676\n",
      "Iteration: 287/1100 (26.1%)  Loss: 0.00400\n",
      "Iteration: 288/1100 (26.2%)  Loss: 0.00329\n",
      "Iteration: 289/1100 (26.3%)  Loss: 0.00451\n",
      "Iteration: 290/1100 (26.4%)  Loss: 0.02088\n",
      "Iteration: 291/1100 (26.5%)  Loss: 0.01090\n",
      "Iteration: 292/1100 (26.5%)  Loss: 0.01131\n",
      "Iteration: 293/1100 (26.6%)  Loss: 0.01281\n",
      "Iteration: 294/1100 (26.7%)  Loss: 0.00361\n",
      "Iteration: 295/1100 (26.8%)  Loss: 0.00842\n",
      "Iteration: 296/1100 (26.9%)  Loss: 0.01459\n",
      "Iteration: 297/1100 (27.0%)  Loss: 0.00484\n",
      "Iteration: 298/1100 (27.1%)  Loss: 0.01864\n",
      "Iteration: 299/1100 (27.2%)  Loss: 0.00441\n",
      "Iteration: 300/1100 (27.3%)  Loss: 0.00303\n",
      "Iteration: 301/1100 (27.4%)  Loss: 0.00816\n",
      "Iteration: 302/1100 (27.5%)  Loss: 0.00424\n",
      "Iteration: 303/1100 (27.5%)  Loss: 0.01574\n",
      "Iteration: 304/1100 (27.6%)  Loss: 0.01892\n",
      "Iteration: 305/1100 (27.7%)  Loss: 0.02723\n",
      "Iteration: 306/1100 (27.8%)  Loss: 0.01957\n",
      "Iteration: 307/1100 (27.9%)  Loss: 0.01505\n",
      "Iteration: 308/1100 (28.0%)  Loss: 0.02118\n",
      "Iteration: 309/1100 (28.1%)  Loss: 0.01509\n",
      "Iteration: 310/1100 (28.2%)  Loss: 0.00664\n",
      "Iteration: 311/1100 (28.3%)  Loss: 0.00736\n",
      "Iteration: 312/1100 (28.4%)  Loss: 0.02203\n",
      "Iteration: 313/1100 (28.5%)  Loss: 0.02109\n",
      "Iteration: 314/1100 (28.5%)  Loss: 0.00412\n",
      "Iteration: 315/1100 (28.6%)  Loss: 0.00919\n",
      "Iteration: 316/1100 (28.7%)  Loss: 0.01660\n",
      "Iteration: 317/1100 (28.8%)  Loss: 0.00694\n",
      "Iteration: 318/1100 (28.9%)  Loss: 0.01226\n",
      "Iteration: 319/1100 (29.0%)  Loss: 0.00950\n",
      "Iteration: 320/1100 (29.1%)  Loss: 0.00565\n",
      "Iteration: 321/1100 (29.2%)  Loss: 0.00343\n",
      "Iteration: 322/1100 (29.3%)  Loss: 0.03006\n",
      "Iteration: 323/1100 (29.4%)  Loss: 0.03362\n",
      "Iteration: 324/1100 (29.5%)  Loss: 0.02247\n",
      "Iteration: 325/1100 (29.5%)  Loss: 0.01388\n",
      "Iteration: 326/1100 (29.6%)  Loss: 0.00407\n",
      "Iteration: 327/1100 (29.7%)  Loss: 0.02572\n",
      "Iteration: 328/1100 (29.8%)  Loss: 0.01071\n",
      "Iteration: 329/1100 (29.9%)  Loss: 0.01722\n",
      "Iteration: 330/1100 (30.0%)  Loss: 0.00925\n",
      "Iteration: 331/1100 (30.1%)  Loss: 0.00520\n",
      "Iteration: 332/1100 (30.2%)  Loss: 0.00950\n",
      "Iteration: 333/1100 (30.3%)  Loss: 0.01816\n",
      "Iteration: 334/1100 (30.4%)  Loss: 0.01317\n",
      "Iteration: 335/1100 (30.5%)  Loss: 0.01159\n",
      "Iteration: 336/1100 (30.5%)  Loss: 0.02294\n",
      "Iteration: 337/1100 (30.6%)  Loss: 0.00855\n",
      "Iteration: 338/1100 (30.7%)  Loss: 0.00610\n",
      "Iteration: 339/1100 (30.8%)  Loss: 0.01591\n",
      "Iteration: 340/1100 (30.9%)  Loss: 0.00706\n",
      "Iteration: 341/1100 (31.0%)  Loss: 0.01912\n",
      "Iteration: 342/1100 (31.1%)  Loss: 0.00159\n",
      "Iteration: 343/1100 (31.2%)  Loss: 0.03007\n",
      "Iteration: 344/1100 (31.3%)  Loss: 0.00954\n",
      "Iteration: 345/1100 (31.4%)  Loss: 0.00559\n",
      "Iteration: 346/1100 (31.5%)  Loss: 0.02236\n",
      "Iteration: 347/1100 (31.5%)  Loss: 0.01058\n",
      "Iteration: 348/1100 (31.6%)  Loss: 0.03907\n",
      "Iteration: 349/1100 (31.7%)  Loss: 0.00280\n",
      "Iteration: 350/1100 (31.8%)  Loss: 0.00533\n",
      "Iteration: 351/1100 (31.9%)  Loss: 0.01414\n",
      "Iteration: 352/1100 (32.0%)  Loss: 0.01609\n",
      "Iteration: 353/1100 (32.1%)  Loss: 0.02212\n",
      "Iteration: 354/1100 (32.2%)  Loss: 0.01324\n",
      "Iteration: 355/1100 (32.3%)  Loss: 0.00403\n",
      "Iteration: 356/1100 (32.4%)  Loss: 0.02103\n",
      "Iteration: 357/1100 (32.5%)  Loss: 0.00319\n",
      "Iteration: 358/1100 (32.5%)  Loss: 0.00429\n",
      "Iteration: 359/1100 (32.6%)  Loss: 0.00177\n",
      "Iteration: 360/1100 (32.7%)  Loss: 0.02804\n",
      "Iteration: 361/1100 (32.8%)  Loss: 0.00737\n",
      "Iteration: 362/1100 (32.9%)  Loss: 0.00996\n",
      "Iteration: 363/1100 (33.0%)  Loss: 0.01918\n",
      "Iteration: 364/1100 (33.1%)  Loss: 0.00879\n",
      "Iteration: 365/1100 (33.2%)  Loss: 0.00178\n",
      "Iteration: 366/1100 (33.3%)  Loss: 0.01121\n",
      "Iteration: 367/1100 (33.4%)  Loss: 0.01229\n",
      "Iteration: 368/1100 (33.5%)  Loss: 0.00576\n",
      "Iteration: 369/1100 (33.5%)  Loss: 0.00310\n",
      "Iteration: 370/1100 (33.6%)  Loss: 0.02417\n",
      "Iteration: 371/1100 (33.7%)  Loss: 0.02110\n",
      "Iteration: 372/1100 (33.8%)  Loss: 0.01493\n",
      "Iteration: 373/1100 (33.9%)  Loss: 0.01163\n",
      "Iteration: 374/1100 (34.0%)  Loss: 0.00654\n",
      "Iteration: 375/1100 (34.1%)  Loss: 0.00983\n",
      "Iteration: 376/1100 (34.2%)  Loss: 0.00514\n",
      "Iteration: 377/1100 (34.3%)  Loss: 0.00557\n",
      "Iteration: 378/1100 (34.4%)  Loss: 0.03790\n",
      "Iteration: 379/1100 (34.5%)  Loss: 0.02572\n",
      "Iteration: 380/1100 (34.5%)  Loss: 0.02934\n",
      "Iteration: 381/1100 (34.6%)  Loss: 0.01383\n",
      "Iteration: 382/1100 (34.7%)  Loss: 0.03403\n",
      "Iteration: 383/1100 (34.8%)  Loss: 0.00263\n",
      "Iteration: 384/1100 (34.9%)  Loss: 0.01561\n",
      "Iteration: 385/1100 (35.0%)  Loss: 0.03253\n",
      "Iteration: 386/1100 (35.1%)  Loss: 0.00277\n",
      "Iteration: 387/1100 (35.2%)  Loss: 0.01755\n",
      "Iteration: 388/1100 (35.3%)  Loss: 0.00556\n",
      "Iteration: 389/1100 (35.4%)  Loss: 0.00639\n",
      "Iteration: 390/1100 (35.5%)  Loss: 0.00479\n",
      "Iteration: 391/1100 (35.5%)  Loss: 0.00180\n",
      "Iteration: 392/1100 (35.6%)  Loss: 0.02386\n",
      "Iteration: 393/1100 (35.7%)  Loss: 0.01631\n",
      "Iteration: 394/1100 (35.8%)  Loss: 0.01893\n",
      "Iteration: 395/1100 (35.9%)  Loss: 0.01510\n",
      "Iteration: 396/1100 (36.0%)  Loss: 0.00708\n",
      "Iteration: 397/1100 (36.1%)  Loss: 0.00995\n",
      "Iteration: 398/1100 (36.2%)  Loss: 0.00315\n",
      "Iteration: 399/1100 (36.3%)  Loss: 0.01325\n",
      "Iteration: 400/1100 (36.4%)  Loss: 0.02279\n",
      "Iteration: 401/1100 (36.5%)  Loss: 0.01834\n",
      "Iteration: 402/1100 (36.5%)  Loss: 0.01959\n",
      "Iteration: 403/1100 (36.6%)  Loss: 0.00835\n",
      "Iteration: 404/1100 (36.7%)  Loss: 0.01784\n",
      "Iteration: 405/1100 (36.8%)  Loss: 0.00494\n",
      "Iteration: 406/1100 (36.9%)  Loss: 0.00410\n",
      "Iteration: 407/1100 (37.0%)  Loss: 0.01824\n",
      "Iteration: 408/1100 (37.1%)  Loss: 0.00100\n",
      "Iteration: 409/1100 (37.2%)  Loss: 0.00130\n",
      "Iteration: 410/1100 (37.3%)  Loss: 0.02985\n",
      "Iteration: 411/1100 (37.4%)  Loss: 0.01728\n",
      "Iteration: 412/1100 (37.5%)  Loss: 0.00298\n",
      "Iteration: 413/1100 (37.5%)  Loss: 0.01162\n",
      "Iteration: 414/1100 (37.6%)  Loss: 0.00421\n",
      "Iteration: 415/1100 (37.7%)  Loss: 0.01857\n",
      "Iteration: 416/1100 (37.8%)  Loss: 0.01102\n",
      "Iteration: 417/1100 (37.9%)  Loss: 0.00712\n",
      "Iteration: 418/1100 (38.0%)  Loss: 0.00888\n",
      "Iteration: 419/1100 (38.1%)  Loss: 0.00423\n",
      "Iteration: 420/1100 (38.2%)  Loss: 0.00285\n",
      "Iteration: 421/1100 (38.3%)  Loss: 0.01328\n",
      "Iteration: 422/1100 (38.4%)  Loss: 0.00448\n",
      "Iteration: 423/1100 (38.5%)  Loss: 0.03131\n",
      "Iteration: 424/1100 (38.5%)  Loss: 0.00339\n",
      "Iteration: 425/1100 (38.6%)  Loss: 0.00758\n",
      "Iteration: 426/1100 (38.7%)  Loss: 0.00993\n",
      "Iteration: 427/1100 (38.8%)  Loss: 0.03701\n",
      "Iteration: 428/1100 (38.9%)  Loss: 0.00124\n",
      "Iteration: 429/1100 (39.0%)  Loss: 0.01839\n",
      "Iteration: 430/1100 (39.1%)  Loss: 0.00131\n",
      "Iteration: 431/1100 (39.2%)  Loss: 0.00534\n",
      "Iteration: 432/1100 (39.3%)  Loss: 0.00629\n",
      "Iteration: 433/1100 (39.4%)  Loss: 0.00775\n",
      "Iteration: 434/1100 (39.5%)  Loss: 0.00199\n",
      "Iteration: 435/1100 (39.5%)  Loss: 0.01543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 436/1100 (39.6%)  Loss: 0.02434\n",
      "Iteration: 437/1100 (39.7%)  Loss: 0.00719\n",
      "Iteration: 438/1100 (39.8%)  Loss: 0.02151\n",
      "Iteration: 439/1100 (39.9%)  Loss: 0.00852\n",
      "Iteration: 440/1100 (40.0%)  Loss: 0.02337\n",
      "Iteration: 441/1100 (40.1%)  Loss: 0.01209\n",
      "Iteration: 442/1100 (40.2%)  Loss: 0.00835\n",
      "Iteration: 443/1100 (40.3%)  Loss: 0.01434\n",
      "Iteration: 444/1100 (40.4%)  Loss: 0.01559\n",
      "Iteration: 445/1100 (40.5%)  Loss: 0.00670\n",
      "Iteration: 446/1100 (40.5%)  Loss: 0.00872\n",
      "Iteration: 447/1100 (40.6%)  Loss: 0.00263\n",
      "Iteration: 448/1100 (40.7%)  Loss: 0.00704\n",
      "Iteration: 449/1100 (40.8%)  Loss: 0.02673\n",
      "Iteration: 450/1100 (40.9%)  Loss: 0.02713\n",
      "Iteration: 451/1100 (41.0%)  Loss: 0.00883\n",
      "Iteration: 452/1100 (41.1%)  Loss: 0.01350\n",
      "Iteration: 453/1100 (41.2%)  Loss: 0.01295\n",
      "Iteration: 454/1100 (41.3%)  Loss: 0.01393\n",
      "Iteration: 455/1100 (41.4%)  Loss: 0.01816\n",
      "Iteration: 456/1100 (41.5%)  Loss: 0.01155\n",
      "Iteration: 457/1100 (41.5%)  Loss: 0.00433\n",
      "Iteration: 458/1100 (41.6%)  Loss: 0.00930\n",
      "Iteration: 459/1100 (41.7%)  Loss: 0.01242\n",
      "Iteration: 460/1100 (41.8%)  Loss: 0.01107\n",
      "Iteration: 461/1100 (41.9%)  Loss: 0.00460\n",
      "Iteration: 462/1100 (42.0%)  Loss: 0.00922\n",
      "Iteration: 463/1100 (42.1%)  Loss: 0.00954\n",
      "Iteration: 464/1100 (42.2%)  Loss: 0.00963\n",
      "Iteration: 465/1100 (42.3%)  Loss: 0.00531\n",
      "Iteration: 466/1100 (42.4%)  Loss: 0.00189\n",
      "Iteration: 467/1100 (42.5%)  Loss: 0.00379\n",
      "Iteration: 468/1100 (42.5%)  Loss: 0.02814\n",
      "Iteration: 469/1100 (42.6%)  Loss: 0.00619\n",
      "Iteration: 470/1100 (42.7%)  Loss: 0.01173\n",
      "Iteration: 471/1100 (42.8%)  Loss: 0.00061\n",
      "Iteration: 472/1100 (42.9%)  Loss: 0.00184\n",
      "Iteration: 473/1100 (43.0%)  Loss: 0.00686\n",
      "Iteration: 474/1100 (43.1%)  Loss: 0.00242\n",
      "Iteration: 475/1100 (43.2%)  Loss: 0.00405\n",
      "Iteration: 476/1100 (43.3%)  Loss: 0.00560\n",
      "Iteration: 477/1100 (43.4%)  Loss: 0.00692\n",
      "Iteration: 478/1100 (43.5%)  Loss: 0.00379\n",
      "Iteration: 479/1100 (43.5%)  Loss: 0.01777\n",
      "Iteration: 480/1100 (43.6%)  Loss: 0.00367\n",
      "Iteration: 481/1100 (43.7%)  Loss: 0.00166\n",
      "Iteration: 482/1100 (43.8%)  Loss: 0.00897\n",
      "Iteration: 483/1100 (43.9%)  Loss: 0.01005\n",
      "Iteration: 484/1100 (44.0%)  Loss: 0.00051\n",
      "Iteration: 485/1100 (44.1%)  Loss: 0.01122\n",
      "Iteration: 486/1100 (44.2%)  Loss: 0.01024\n",
      "Iteration: 487/1100 (44.3%)  Loss: 0.02312\n",
      "Iteration: 488/1100 (44.4%)  Loss: 0.01906\n",
      "Iteration: 489/1100 (44.5%)  Loss: 0.01334\n",
      "Iteration: 490/1100 (44.5%)  Loss: 0.03327\n",
      "Iteration: 491/1100 (44.6%)  Loss: 0.01072\n",
      "Iteration: 492/1100 (44.7%)  Loss: 0.00400\n",
      "Iteration: 493/1100 (44.8%)  Loss: 0.00163\n",
      "Iteration: 494/1100 (44.9%)  Loss: 0.00452\n",
      "Iteration: 495/1100 (45.0%)  Loss: 0.00799\n",
      "Iteration: 496/1100 (45.1%)  Loss: 0.00444\n",
      "Iteration: 497/1100 (45.2%)  Loss: 0.00463\n",
      "Iteration: 498/1100 (45.3%)  Loss: 0.01207\n",
      "Iteration: 499/1100 (45.4%)  Loss: 0.00605\n",
      "Iteration: 500/1100 (45.5%)  Loss: 0.00670\n",
      "Iteration: 501/1100 (45.5%)  Loss: 0.00903\n",
      "Iteration: 502/1100 (45.6%)  Loss: 0.03185\n",
      "Iteration: 503/1100 (45.7%)  Loss: 0.01924\n",
      "Iteration: 504/1100 (45.8%)  Loss: 0.00379\n",
      "Iteration: 505/1100 (45.9%)  Loss: 0.00476\n",
      "Iteration: 506/1100 (46.0%)  Loss: 0.00735\n",
      "Iteration: 507/1100 (46.1%)  Loss: 0.01369\n",
      "Iteration: 508/1100 (46.2%)  Loss: 0.00648\n",
      "Iteration: 509/1100 (46.3%)  Loss: 0.01424\n",
      "Iteration: 510/1100 (46.4%)  Loss: 0.00446\n",
      "Iteration: 511/1100 (46.5%)  Loss: 0.02017\n",
      "Iteration: 512/1100 (46.5%)  Loss: 0.03090\n",
      "Iteration: 513/1100 (46.6%)  Loss: 0.01765\n",
      "Iteration: 514/1100 (46.7%)  Loss: 0.01135\n",
      "Iteration: 515/1100 (46.8%)  Loss: 0.01909\n",
      "Iteration: 516/1100 (46.9%)  Loss: 0.01157\n",
      "Iteration: 517/1100 (47.0%)  Loss: 0.00864\n",
      "Iteration: 518/1100 (47.1%)  Loss: 0.00493\n",
      "Iteration: 519/1100 (47.2%)  Loss: 0.01148\n",
      "Iteration: 520/1100 (47.3%)  Loss: 0.01182\n",
      "Iteration: 521/1100 (47.4%)  Loss: 0.00080\n",
      "Iteration: 522/1100 (47.5%)  Loss: 0.00914\n",
      "Iteration: 523/1100 (47.5%)  Loss: 0.00890\n",
      "Iteration: 524/1100 (47.6%)  Loss: 0.02575\n",
      "Iteration: 525/1100 (47.7%)  Loss: 0.00866\n",
      "Iteration: 526/1100 (47.8%)  Loss: 0.00309\n",
      "Iteration: 527/1100 (47.9%)  Loss: 0.00625\n",
      "Iteration: 528/1100 (48.0%)  Loss: 0.01464\n",
      "Iteration: 529/1100 (48.1%)  Loss: 0.00800\n",
      "Iteration: 530/1100 (48.2%)  Loss: 0.01312\n",
      "Iteration: 531/1100 (48.3%)  Loss: 0.01264\n",
      "Iteration: 532/1100 (48.4%)  Loss: 0.01922\n",
      "Iteration: 533/1100 (48.5%)  Loss: 0.00354\n",
      "Iteration: 534/1100 (48.5%)  Loss: 0.00687\n",
      "Iteration: 535/1100 (48.6%)  Loss: 0.01120\n",
      "Iteration: 536/1100 (48.7%)  Loss: 0.00412\n",
      "Iteration: 537/1100 (48.8%)  Loss: 0.02879\n",
      "Iteration: 538/1100 (48.9%)  Loss: 0.01175\n",
      "Iteration: 539/1100 (49.0%)  Loss: 0.00139\n",
      "Iteration: 540/1100 (49.1%)  Loss: 0.00288\n",
      "Iteration: 541/1100 (49.2%)  Loss: 0.00289\n",
      "Iteration: 542/1100 (49.3%)  Loss: 0.00236\n",
      "Iteration: 543/1100 (49.4%)  Loss: 0.00092\n",
      "Iteration: 544/1100 (49.5%)  Loss: 0.01528\n",
      "Iteration: 545/1100 (49.5%)  Loss: 0.00753\n",
      "Iteration: 546/1100 (49.6%)  Loss: 0.00426\n",
      "Iteration: 547/1100 (49.7%)  Loss: 0.01266\n",
      "Iteration: 548/1100 (49.8%)  Loss: 0.00433\n",
      "Iteration: 549/1100 (49.9%)  Loss: 0.00097\n",
      "Iteration: 550/1100 (50.0%)  Loss: 0.00533\n",
      "Iteration: 551/1100 (50.1%)  Loss: 0.00532\n",
      "Iteration: 552/1100 (50.2%)  Loss: 0.00879\n",
      "Iteration: 553/1100 (50.3%)  Loss: 0.00966\n",
      "Iteration: 554/1100 (50.4%)  Loss: 0.00561\n",
      "Iteration: 555/1100 (50.5%)  Loss: 0.00782\n",
      "Iteration: 556/1100 (50.5%)  Loss: 0.00239\n",
      "Iteration: 557/1100 (50.6%)  Loss: 0.03118\n",
      "Iteration: 558/1100 (50.7%)  Loss: 0.00629\n",
      "Iteration: 559/1100 (50.8%)  Loss: 0.01271\n",
      "Iteration: 560/1100 (50.9%)  Loss: 0.01976\n",
      "Iteration: 561/1100 (51.0%)  Loss: 0.00814\n",
      "Iteration: 562/1100 (51.1%)  Loss: 0.00844\n",
      "Iteration: 563/1100 (51.2%)  Loss: 0.01318\n",
      "Iteration: 564/1100 (51.3%)  Loss: 0.00767\n",
      "Iteration: 565/1100 (51.4%)  Loss: 0.00100\n",
      "Iteration: 566/1100 (51.5%)  Loss: 0.01953\n",
      "Iteration: 567/1100 (51.5%)  Loss: 0.00815\n",
      "Iteration: 568/1100 (51.6%)  Loss: 0.02012\n",
      "Iteration: 569/1100 (51.7%)  Loss: 0.00315\n",
      "Iteration: 570/1100 (51.8%)  Loss: 0.00716\n",
      "Iteration: 571/1100 (51.9%)  Loss: 0.00392\n",
      "Iteration: 572/1100 (52.0%)  Loss: 0.00073\n",
      "Iteration: 573/1100 (52.1%)  Loss: 0.02500\n",
      "Iteration: 574/1100 (52.2%)  Loss: 0.04081\n",
      "Iteration: 575/1100 (52.3%)  Loss: 0.01109\n",
      "Iteration: 576/1100 (52.4%)  Loss: 0.00588\n",
      "Iteration: 577/1100 (52.5%)  Loss: 0.01694\n",
      "Iteration: 578/1100 (52.5%)  Loss: 0.00104\n",
      "Iteration: 579/1100 (52.6%)  Loss: 0.00978\n",
      "Iteration: 580/1100 (52.7%)  Loss: 0.01008\n",
      "Iteration: 581/1100 (52.8%)  Loss: 0.00531\n",
      "Iteration: 582/1100 (52.9%)  Loss: 0.00664\n",
      "Iteration: 583/1100 (53.0%)  Loss: 0.02543\n",
      "Iteration: 584/1100 (53.1%)  Loss: 0.00986\n",
      "Iteration: 585/1100 (53.2%)  Loss: 0.01440\n",
      "Iteration: 586/1100 (53.3%)  Loss: 0.02118\n",
      "Iteration: 587/1100 (53.4%)  Loss: 0.00732\n",
      "Iteration: 588/1100 (53.5%)  Loss: 0.02153\n",
      "Iteration: 589/1100 (53.5%)  Loss: 0.00946\n",
      "Iteration: 590/1100 (53.6%)  Loss: 0.02966\n",
      "Iteration: 591/1100 (53.7%)  Loss: 0.00763\n",
      "Iteration: 592/1100 (53.8%)  Loss: 0.00807\n",
      "Iteration: 593/1100 (53.9%)  Loss: 0.00477\n",
      "Iteration: 594/1100 (54.0%)  Loss: 0.00487\n",
      "Iteration: 595/1100 (54.1%)  Loss: 0.00478\n",
      "Iteration: 596/1100 (54.2%)  Loss: 0.02071\n",
      "Iteration: 597/1100 (54.3%)  Loss: 0.00941\n",
      "Iteration: 598/1100 (54.4%)  Loss: 0.00430\n",
      "Iteration: 599/1100 (54.5%)  Loss: 0.01754\n",
      "Iteration: 600/1100 (54.5%)  Loss: 0.01037\n",
      "Iteration: 601/1100 (54.6%)  Loss: 0.01307\n",
      "Iteration: 602/1100 (54.7%)  Loss: 0.00880\n",
      "Iteration: 603/1100 (54.8%)  Loss: 0.00546\n",
      "Iteration: 604/1100 (54.9%)  Loss: 0.01207\n",
      "Iteration: 605/1100 (55.0%)  Loss: 0.00840\n",
      "Iteration: 606/1100 (55.1%)  Loss: 0.01905\n",
      "Iteration: 607/1100 (55.2%)  Loss: 0.02513\n",
      "Iteration: 608/1100 (55.3%)  Loss: 0.00853\n",
      "Iteration: 609/1100 (55.4%)  Loss: 0.00804\n",
      "Iteration: 610/1100 (55.5%)  Loss: 0.01513\n",
      "Iteration: 611/1100 (55.5%)  Loss: 0.01913\n",
      "Iteration: 612/1100 (55.6%)  Loss: 0.00999\n",
      "Iteration: 613/1100 (55.7%)  Loss: 0.00366\n",
      "Iteration: 614/1100 (55.8%)  Loss: 0.00589\n",
      "Iteration: 615/1100 (55.9%)  Loss: 0.02114\n",
      "Iteration: 616/1100 (56.0%)  Loss: 0.00861\n",
      "Iteration: 617/1100 (56.1%)  Loss: 0.02432\n",
      "Iteration: 618/1100 (56.2%)  Loss: 0.01502\n",
      "Iteration: 619/1100 (56.3%)  Loss: 0.02599\n",
      "Iteration: 620/1100 (56.4%)  Loss: 0.01358\n",
      "Iteration: 621/1100 (56.5%)  Loss: 0.00746\n",
      "Iteration: 622/1100 (56.5%)  Loss: 0.00608\n",
      "Iteration: 623/1100 (56.6%)  Loss: 0.00719\n",
      "Iteration: 624/1100 (56.7%)  Loss: 0.00635\n",
      "Iteration: 625/1100 (56.8%)  Loss: 0.00750\n",
      "Iteration: 626/1100 (56.9%)  Loss: 0.03489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 627/1100 (57.0%)  Loss: 0.01209\n",
      "Iteration: 628/1100 (57.1%)  Loss: 0.02732\n",
      "Iteration: 629/1100 (57.2%)  Loss: 0.01395\n",
      "Iteration: 630/1100 (57.3%)  Loss: 0.00531\n",
      "Iteration: 631/1100 (57.4%)  Loss: 0.00419\n",
      "Iteration: 632/1100 (57.5%)  Loss: 0.00432\n",
      "Iteration: 633/1100 (57.5%)  Loss: 0.02733\n",
      "Iteration: 634/1100 (57.6%)  Loss: 0.00961\n",
      "Iteration: 635/1100 (57.7%)  Loss: 0.01170\n",
      "Iteration: 636/1100 (57.8%)  Loss: 0.00734\n",
      "Iteration: 637/1100 (57.9%)  Loss: 0.01894\n",
      "Iteration: 638/1100 (58.0%)  Loss: 0.00610\n",
      "Iteration: 639/1100 (58.1%)  Loss: 0.03038\n",
      "Iteration: 640/1100 (58.2%)  Loss: 0.00207\n",
      "Iteration: 641/1100 (58.3%)  Loss: 0.01271\n",
      "Iteration: 642/1100 (58.4%)  Loss: 0.03383\n",
      "Iteration: 643/1100 (58.5%)  Loss: 0.02766\n",
      "Iteration: 644/1100 (58.5%)  Loss: 0.01487\n",
      "Iteration: 645/1100 (58.6%)  Loss: 0.01613\n",
      "Iteration: 646/1100 (58.7%)  Loss: 0.01247\n",
      "Iteration: 647/1100 (58.8%)  Loss: 0.01253\n",
      "Iteration: 648/1100 (58.9%)  Loss: 0.00419\n",
      "Iteration: 649/1100 (59.0%)  Loss: 0.01865\n",
      "Iteration: 650/1100 (59.1%)  Loss: 0.01785\n",
      "Iteration: 651/1100 (59.2%)  Loss: 0.01082\n",
      "Iteration: 652/1100 (59.3%)  Loss: 0.01554\n",
      "Iteration: 653/1100 (59.4%)  Loss: 0.01252\n",
      "Iteration: 654/1100 (59.5%)  Loss: 0.00609\n",
      "Iteration: 655/1100 (59.5%)  Loss: 0.01299\n",
      "Iteration: 656/1100 (59.6%)  Loss: 0.00425\n",
      "Iteration: 657/1100 (59.7%)  Loss: 0.00497\n",
      "Iteration: 658/1100 (59.8%)  Loss: 0.02735\n"
     ]
    }
   ],
   "source": [
    "# LOGITS\n",
    "correct = tf.equal(yLabel, y_predict, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "\n",
    "customLoss = margin_loss(caps2_output)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training = optimizer.minimize(customLoss, name=\"training_op\")\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# TRAINER\n",
    "\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "batchSize = 50\n",
    "nBatches =  mnist.train.num_examples // batchSize\n",
    "nBatches_validation = mnist.validation.num_examples // batchSize\n",
    "\n",
    "printMsg_lossTrain = []\n",
    "printMsg_accTrain = []\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range (nBatches):\n",
    "          xInput, yInput = mnist.train.next_batch(batchSize)\n",
    "          # Reshape to img\n",
    "          xInput = xInput.reshape([-1, 28, 28, 1])\n",
    "          _, trainingLoss = sess.run([training,customLoss], feed_dict={X: xInput, yLabel: yInput, })\n",
    "          print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                          batch, nBatches,\n",
    "                          batch * 100 / nBatches,\n",
    "                          trainingLoss))\n",
    "\n",
    "\n",
    "    # End of training loop, measure accuracy on validation set\n",
    "    loss_vals = []\n",
    "    acc_vals = []\n",
    "    for batch in range(1, nBatches_validation + 1):\n",
    "        xInput, yInput = mnist.validation.next_batch(batchSize)\n",
    "        xInput = xInput.reshape([-1, 28, 28, 1])\n",
    "        loss_val, acc_val = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: xInput,\n",
    "                           y: yInput})\n",
    "\n",
    "        loss_vals.append(loss_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(batch, nBatches_validation,batch * 100 / nBatches_validation))\n",
    "    loss_val = np.mean(loss_vals)\n",
    "    acc_val = np.mean(acc_vals)\n",
    "    \n",
    "    printMsg_lossTrain.append(loss_val)\n",
    "    printMsg_accTrain.append(acc_val)\n",
    "\n",
    "    print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(epoch + 1, acc_val * 100, loss_val,\" (improved)\" if loss_val < best_loss_val else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
