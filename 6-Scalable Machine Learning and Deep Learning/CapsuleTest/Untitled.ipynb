{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fixing the seed to get the same output everytime \n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST Data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACDCAYAAACp4J7uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAHFBMAQAAOPgXNIeXh9H25HoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cf803620b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = mnist.train.images[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
    "# 32 maps of 6x6 capsules each, each capsule outputting a vector of size 8, 36 vectors of size 8 in total\n",
    "\n",
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Primary Capsules\n",
    "output_vec_no  = 32\n",
    "output_vect_length = 8\n",
    "caps_dims = 6\n",
    "\n",
    "#Parameters for digital Capsules\n",
    "\n",
    "digital_cap_no = 10\n",
    "digital_cap_dim = 16\n",
    "\n",
    "\n",
    "# Taking as an input the pictures, and outputing 32 vectors of length 8 for each of the inputs \n",
    "def primarycapsule(output_vec_no, output_vect_length, cap_dims, X):\n",
    "  # Two convolutional layers followed by a reshape and squash:\n",
    "  \n",
    "    first_layer = tf.layers.conv2d(X, filters=256, kernel_size=9,\n",
    "                                   padding='VALID', \n",
    "                                   strides=1, \n",
    "                                   activation=tf.nn.relu,\n",
    "                                   name='first_layer')\n",
    "    second_layer = tf.layers.conv2d(first_layer, filters=output_vec_no * output_vect_length,\n",
    "                                    kernel_size = 9, padding = \"VALID\", strides = 2, activation = tf.nn.relu, name = \"secondLayer\")\n",
    "\n",
    "    caps = tf.reshape(second_layer, [-1, output_vec_no * cap_dims * cap_dims, output_vect_length])\n",
    "\n",
    "    caps = squash(caps)\n",
    "\n",
    "    return caps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1152, 8)\n",
      "(?, 1152, 10, 16, 8)\n",
      "(?, 1152, 10, 8, 1)\n",
      "(?, 1152, 10, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function Under Construction\n",
    "def digitalcapsule(primary_out, digital_cap_no, digital_cap_dim):\n",
    "  \n",
    "    W_init = tf.random_normal(shape=(1, output_vec_no * caps_dims * caps_dims, digital_cap_no, digital_cap_dim, output_vect_length),stddev=0.1, dtype=tf.float32, name=\"W_init\")\n",
    "    # Register as variable\n",
    "    W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    # lets you create an array containing many copies of a base array\n",
    "    W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "\n",
    "    # Inserts a dimension of 1 into a tensor's shape.\n",
    "    primaryCapsOutput_Expanded = tf.expand_dims(primary_out, -1,name=\"caps1_output_expanded\")\n",
    "    primaryCapsOutput_Tile = tf.expand_dims(primaryCapsOutput_Expanded, 2,name=\"caps1_output_tile\")\n",
    "    primaryCapsOutput_Tiled = tf.tile(primaryCapsOutput_Tile, [1, 1, digital_cap_no, 1, 1],name=\"caps1_output_tiled\")\n",
    "\n",
    "    print(W_tiled.shape) # ?, 1152, 10, 16, 8) dtype=float32>\n",
    "    print(primaryCapsOutput_Tiled.shape) # (?, 1152, 10, 8, 1) dtype=float32>\n",
    "\n",
    "    digitCapsulesPrediction = tf.matmul(W_tiled, primaryCapsOutput_Tiled,name=\"caps2_predicted\")  \n",
    "    print(digitCapsulesPrediction.shape)\n",
    "\n",
    "    return digitCapsulesPrediction\n",
    "\n",
    "\n",
    "primarycaps = primarycapsule(output_vec_no, output_vect_length, caps_dims, X)\n",
    "print(primarycaps.shape)\n",
    "digitalcaps = digitalcapsule(primarycaps, digital_cap_no, digital_cap_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUTING BY AGREEMENT CELL \n",
    "\n",
    "\n",
    "\n",
    "# first routing by agreement function, multiply weights with outputs, then sum \n",
    "def routing_1(batch_size, digitCapsules):\n",
    "    # initalizing the weights with zeroes\n",
    "\n",
    "    initialWeights = tf.zeros([batch_size, caps1_n_caps, digital_cap_no, 1, 1], dtype=np.float32, name=\"initialWeights\")\n",
    "\n",
    "    # Applying the softmax on the initial weights\n",
    "    routingWeights = tf.nn.softmax(initialWeights, dim=2, name=\"routingWeights\")\n",
    "\n",
    "    # Compute the weighted sum:\n",
    "\n",
    "    # We first multiply the weights with the output of the last capsule:\n",
    "    routingPrediction = tf.multiply(routingWeights, digitCapsules,name=\"weighted_predictions\")\n",
    "\n",
    "    # The we do the sum:\n",
    "    routingSum = tf.reduce_sum(routingPrediction, axis=1, keep_dims=True,name=\"routingSum\")\n",
    "\n",
    "    # applying squash function\n",
    "    digitCapsulesOutput = squash(routingSum, axis=-2,name=\"caps2_output_round_1\")\n",
    "\n",
    "    return digitCapsulesOutput, initialWeights\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def routing_2(initialWeights, digitCapsulesOutput, digitCapsulesPrediction):\n",
    "\n",
    "    # duplicating the number of each output per primary capsules to primaryCapsules.size to prepare the multiplication \n",
    "    digitCapsulesOutput_Tiled = tf.tile(digitCapsulesOutput, [1, caps1_n_caps, 1, 1, 1],name=\"caps2_output_round_1_tiled\")\n",
    "\n",
    "    # Multipliyng\n",
    "\n",
    "    routingAgreement = tf.matmul(digitCapsulesPrediction, digitCapsulesOutput_Tiled,\n",
    "                      transpose_a=True, name=\"agreement\")\n",
    "\n",
    "\n",
    "    # adding the agreement update to the initial weights before softmax (used in previous round)\n",
    "    raw_weights_round_2 = tf.add(initialWeights, routingAgreement,name=\"raw_weights_round_2\")\n",
    "\n",
    "\n",
    "    # Rest is like round 1, multiply by weights, sum and squash\n",
    "\n",
    "    routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,dim=2,name=\"routing_weights_round_2\")\n",
    "    weighted_predictions_round_2 = tf.multiply(routing_weights_round_2, digitCapsulesOutput, name=\"weighted_predictions_round_2\")\n",
    "    weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2, axis=1, keep_dims=True,  name=\"weighted_sum_round_2\")\n",
    "    caps2_output_round_2 = squash(weighted_sum_round_2,axis=-2,name=\"caps2_output_round_2\")\n",
    "\n",
    "    return caps2_output_round_2\n",
    "\n",
    "\n",
    "# routing by agreement calls \n",
    "batch_sizes = tf.shape(X)[0]\n",
    "digitCapsOutput, initWeights = routing_1(batch_sizes, digitalcaps)\n",
    "caps2_output = routing_2(initWeights, digitCapsOutput, digitalcaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize class probabilities, but do it safely like with the squash as we can get a division by 0\n",
    "def safe_normalize(probabilities, axis=-1, eps=1e-7, keepDimensions=False, name=None):\n",
    "    with tf.name_scope(name, default_name='safeNormalize'):\n",
    "        squaredProbabilities = tf.reduce_sum(tf.square(probabilities),axis=axis, keep_dims=keepDimensions)\n",
    "        sqrtVal = tf.sqrt(squaredProbabilities + eps)\n",
    "        return sqrtVal\n",
    "  \n",
    "def getPrediction(routingInput):\n",
    "    # Find the predicted index\n",
    "    y_hat_index = tf.argmax((safe_normalize(routingInput, axis=-2, name=\"y_hat\")), axis=2, name=\"y_proba\")\n",
    "\n",
    "    # Remove \n",
    "    yPrediction = tf.squeeze(y_hat_index, axis=[1,2], name=\"y_pred\")\n",
    "    return yPrediction\n",
    "\n",
    "\n",
    "y_predict = getPrediction(caps2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "yLabel = tf.placeholder(shape=[None], dtype=tf.int64, name=\"yLabel\")\n",
    "\n",
    "# Margin loss\n",
    "def margin_loss(routingInput, mPlus = 0.9, mMinus = 0.1, lambdaVal = 0.5):\n",
    "\n",
    "    # One hot representation of classes\n",
    "    Tk = tf.one_hot(yLabel, depth=10, name=\"T\")\n",
    "\n",
    "    caps2_norm = safe_normalize(routingInput, axis=-2, name=\"y_hat\")\n",
    "\n",
    "    # Find error for both agreeing classes and disagreeing classes on same image\n",
    "    correctPlacementError = tf.reshape(tf.square(tf.maximum(0.0,mPlus-caps2_norm)),  shape=(-1, 10), name=\"correctPlaceError\")\n",
    "    disPlacementError = tf.reshape(tf.square(tf.maximum(0.0,caps2_norm-mMinus)),  shape=(-1, 10), name=\"disPlaceError\")\n",
    "\n",
    "    #Compute loss for each digit\n",
    "    loss = tf.add(Tk * correctPlacementError, lambdaVal * (1.0-Tk) * disPlacementError, name=\"customLoss\")\n",
    "\n",
    "    marginLoss = tf.reduce_mean(tf.reduce_sum(loss, axis=1), name=\"marginLoss\")\n",
    "    return marginLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration: 50/1 (5000.0%)  Loss: 0.50000\n",
      "\r",
      "Iteration: 50/1 (5000.0%)  Loss: 0.50000\n"
     ]
    }
   ],
   "source": [
    " print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                          50, 1,\n",
    "                          50 * 100 / 1,\n",
    "                          0.5))\n",
    "    \n",
    " print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                          50, 1,\n",
    "                          50 * 100 / 1,\n",
    "                          0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0/1100 (0.0%)  Loss: 0.80940\n",
      "Iteration: 1/1100 (0.1%)  Loss: 3.60319\n",
      "Iteration: 2/1100 (0.2%)  Loss: 3.16828\n",
      "Iteration: 3/1100 (0.3%)  Loss: 0.67661\n",
      "Iteration: 4/1100 (0.4%)  Loss: 0.76622\n",
      "Iteration: 5/1100 (0.5%)  Loss: 0.69823\n",
      "Iteration: 6/1100 (0.5%)  Loss: 0.77993\n",
      "Iteration: 7/1100 (0.6%)  Loss: 0.73408\n",
      "Iteration: 8/1100 (0.7%)  Loss: 0.78135\n",
      "Iteration: 9/1100 (0.8%)  Loss: 0.77944\n",
      "Iteration: 10/1100 (0.9%)  Loss: 0.76100\n",
      "Iteration: 11/1100 (1.0%)  Loss: 0.70351\n",
      "Iteration: 12/1100 (1.1%)  Loss: 0.64592\n",
      "Iteration: 13/1100 (1.2%)  Loss: 0.59800\n",
      "Iteration: 14/1100 (1.3%)  Loss: 0.59656\n",
      "Iteration: 15/1100 (1.4%)  Loss: 0.46559\n",
      "Iteration: 16/1100 (1.5%)  Loss: 0.65094\n",
      "Iteration: 17/1100 (1.5%)  Loss: 0.56323\n",
      "Iteration: 18/1100 (1.6%)  Loss: 0.69559\n",
      "Iteration: 19/1100 (1.7%)  Loss: 0.69847\n",
      "Iteration: 20/1100 (1.8%)  Loss: 0.62548\n",
      "Iteration: 21/1100 (1.9%)  Loss: 0.42737\n",
      "Iteration: 22/1100 (2.0%)  Loss: 0.47385\n",
      "Iteration: 23/1100 (2.1%)  Loss: 0.60070\n",
      "Iteration: 24/1100 (2.2%)  Loss: 0.37707\n",
      "Iteration: 25/1100 (2.3%)  Loss: 0.39506\n",
      "Iteration: 26/1100 (2.4%)  Loss: 0.47326\n",
      "Iteration: 27/1100 (2.5%)  Loss: 0.46269\n",
      "Iteration: 28/1100 (2.5%)  Loss: 0.26503\n",
      "Iteration: 29/1100 (2.6%)  Loss: 0.40322\n",
      "Iteration: 30/1100 (2.7%)  Loss: 0.35304\n",
      "Iteration: 31/1100 (2.8%)  Loss: 0.24050\n",
      "Iteration: 32/1100 (2.9%)  Loss: 0.31322\n",
      "Iteration: 33/1100 (3.0%)  Loss: 0.32219\n",
      "Iteration: 34/1100 (3.1%)  Loss: 0.24873\n",
      "Iteration: 35/1100 (3.2%)  Loss: 0.24181\n",
      "Iteration: 36/1100 (3.3%)  Loss: 0.27199\n",
      "Iteration: 37/1100 (3.4%)  Loss: 0.17981\n",
      "Iteration: 38/1100 (3.5%)  Loss: 0.17969\n",
      "Iteration: 39/1100 (3.5%)  Loss: 0.21754\n",
      "Iteration: 40/1100 (3.6%)  Loss: 0.20233\n",
      "Iteration: 41/1100 (3.7%)  Loss: 0.21185\n",
      "Iteration: 42/1100 (3.8%)  Loss: 0.15365\n",
      "Iteration: 43/1100 (3.9%)  Loss: 0.19438\n",
      "Iteration: 44/1100 (4.0%)  Loss: 0.18453\n",
      "Iteration: 45/1100 (4.1%)  Loss: 0.14420\n",
      "Iteration: 46/1100 (4.2%)  Loss: 0.17924\n",
      "Iteration: 47/1100 (4.3%)  Loss: 0.20813\n",
      "Iteration: 48/1100 (4.4%)  Loss: 0.18560\n",
      "Iteration: 49/1100 (4.5%)  Loss: 0.12921\n",
      "Iteration: 50/1100 (4.5%)  Loss: 0.12485\n",
      "Iteration: 51/1100 (4.6%)  Loss: 0.16550\n",
      "Iteration: 52/1100 (4.7%)  Loss: 0.16404\n",
      "Iteration: 53/1100 (4.8%)  Loss: 0.08492\n",
      "Iteration: 54/1100 (4.9%)  Loss: 0.15147\n",
      "Iteration: 55/1100 (5.0%)  Loss: 0.14371\n",
      "Iteration: 56/1100 (5.1%)  Loss: 0.13212\n",
      "Iteration: 57/1100 (5.2%)  Loss: 0.12978\n",
      "Iteration: 58/1100 (5.3%)  Loss: 0.12609\n",
      "Iteration: 59/1100 (5.4%)  Loss: 0.09882\n",
      "Iteration: 60/1100 (5.5%)  Loss: 0.07855\n",
      "Iteration: 61/1100 (5.5%)  Loss: 0.11805\n",
      "Iteration: 62/1100 (5.6%)  Loss: 0.08972\n",
      "Iteration: 63/1100 (5.7%)  Loss: 0.10814\n",
      "Iteration: 64/1100 (5.8%)  Loss: 0.09844\n",
      "Iteration: 65/1100 (5.9%)  Loss: 0.11913\n",
      "Iteration: 66/1100 (6.0%)  Loss: 0.10513\n",
      "Iteration: 67/1100 (6.1%)  Loss: 0.05482\n",
      "Iteration: 68/1100 (6.2%)  Loss: 0.08736\n",
      "Iteration: 69/1100 (6.3%)  Loss: 0.04404\n",
      "Iteration: 70/1100 (6.4%)  Loss: 0.08267\n",
      "Iteration: 71/1100 (6.5%)  Loss: 0.06460\n",
      "Iteration: 72/1100 (6.5%)  Loss: 0.07101\n",
      "Iteration: 73/1100 (6.6%)  Loss: 0.07679\n",
      "Iteration: 74/1100 (6.7%)  Loss: 0.05830\n",
      "Iteration: 75/1100 (6.8%)  Loss: 0.06444\n",
      "Iteration: 76/1100 (6.9%)  Loss: 0.08651\n",
      "Iteration: 77/1100 (7.0%)  Loss: 0.07553\n",
      "Iteration: 78/1100 (7.1%)  Loss: 0.05861\n",
      "Iteration: 79/1100 (7.2%)  Loss: 0.06711\n",
      "Iteration: 80/1100 (7.3%)  Loss: 0.05376\n",
      "Iteration: 81/1100 (7.4%)  Loss: 0.08503\n",
      "Iteration: 82/1100 (7.5%)  Loss: 0.09411\n",
      "Iteration: 83/1100 (7.5%)  Loss: 0.06764\n",
      "Iteration: 84/1100 (7.6%)  Loss: 0.05832\n",
      "Iteration: 85/1100 (7.7%)  Loss: 0.04847\n",
      "Iteration: 86/1100 (7.8%)  Loss: 0.05624\n",
      "Iteration: 87/1100 (7.9%)  Loss: 0.06802\n",
      "Iteration: 88/1100 (8.0%)  Loss: 0.05373\n",
      "Iteration: 89/1100 (8.1%)  Loss: 0.07815\n",
      "Iteration: 90/1100 (8.2%)  Loss: 0.05181\n",
      "Iteration: 91/1100 (8.3%)  Loss: 0.06590\n",
      "Iteration: 92/1100 (8.4%)  Loss: 0.04006\n",
      "Iteration: 93/1100 (8.5%)  Loss: 0.04767\n",
      "Iteration: 94/1100 (8.5%)  Loss: 0.05395\n",
      "Iteration: 95/1100 (8.6%)  Loss: 0.05125\n",
      "Iteration: 96/1100 (8.7%)  Loss: 0.05715\n",
      "Iteration: 97/1100 (8.8%)  Loss: 0.05282\n",
      "Iteration: 98/1100 (8.9%)  Loss: 0.02364\n",
      "Iteration: 99/1100 (9.0%)  Loss: 0.05621\n",
      "Iteration: 100/1100 (9.1%)  Loss: 0.04803\n",
      "Iteration: 101/1100 (9.2%)  Loss: 0.05878\n",
      "Iteration: 102/1100 (9.3%)  Loss: 0.06196\n",
      "Iteration: 103/1100 (9.4%)  Loss: 0.06353\n",
      "Iteration: 104/1100 (9.5%)  Loss: 0.04720\n",
      "Iteration: 105/1100 (9.5%)  Loss: 0.04991\n",
      "Iteration: 106/1100 (9.6%)  Loss: 0.04234\n",
      "Iteration: 107/1100 (9.7%)  Loss: 0.05367\n",
      "Iteration: 108/1100 (9.8%)  Loss: 0.07950\n",
      "Iteration: 109/1100 (9.9%)  Loss: 0.05210\n",
      "Iteration: 110/1100 (10.0%)  Loss: 0.04720\n",
      "Iteration: 111/1100 (10.1%)  Loss: 0.04815\n",
      "Iteration: 112/1100 (10.2%)  Loss: 0.05666\n",
      "Iteration: 113/1100 (10.3%)  Loss: 0.03667\n",
      "Iteration: 114/1100 (10.4%)  Loss: 0.05783\n",
      "Iteration: 115/1100 (10.5%)  Loss: 0.05011\n",
      "Iteration: 116/1100 (10.5%)  Loss: 0.03095\n",
      "Iteration: 117/1100 (10.6%)  Loss: 0.04005\n",
      "Iteration: 118/1100 (10.7%)  Loss: 0.07292\n",
      "Iteration: 119/1100 (10.8%)  Loss: 0.07153\n",
      "Iteration: 120/1100 (10.9%)  Loss: 0.04758\n",
      "Iteration: 121/1100 (11.0%)  Loss: 0.04590\n",
      "Iteration: 122/1100 (11.1%)  Loss: 0.03289\n",
      "Iteration: 123/1100 (11.2%)  Loss: 0.06951\n",
      "Iteration: 124/1100 (11.3%)  Loss: 0.05637\n",
      "Iteration: 125/1100 (11.4%)  Loss: 0.04267\n",
      "Iteration: 126/1100 (11.5%)  Loss: 0.09977\n",
      "Iteration: 127/1100 (11.5%)  Loss: 0.04077\n",
      "Iteration: 128/1100 (11.6%)  Loss: 0.07779\n",
      "Iteration: 129/1100 (11.7%)  Loss: 0.04191\n",
      "Iteration: 130/1100 (11.8%)  Loss: 0.03514\n",
      "Iteration: 131/1100 (11.9%)  Loss: 0.04663\n",
      "Iteration: 132/1100 (12.0%)  Loss: 0.01905\n",
      "Iteration: 133/1100 (12.1%)  Loss: 0.02513\n",
      "Iteration: 134/1100 (12.2%)  Loss: 0.05330\n",
      "Iteration: 135/1100 (12.3%)  Loss: 0.03916\n",
      "Iteration: 136/1100 (12.4%)  Loss: 0.02047\n",
      "Iteration: 137/1100 (12.5%)  Loss: 0.06825\n",
      "Iteration: 138/1100 (12.5%)  Loss: 0.02257\n",
      "Iteration: 139/1100 (12.6%)  Loss: 0.04219\n",
      "Iteration: 140/1100 (12.7%)  Loss: 0.07021\n",
      "Iteration: 141/1100 (12.8%)  Loss: 0.04021\n",
      "Iteration: 142/1100 (12.9%)  Loss: 0.07384\n",
      "Iteration: 143/1100 (13.0%)  Loss: 0.02784\n",
      "Iteration: 144/1100 (13.1%)  Loss: 0.02891\n",
      "Iteration: 145/1100 (13.2%)  Loss: 0.01478\n",
      "Iteration: 146/1100 (13.3%)  Loss: 0.02401\n",
      "Iteration: 147/1100 (13.4%)  Loss: 0.03539\n",
      "Iteration: 148/1100 (13.5%)  Loss: 0.03104\n",
      "Iteration: 149/1100 (13.5%)  Loss: 0.04683\n",
      "Iteration: 150/1100 (13.6%)  Loss: 0.05903\n",
      "Iteration: 151/1100 (13.7%)  Loss: 0.04698\n",
      "Iteration: 152/1100 (13.8%)  Loss: 0.05288\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-33054175e358>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m           \u001b[1;31m# Reshape to img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m           \u001b[0mxInput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxInput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m           \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcustomLoss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myLabel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m           print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n\u001b[0;32m     36\u001b[0m                           \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnBatches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LOGITS\n",
    "correct = tf.equal(yLabel, y_predict, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "\n",
    "customLoss = margin_loss(caps2_output)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training = optimizer.minimize(customLoss, name=\"training_op\")\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# TRAINER\n",
    "\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "batchSize = 50\n",
    "nBatches =  mnist.train.num_examples // batchSize\n",
    "nBatches_validation = mnist.validation.num_examples // batchSize\n",
    "\n",
    "printMsg_lossTrain = []\n",
    "printMsg_accTrain = []\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range (nBatches):\n",
    "          xInput, yInput = mnist.train.next_batch(batchSize)\n",
    "          # Reshape to img\n",
    "          xInput = xInput.reshape([-1, 28, 28, 1])\n",
    "          _, trainingLoss = sess.run([training,customLoss], feed_dict={X: xInput, yLabel: yInput, })\n",
    "          print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                          batch, nBatches,\n",
    "                          batch * 100 / nBatches,\n",
    "                          trainingLoss, end=\"\"))\n",
    "\n",
    "\n",
    "    # End of training loop, measure accuracy on validation set\n",
    "    loss_vals = []\n",
    "    acc_vals = []\n",
    "    for batch in range(1, nBatches_validation + 1):\n",
    "        xInput, yInput = mnist.validation.next_batch(batchSize)\n",
    "        xInput = xInput.reshape([-1, 28, 28, 1])\n",
    "        loss_val, acc_val = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: xInput,\n",
    "                           y: yInput})\n",
    "\n",
    "        loss_vals.append(loss_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(batch, nBatches_validation,batch * 100 / nBatches_validation))\n",
    "    loss_val = np.mean(loss_vals)\n",
    "    acc_val = np.mean(acc_vals)\n",
    "    \n",
    "    printMsg_lossTrain.append(loss_val)\n",
    "    printMsg_accTrain.append(acc_val)\n",
    "\n",
    "    print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(epoch + 1, acc_val * 100, loss_val,\" (improved)\" if loss_val < best_loss_val else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
